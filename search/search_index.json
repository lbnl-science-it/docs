{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Whether you need a virtual server, help deciding on a data storage solution, help with procuring lab equipment, or access to the Lab's Lawrencium HPC, the place to start is with an email to scienceit@lbl.gov! Initial consultations are always free We have many low cost services We support on and off premises capabilities - if the right answer isn't available at LBL, we can find what you need at UCB, with external or cloud services providers (AWS, GCP) or elsewhere! Free drop-in office hours are available every Wednesday from 10:30am to Noon in 50B-3209","title":"Home"},{"location":"#getting-started","text":"Whether you need a virtual server, help deciding on a data storage solution, help with procuring lab equipment, or access to the Lab's Lawrencium HPC, the place to start is with an email to scienceit@lbl.gov! Initial consultations are always free We have many low cost services We support on and off premises capabilities - if the right answer isn't available at LBL, we can find what you need at UCB, with external or cloud services providers (AWS, GCP) or elsewhere! Free drop-in office hours are available every Wednesday from 10:30am to Noon in 50B-3209","title":"Getting Started"},{"location":"cloud/","text":"Cloud Resources LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP). This program provides the following benefits: No charge to have an account in the program Charges only for actual usage of cloud services like storage or compute Monthly billing for cloud usage are direct to your PID via recharge mechanism Simple enrollment process. De-enrollment only changes the billing setup in your account, and your account will continue to be active. You maintain complete control of your own AWS or GCP dashboard, and your data, tools, and services in your account are only accessible by you. Both AWS and GCP make discounts available to LBNL users for using their services. Generally speaking, costs for using GCP will be lower than AWS due to lower pricing and additional discounts. The discounts are: Type AWS GCP Overall 7% 13% Data Egress 15% 25% For AWS, restrictions apply Currently there are over 125 people at LBNL with cloud accounts on AWS and GCP. Most commonly people use virtual machines and storage, however there are many advanced users using containerization, machine learning, AI, and data visualization services on both platforms. Maybe list a few of the more interesting projects using AWS and GCP here? To set up a cloud account on either AWS or GCP, please send email to scienceit@lbl.gov to start the process.","title":"Cloud Resources"},{"location":"cloud/#cloud-resources","text":"LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP). This program provides the following benefits: No charge to have an account in the program Charges only for actual usage of cloud services like storage or compute Monthly billing for cloud usage are direct to your PID via recharge mechanism Simple enrollment process. De-enrollment only changes the billing setup in your account, and your account will continue to be active. You maintain complete control of your own AWS or GCP dashboard, and your data, tools, and services in your account are only accessible by you. Both AWS and GCP make discounts available to LBNL users for using their services. Generally speaking, costs for using GCP will be lower than AWS due to lower pricing and additional discounts. The discounts are: Type AWS GCP Overall 7% 13% Data Egress 15% 25% For AWS, restrictions apply Currently there are over 125 people at LBNL with cloud accounts on AWS and GCP. Most commonly people use virtual machines and storage, however there are many advanced users using containerization, machine learning, AI, and data visualization services on both platforms. Maybe list a few of the more interesting projects using AWS and GCP here? To set up a cloud account on either AWS or GCP, please send email to scienceit@lbl.gov to start the process.","title":"Cloud Resources"},{"location":"compute/","text":"Lawrencium Cluster Overview Detailed information of Lawrencium Accounts on Lawrencium Three types of Project Accounts PI Computing Allowance (PCA) account: free 300K SUs per year (pc_xxx) Condo account: PIs buy in compute nodes to the general condo pool (lr_xxx) Recharge account: with minimal recharge rate ~ $0.01/SU (ac_xxx) User accounts User account request User agreement consent https://sites.google.com/a/lbl.gov/hpc/getting-an-account Software Module Farm Module commands module purge : clear user\u2019s work environment module avail : check available software packages module load xxx : load a package module list : check currently loaded software Users may install their own software https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide SLURM: Resource Manager & Job Scheduler Job Submission Get help with the complete command options sbatch --help sbatch: submit a job to the batch queue system sbatch myjob.sh srun: request an interactive node(s) and login automatically srun -A ac_xxx -p lr5 -q lr_normal -t 1:0:0 --pty bash salloc : request an interactive node(s) salloc \u2013A pc_xxx \u2013p lr6 \u2013q lr_debug \u2013t 0:30:0 Job Monitoring sinfo: check status of partitions and nodes (idle, allocated, drain, down) sinfo \u2013r \u2013p lr6 squeue: check jobs in the batch queuing system (R or PD) squeue \u2013u $USER sacct: check job information or history sacct -X -o 'jobid,user,partition,nodelist,stat' scancel : cancel a job scancel jobID Check slurm association, such as qos, account, partition sacctmgr show association user=UserName -p perceus-00|ac_test|UserName|lr6|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|ac_test|UserName|lr5|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|pc_test|UserName|lr4|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|lr_test|UserName|lr3|1||||||||||||lr_debug,lr_lowprio,lr_normal,te perceus-00|scs|UserName|es1|1||||||||||||es_debug,es_lowprio,es_normal||| https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions Data Transfer Node & Visualization Node Designate data transfer node lrc-xfer.lbl.gov scp -r /your/source/file $USER@lrc-xder.lbl.gov:/cluster/path rsync -avzh /your/source/file $USER @lrc-xfer.lbl.gov:/cluster/path Globus Online provide secured unified interface for data transfer endpoint lbn#lrc, Globus Connect, AWS S3 connector Visualization and remote desktop node viz.lbl.gov Detailed information click here JupyterHub LRC JupyterHub Virtual Machine Services More information of VM click here","title":"Compute Resources"},{"location":"compute/#lawrencium-cluster-overview","text":"Detailed information of Lawrencium","title":"Lawrencium Cluster Overview"},{"location":"compute/#accounts-on-lawrencium","text":"","title":"Accounts on Lawrencium"},{"location":"compute/#three-types-of-project-accounts","text":"PI Computing Allowance (PCA) account: free 300K SUs per year (pc_xxx) Condo account: PIs buy in compute nodes to the general condo pool (lr_xxx) Recharge account: with minimal recharge rate ~ $0.01/SU (ac_xxx)","title":"Three types of Project Accounts"},{"location":"compute/#user-accounts","text":"User account request User agreement consent https://sites.google.com/a/lbl.gov/hpc/getting-an-account","title":"User accounts"},{"location":"compute/#software-module-farm","text":"","title":"Software Module Farm"},{"location":"compute/#module-commands","text":"module purge : clear user\u2019s work environment module avail : check available software packages module load xxx : load a package module list : check currently loaded software Users may install their own software https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide","title":"Module commands"},{"location":"compute/#slurm-resource-manager-job-scheduler","text":"","title":"SLURM: Resource Manager &amp; Job Scheduler"},{"location":"compute/#job-submission","text":"Get help with the complete command options sbatch --help sbatch: submit a job to the batch queue system sbatch myjob.sh srun: request an interactive node(s) and login automatically srun -A ac_xxx -p lr5 -q lr_normal -t 1:0:0 --pty bash salloc : request an interactive node(s) salloc \u2013A pc_xxx \u2013p lr6 \u2013q lr_debug \u2013t 0:30:0","title":"Job Submission"},{"location":"compute/#job-monitoring","text":"sinfo: check status of partitions and nodes (idle, allocated, drain, down) sinfo \u2013r \u2013p lr6 squeue: check jobs in the batch queuing system (R or PD) squeue \u2013u $USER sacct: check job information or history sacct -X -o 'jobid,user,partition,nodelist,stat' scancel : cancel a job scancel jobID Check slurm association, such as qos, account, partition sacctmgr show association user=UserName -p perceus-00|ac_test|UserName|lr6|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|ac_test|UserName|lr5|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|pc_test|UserName|lr4|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|lr_test|UserName|lr3|1||||||||||||lr_debug,lr_lowprio,lr_normal,te perceus-00|scs|UserName|es1|1||||||||||||es_debug,es_lowprio,es_normal||| https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions","title":"Job Monitoring"},{"location":"compute/#data-transfer-node-visualization-node","text":"Designate data transfer node lrc-xfer.lbl.gov scp -r /your/source/file $USER@lrc-xder.lbl.gov:/cluster/path rsync -avzh /your/source/file $USER @lrc-xfer.lbl.gov:/cluster/path Globus Online provide secured unified interface for data transfer endpoint lbn#lrc, Globus Connect, AWS S3 connector Visualization and remote desktop node viz.lbl.gov Detailed information click here","title":"Data Transfer Node &amp; Visualization Node"},{"location":"compute/#jupyterhub","text":"LRC JupyterHub","title":"JupyterHub"},{"location":"compute/#virtual-machine-services","text":"More information of VM click here","title":"Virtual Machine Services"},{"location":"consult/","text":"Consulting & Training Science IT provides on-demand and long-term \"embedded\" research consulting support, depending on your needs. We also support hiring of interns for science programs through the Science IT Intern program, as well as collaborative evaluation & support for hiring of staff to meet science project IT needs. One email to scienceit@lbl.gov is all you need to do to get the help you want. We provide a wide range of training opportunities -- all at no cost to science users. Via our partnerships with UCB's D-Lab, NSF's XSEDE, and AWS/GCP training programs we offer brown-bags, technical sessions, and custom opportunities to grow your IT skills. For more information, and to register for upcoming events, please see ittraining.lbl.gov.","title":"Consulting & Training"},{"location":"consult/#consulting-training","text":"Science IT provides on-demand and long-term \"embedded\" research consulting support, depending on your needs. We also support hiring of interns for science programs through the Science IT Intern program, as well as collaborative evaluation & support for hiring of staff to meet science project IT needs. One email to scienceit@lbl.gov is all you need to do to get the help you want. We provide a wide range of training opportunities -- all at no cost to science users. Via our partnerships with UCB's D-Lab, NSF's XSEDE, and AWS/GCP training programs we offer brown-bags, technical sessions, and custom opportunities to grow your IT skills. For more information, and to register for upcoming events, please see ittraining.lbl.gov.","title":"Consulting &amp; Training"},{"location":"container-101/","text":"Container 101 on Lawrencium; June 10, 2020; Wei Feinstein Outline Container technology overview Build singularity containers Run singularity containers on Lawrencium Containerization Standardized packaging of software and dependencies Portable, shareable, and reproducible. Your application brings its environment with it. Share the same OS kernel Applications Package an analysis pipeline so that it runs on your laptop, in the cloud, and in HPC environment to produce the same result. Publish a paper and include a link to a container with all of the data and software that you used so that others can easily reproduce your results. Install and run an application that requires a complicated stack of dependencies Legacy codes require outdated OS Container vs. Virtual Machine Singularity Technology Open-source computer software that encapsulates an application and all its dependencies into a single image Bring containers and reproducibility to scientific computing and HPC Developed by Greg Kurtzer Typically users have a build system as root users, but may not be root users on a production system Docker Bring containerization to the community-scale Rich image repository Widely used by scientific communities Compose for defining multi-container, recipe/definition file to build docker images Security concerns not ideal for the HPC environment Learn more docker Singularity Workflow Install Singularity on a local machine Build Singularity images locally with a root permission Transfer images to LRC clusters Run containers on the cluster Root privilege is not permitted Singularity Installation OS platforms - Linux - Mac - Window Refer to instructions here for details Test your installation: $ singularity --version singularity version 3.2.1-1 $ singularity run docker://godlovedc/lolcow ______________________________________ / A tall, dark stranger will have more \\ \\ fun than you. / -------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Create Singularity Containers Build directly from pre-built docker images Docker hub Sylabs Cloud and Singularity hub More involved using docker images from other external resources Nvidia HPC containers Biocontainers AWS Build using definition files or recipes Singularity pull directly from docker:// & shub:// No root/sudo privilege is needed Create/download immutable squashfs images/containers singularity pull --help Docker Hub: Pull a container from Docker Hub. $ singularity pull docker://ubuntu:18.04 $ singularity pull docker://gcc:7.2.0 Singularity Hub: If no tag is specified, the master branch of the repository is used $ singularity pull hello-world.sif shub://singularityhub/hello-world Run Singularity with shell, run, exec shell sub-command: invokes an interactive shell within a container singularity shell hello-world.sif run sub-command: executes the container\u2019s runscript singularity run hello-world.sif exec sub-command: execute an arbitrary command within container singularity exec hello-world.sif cat /etc/os-release Singularity pull when Docker Containers Provided by Other External docker repositories Nvidia HPC containers Biocontainers AWS Steps to build singularity containers from NGC: Pull a docker image locally (e.g. pgi compiler from NGC) $ docker pull nvcr.io/hpc/pgi-compilers:ce ..... $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nvcr.io/hpc/pgi-compiler ce c13ce6cf7f66 6 months ago 9.9GB openmpi3.1 latest 08a5518bb344 9 months ago 14.3GB registry 2 f32a97de94e1 15 months ago 25.8MB ... Steps to build singularity containers from NGC: Push to the Docker Hub (docker login) or simply use your local docker images Singularity build from local registry $ singularity build pgi.sif docker-daemon://nvcr.io/hpc/pgi-compilers:ce $ ls Dokerfile hello-world.sif pgi.sif saxpy.c Singularity Compile OpenACC code $ singularity exec pgi.sif pgcc -o saxpy -acc -Minfo saxpy.c saxpy: 9, Loop is parallelizable Generating Tesla code 9, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */ 9, FMA (fused multiply-add) instruction(s) generated ... Execute binary ./saxpy $ singularity exec pgi.sif ./saxpy y[0] = 2.000000 Another example of using docker containers from AWS Singularity build Root/sudo privilege is needed singularity build --help Build from a definition file sudo singularity --debug build mycontainer.sif Singularity Definition File/Recipe Bootstrap: docker #library, docker, shub, localimage, yum, debootstrap, arch, busybox, zypper From: ubuntu ## used singularity run-help %help Hello. I'm in the container. ## executed on host after the base OS is installed. %setup touch ${SINGULARITY_ROOTFS}/tacos.txt echo \"I love avocado\" >> avocados.txt # copy files from your host system into the container %files avocados.txt /opt %environment export NAME=avocado ## executed within the container after the base OS is installed at build time #install new software and libraries, config files, directories, etc %post echo 'export Avocado=TRUE' >> $SINGULARITY_ENVIRONMENT ## executed when the container image is run: singularity run %runscript echo \"Hello! Arguments received: $* \\n\" exec echo \"$@\" More information of singularity recipes Singularity Build Rewritable Sandbox Can be built from a recipe or existing container Used to develop, test, and make changes, then build or convert it into a standard image sudo singularity build --sandbox gccbox docker://gcc:7.2.0 sudo singularity build --sandbox test-box Singularity When you want to alter your image, you can use commands like shell, exec, run, with the --writable option sudo singularity shell --writable test-box Convert a sandbox to an immutable final container: sudo singularity build test-box.sif test-box Inspect Containers To check how a image is built, running script and environment variables.. singularity inspect [options] image_name --labels --runscript --deffile --environment e.g. singularity inspect --deffile mycontainer.sif Singularity Python (spython) Python API for Singularity containers Convert Dockerfile to Singularity def spython recipe Dockerfile > dock2sif.def Run Singularity Containers on Lawrencium File transfer to LRC cluster scp xxx.sif $USER@lrc-xfer.lbl.gov:/your/path/on/cluster Run your container interactively Request an interactive compute node singularity shell/run/exec container.sif Submit a slurm job Job Submission Example #!/bin/bash -l #SBATCH --job-name=container-test #SBATCH --partition=lr5 #SBATCH --account=ac_xxx #SBATCH --qos=lr_normal #SBATCH --nodes=1 #SBATCH --time=1-2:0:0 cd $SLURM_SUBMIT_DIR singularity exec mycontainer.sif cat /etc/os-release Container Bind Path Singularity allow mapping directories on host to directories within container Easy data access within containers System-defined bind paths on LRC /global/home/users/ /global/scratch/ User can define own bind paths: e.g.: mount /host/path/ on the host to /container/path inside the container -B /host/path/:/container/path singularity run --nv -B /global/home/users/$USER:/tmp pytorch_19_12_py3.sif ls /tmp Run GPU and MPI Containers Singularity supports NVIDIA\u2019s CUDA GPU compute framework or AMD\u2019s ROCm solution --nv enables NVIDIA GPU support in Singularity Remember to request a GPU node from the ES1 partition singularity exec --nv pytorch_19_12_py3.sif python -c \"import torch; print(torch.__version__)\" 1.4.0a0+a5b4d78 Run MPI Containers If launch on multiple nodes, MPI libraries on the host and inside the container need to match If launch on one node, only MPI library inside the container is called Exercise 1) singularity pull hello-world.sif shub://singularityhub/hello-world 2) generate hello-world.def or generate a sandbox to start with 3) add /data directory inside the container 4) build a new image hello-world-new.sif 5) bind /home/$USER on host to /data inside container 6) ls /data inside the new container Getting help Virtual Office Hours: Time: 10:30am - noon (Wednesdays) Request online Sending us tickets at hpcshelp@lbl.gov More information, documents, tips of how to use LBNL Supercluster http://scs.lbl.gov/ DLab consulting: https://dlab.berkeley.edu/consulting Please fill out Training Survey to get your comments and help us improve.","title":"Container 101 on Lawrencium"},{"location":"container-101/#outline","text":"Container technology overview Build singularity containers Run singularity containers on Lawrencium","title":"Outline"},{"location":"container-101/#containerization","text":"Standardized packaging of software and dependencies Portable, shareable, and reproducible. Your application brings its environment with it. Share the same OS kernel","title":"Containerization"},{"location":"container-101/#applications","text":"Package an analysis pipeline so that it runs on your laptop, in the cloud, and in HPC environment to produce the same result. Publish a paper and include a link to a container with all of the data and software that you used so that others can easily reproduce your results. Install and run an application that requires a complicated stack of dependencies Legacy codes require outdated OS","title":"Applications"},{"location":"container-101/#container-vs-virtual-machine","text":"","title":"Container vs. Virtual Machine"},{"location":"container-101/#singularity-technology","text":"Open-source computer software that encapsulates an application and all its dependencies into a single image Bring containers and reproducibility to scientific computing and HPC Developed by Greg Kurtzer Typically users have a build system as root users, but may not be root users on a production system","title":"Singularity Technology"},{"location":"container-101/#docker","text":"Bring containerization to the community-scale Rich image repository Widely used by scientific communities Compose for defining multi-container, recipe/definition file to build docker images Security concerns not ideal for the HPC environment Learn more docker","title":"Docker"},{"location":"container-101/#singularity-workflow","text":"Install Singularity on a local machine Build Singularity images locally with a root permission Transfer images to LRC clusters Run containers on the cluster Root privilege is not permitted","title":"Singularity Workflow"},{"location":"container-101/#singularity-installation","text":"OS platforms - Linux - Mac - Window Refer to instructions here for details Test your installation: $ singularity --version singularity version 3.2.1-1 $ singularity run docker://godlovedc/lolcow ______________________________________ / A tall, dark stranger will have more \\ \\ fun than you. / -------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Singularity Installation"},{"location":"container-101/#create-singularity-containers","text":"Build directly from pre-built docker images Docker hub Sylabs Cloud and Singularity hub More involved using docker images from other external resources Nvidia HPC containers Biocontainers AWS Build using definition files or recipes","title":"Create Singularity Containers"},{"location":"container-101/#singularity-pull-directly-from-docker-shub","text":"No root/sudo privilege is needed Create/download immutable squashfs images/containers singularity pull --help Docker Hub: Pull a container from Docker Hub. $ singularity pull docker://ubuntu:18.04 $ singularity pull docker://gcc:7.2.0 Singularity Hub: If no tag is specified, the master branch of the repository is used $ singularity pull hello-world.sif shub://singularityhub/hello-world","title":"Singularity pull directly from docker:// &amp; shub://"},{"location":"container-101/#run-singularity-with-shell-run-exec","text":"shell sub-command: invokes an interactive shell within a container singularity shell hello-world.sif run sub-command: executes the container\u2019s runscript singularity run hello-world.sif exec sub-command: execute an arbitrary command within container singularity exec hello-world.sif cat /etc/os-release","title":"Run Singularity with shell, run, exec"},{"location":"container-101/#singularity-pull-when-docker-containers-provided-by-other-external-docker-repositories","text":"Nvidia HPC containers Biocontainers AWS","title":"Singularity pull when Docker Containers Provided by Other External docker repositories"},{"location":"container-101/#steps-to-build-singularity-containers-from-ngc","text":"Pull a docker image locally (e.g. pgi compiler from NGC) $ docker pull nvcr.io/hpc/pgi-compilers:ce ..... $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nvcr.io/hpc/pgi-compiler ce c13ce6cf7f66 6 months ago 9.9GB openmpi3.1 latest 08a5518bb344 9 months ago 14.3GB registry 2 f32a97de94e1 15 months ago 25.8MB ...","title":"Steps to build singularity containers from NGC:"},{"location":"container-101/#steps-to-build-singularity-containers-from-ngc_1","text":"Push to the Docker Hub (docker login) or simply use your local docker images Singularity build from local registry $ singularity build pgi.sif docker-daemon://nvcr.io/hpc/pgi-compilers:ce $ ls Dokerfile hello-world.sif pgi.sif saxpy.c Singularity Compile OpenACC code $ singularity exec pgi.sif pgcc -o saxpy -acc -Minfo saxpy.c saxpy: 9, Loop is parallelizable Generating Tesla code 9, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */ 9, FMA (fused multiply-add) instruction(s) generated ... Execute binary ./saxpy $ singularity exec pgi.sif ./saxpy y[0] = 2.000000 Another example of using docker containers from AWS","title":"Steps to build singularity containers from NGC:"},{"location":"container-101/#singularity-build","text":"Root/sudo privilege is needed singularity build --help Build from a definition file sudo singularity --debug build mycontainer.sif Singularity","title":"Singularity build"},{"location":"container-101/#definition-filerecipe","text":"Bootstrap: docker #library, docker, shub, localimage, yum, debootstrap, arch, busybox, zypper From: ubuntu ## used singularity run-help %help Hello. I'm in the container. ## executed on host after the base OS is installed. %setup touch ${SINGULARITY_ROOTFS}/tacos.txt echo \"I love avocado\" >> avocados.txt # copy files from your host system into the container %files avocados.txt /opt %environment export NAME=avocado ## executed within the container after the base OS is installed at build time #install new software and libraries, config files, directories, etc %post echo 'export Avocado=TRUE' >> $SINGULARITY_ENVIRONMENT ## executed when the container image is run: singularity run %runscript echo \"Hello! Arguments received: $* \\n\" exec echo \"$@\" More information of singularity recipes","title":"Definition File/Recipe"},{"location":"container-101/#singularity-build-rewritable-sandbox","text":"Can be built from a recipe or existing container Used to develop, test, and make changes, then build or convert it into a standard image sudo singularity build --sandbox gccbox docker://gcc:7.2.0 sudo singularity build --sandbox test-box Singularity When you want to alter your image, you can use commands like shell, exec, run, with the --writable option sudo singularity shell --writable test-box Convert a sandbox to an immutable final container: sudo singularity build test-box.sif test-box","title":"Singularity Build Rewritable Sandbox"},{"location":"container-101/#inspect-containers","text":"To check how a image is built, running script and environment variables.. singularity inspect [options] image_name --labels --runscript --deffile --environment e.g. singularity inspect --deffile mycontainer.sif","title":"Inspect Containers"},{"location":"container-101/#singularity-python-spython","text":"Python API for Singularity containers Convert Dockerfile to Singularity def spython recipe Dockerfile > dock2sif.def","title":"Singularity Python (spython)"},{"location":"container-101/#run-singularity-containers-on-lawrencium","text":"File transfer to LRC cluster scp xxx.sif $USER@lrc-xfer.lbl.gov:/your/path/on/cluster Run your container interactively Request an interactive compute node singularity shell/run/exec container.sif Submit a slurm job","title":"Run Singularity Containers on Lawrencium"},{"location":"container-101/#job-submission-example","text":"#!/bin/bash -l #SBATCH --job-name=container-test #SBATCH --partition=lr5 #SBATCH --account=ac_xxx #SBATCH --qos=lr_normal #SBATCH --nodes=1 #SBATCH --time=1-2:0:0 cd $SLURM_SUBMIT_DIR singularity exec mycontainer.sif cat /etc/os-release","title":"Job Submission Example"},{"location":"container-101/#container-bind-path","text":"Singularity allow mapping directories on host to directories within container Easy data access within containers System-defined bind paths on LRC /global/home/users/ /global/scratch/ User can define own bind paths: e.g.: mount /host/path/ on the host to /container/path inside the container -B /host/path/:/container/path singularity run --nv -B /global/home/users/$USER:/tmp pytorch_19_12_py3.sif ls /tmp","title":"Container Bind Path"},{"location":"container-101/#run-gpu-and-mpi-containers","text":"Singularity supports NVIDIA\u2019s CUDA GPU compute framework or AMD\u2019s ROCm solution --nv enables NVIDIA GPU support in Singularity Remember to request a GPU node from the ES1 partition singularity exec --nv pytorch_19_12_py3.sif python -c \"import torch; print(torch.__version__)\" 1.4.0a0+a5b4d78","title":"Run GPU and MPI Containers"},{"location":"container-101/#run-mpi-containers","text":"If launch on multiple nodes, MPI libraries on the host and inside the container need to match If launch on one node, only MPI library inside the container is called","title":"Run MPI Containers"},{"location":"container-101/#exercise","text":"1) singularity pull hello-world.sif shub://singularityhub/hello-world 2) generate hello-world.def or generate a sandbox to start with 3) add /data directory inside the container 4) build a new image hello-world-new.sif 5) bind /home/$USER on host to /data inside container 6) ls /data inside the new container","title":"Exercise"},{"location":"container-101/#getting-help","text":"Virtual Office Hours: Time: 10:30am - noon (Wednesdays) Request online Sending us tickets at hpcshelp@lbl.gov More information, documents, tips of how to use LBNL Supercluster http://scs.lbl.gov/ DLab consulting: https://dlab.berkeley.edu/consulting Please fill out Training Survey to get your comments and help us improve.","title":"Getting help"},{"location":"data/","text":"Data Management & Storage Science IT can help you design the right data management solution supporting your research, and we have a wide variety of options supporting scientific data collection, storage, and analytics. These include: Our Science Project Storage Service ($25/Mo/TB) for performance data storage on demand, including free data movement and migration Our Compellent and Lawrencium storage condo option for those storing fixed amounts of data for longer than 6 months Access to cloud storage and assistance with local storage provision Help with data migration, including Globus and unlimited, no cost GDrive data storage","title":"Data Management & Storage"},{"location":"data/#data-management-storage","text":"Science IT can help you design the right data management solution supporting your research, and we have a wide variety of options supporting scientific data collection, storage, and analytics. These include: Our Science Project Storage Service ($25/Mo/TB) for performance data storage on demand, including free data movement and migration Our Compellent and Lawrencium storage condo option for those storing fixed amounts of data for longer than 6 months Access to cloud storage and assistance with local storage provision Help with data migration, including Globus and unlimited, no cost GDrive data storage","title":"Data Management &amp; Storage"},{"location":"hpc/","text":"High-Performance Computing Science IT supports and operates LBL's institutional high performance compute resource called Lawrencium. Lawrencium is a 55,000 core cluster which consist of shared nodes and PI-contributed condo nodes currently supporting compute needs across virtually every scientific discipline studied at LBL. For account information (including access to our PI Grant program) please see scs.lbl.gov. Support for both shared and private compute node hosting GPU nodes available now (Dual V100 with NVLink or Quad GTX1080Ti nodes) supporting machine learning and other needs High Speed Network connections between nodes - FDR & EDR Infiniband interconnect Users may also directly purchase service units ($0.01) on-demand","title":"High-Performance Computing"},{"location":"hpc/#high-performance-computing","text":"Science IT supports and operates LBL's institutional high performance compute resource called Lawrencium. Lawrencium is a 55,000 core cluster which consist of shared nodes and PI-contributed condo nodes currently supporting compute needs across virtually every scientific discipline studied at LBL. For account information (including access to our PI Grant program) please see scs.lbl.gov. Support for both shared and private compute node hosting GPU nodes available now (Dual V100 with NVLink or Quad GTX1080Ti nodes) supporting machine learning and other needs High Speed Network connections between nodes - FDR & EDR Infiniband interconnect Users may also directly purchase service units ($0.01) on-demand","title":"High-Performance Computing"},{"location":"jupyter_gpu/","text":"Jupyter Notebook on Lawrencium GPU node https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/1.12.0-py36 $ start_jupyter.py Login from visualization node via VNC Viewer: https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.VERSION) 1.12.0 print(date.today()) 2020-04-30 Test if TF can access a GPU tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True Print the name of the GPU device tf.test.gpu_device_name() '/device:GPU:0' Run the code on a GPU or CPU device_name = tf.test.gpu_device_name() #device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750466000.0 Shape: (3000, 3000) Device: /device:GPU:0 Time taken: 0:00:00.044407 #device_name = tf.test.gpu_device_name() device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750241300.0 Shape: (3000, 3000) Device: /device:CPU:0 Time taken: 0:00:00.435257","title":"TensorFlow 1.12"},{"location":"jupyter_gpu/#jupyter-notebook-on-lawrencium-gpu-node","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/1.12.0-py36 $ start_jupyter.py","title":"Jupyter Notebook on Lawrencium GPU node"},{"location":"jupyter_gpu/#login-from-visualization-node-via-vnc-viewer","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.VERSION) 1.12.0 print(date.today()) 2020-04-30","title":"Login from visualization node via VNC Viewer:"},{"location":"jupyter_gpu/#test-if-tf-can-access-a-gpu","text":"tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True","title":"Test if TF can access a GPU"},{"location":"jupyter_gpu/#print-the-name-of-the-gpu-device","text":"tf.test.gpu_device_name() '/device:GPU:0'","title":"Print the name of the GPU device"},{"location":"jupyter_gpu/#run-the-code-on-a-gpu-or-cpu","text":"device_name = tf.test.gpu_device_name() #device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750466000.0 Shape: (3000, 3000) Device: /device:GPU:0 Time taken: 0:00:00.044407 #device_name = tf.test.gpu_device_name() device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750241300.0 Shape: (3000, 3000) Device: /device:CPU:0 Time taken: 0:00:00.435257","title":"Run the code on a GPU or CPU"},{"location":"jupyter_gpu_tensorflow_2.0/","text":"Jupyter Notebook on Lawrencium GPU node https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/2.0-py36 $ start_jupyter.py Login from visualization node via VNC Viewer: https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.compat.v1.VERSION) 2.0.0 print(date.today()) 2020-05-11 Test if TF can access a GPU tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True Print the name of the GPU device tf.test.gpu_device_name() '/device:GPU:0'","title":"TensorFlow 2.0"},{"location":"jupyter_gpu_tensorflow_2.0/#jupyter-notebook-on-lawrencium-gpu-node","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/2.0-py36 $ start_jupyter.py","title":"Jupyter Notebook on Lawrencium GPU node"},{"location":"jupyter_gpu_tensorflow_2.0/#login-from-visualization-node-via-vnc-viewer","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.compat.v1.VERSION) 2.0.0 print(date.today()) 2020-05-11","title":"Login from visualization node via VNC Viewer:"},{"location":"jupyter_gpu_tensorflow_2.0/#test-if-tf-can-access-a-gpu","text":"tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True","title":"Test if TF can access a GPU"},{"location":"jupyter_gpu_tensorflow_2.0/#print-the-name-of-the-gpu-device","text":"tf.test.gpu_device_name() '/device:GPU:0'","title":"Print the name of the GPU device"},{"location":"jupyter_gpu_tensorflow_2.1/","text":"Jupyter Notebook on Lawrencium GPU node https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/2.1.0-py37 $ start_jupyter.py Login from visualization node via VNC Viewer: https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0] print(tf.compat.v1.VERSION) 2.1.0 print(date.today()) 2020-05-13 Test if TF can access a GPU tf.config.list_physical_devices('GPU') [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')] Print the name of the GPU device tf.test.gpu_device_name() '/device:GPU:0'","title":"TensorFlow 2.1"},{"location":"jupyter_gpu_tensorflow_2.1/#jupyter-notebook-on-lawrencium-gpu-node","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/2.1.0-py37 $ start_jupyter.py","title":"Jupyter Notebook on Lawrencium GPU node"},{"location":"jupyter_gpu_tensorflow_2.1/#login-from-visualization-node-via-vnc-viewer","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0] print(tf.compat.v1.VERSION) 2.1.0 print(date.today()) 2020-05-13","title":"Login from visualization node via VNC Viewer:"},{"location":"jupyter_gpu_tensorflow_2.1/#test-if-tf-can-access-a-gpu","text":"tf.config.list_physical_devices('GPU') [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]","title":"Test if TF can access a GPU"},{"location":"jupyter_gpu_tensorflow_2.1/#print-the-name-of-the-gpu-device","text":"tf.test.gpu_device_name() '/device:GPU:0'","title":"Print the name of the GPU device"},{"location":"jupyterhub_gpu/","text":"Jupyter Notebook via Jupyterhub https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook select ES1 partition for GPU Select a TensorFlow kernel : Python3.6 TF-1.12 import sys import os import getpass import numpy as np from datetime import datetime from datetime import date system('hostname') ['n0009.es1'] import tensorflow as tf print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.VERSION) 1.12.0 print(date.today()) 2020-05-21 Test if TF can access a GPU tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True Print the name of the GPU device tf.test.gpu_device_name() '/device:GPU:0' Run the code on a GPU or CPU device_name = tf.test.gpu_device_name() #device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750205000.0 Shape: (3000, 3000) Device: /device:GPU:0 Time taken: 0:00:00.043736 #device_name = tf.test.gpu_device_name() device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6749143000.0 Shape: (3000, 3000) Device: /device:CPU:0 Time taken: 0:00:00.765788","title":"JupyterHub"},{"location":"jupyterhub_gpu/#jupyter-notebook-via-jupyterhub","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook select ES1 partition for GPU Select a TensorFlow kernel : Python3.6 TF-1.12 import sys import os import getpass import numpy as np from datetime import datetime from datetime import date system('hostname') ['n0009.es1'] import tensorflow as tf print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.VERSION) 1.12.0 print(date.today()) 2020-05-21","title":"Jupyter Notebook via Jupyterhub"},{"location":"jupyterhub_gpu/#test-if-tf-can-access-a-gpu","text":"tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True","title":"Test if TF can access a GPU"},{"location":"jupyterhub_gpu/#print-the-name-of-the-gpu-device","text":"tf.test.gpu_device_name() '/device:GPU:0' Run the code on a GPU or CPU device_name = tf.test.gpu_device_name() #device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750205000.0 Shape: (3000, 3000) Device: /device:GPU:0 Time taken: 0:00:00.043736 #device_name = tf.test.gpu_device_name() device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6749143000.0 Shape: (3000, 3000) Device: /device:CPU:0 Time taken: 0:00:00.765788","title":"Print the name of the GPU device"},{"location":"singularity_docker/","text":"Text Classification Using AWS Deep Learning Docker Containers Outline 1) Building the Singularity container using available AWS Deep-Learning docker containers 2) Local Test 3) Train text classifier on Lawrencium: Upload the Singularity image and training data Run the Singularity image on compute nodes Building the Singularity container using available AWS Deep-Learning docker containers AWS Deep-Learning images The following shell code shows how to build the container image using docker and convert the container image to a Singularity image. Download the GitHub repository for this tutorial %%sh git clone https://github.com/lbnl-science-it/singularity_aws_dl_container.git cd singularity_aws_dl_container Download and unzip the dataset %%sh cd container #################################################### ########## Download and unzip the dataset ########## #################################################### cd ../data/ wget https://danilop.s3-eu-west-1.amazonaws.com/reInvent-Workshop-Data-Backup.zip && unzip reInvent-Workshop-Data-Backup.zip mv reInvent-Workshop-Data-Backup/* ./ rm -rf reInvent-Workshop-Data-Backup reInvent-Workshop-Data-Backup.zip cd ../container/ Build the SageMaker Container & Convert it to Singularity image %%sh cd container ################################################################################### ######### Build the SageMaker Container & Convert it to Singularity image ######### ################################################################################### algorithm_name=sagemaker-keras-text-classification chmod +x sagemaker_keras_text_classification/train chmod +x sagemaker_keras_text_classification/serve ## Get the region defined in the current configuration region=$(aws configure get region) fullname=\"local_${algorithm_name}:latest\" ## Get the login command from ECR and execute it directly $(aws ecr get-login --no-include-email --region ${region} --registry-ids 763104351884) ## Build the docker image locally with the image name ## In the \"Dockerfile\", modify the source image to select one of the available deep learning docker containers images: ## https://aws.amazon.com/releasenotes/available-deep-learning-containers-images docker build -t ${algorithm_name} . docker tag ${algorithm_name} ${fullname} ## Build Singularity image from local docker image sifname=\"local_sagemaker-keras-text-classification.sif\" sudo singularity build ${sifname} docker-daemon:${fullname} Login Succeeded Sending build context to Docker daemon 456.3MB Step 1/9 : FROM 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:1.14.0-cpu-py36-ubuntu16.04 ---> e6a210ff54e4 Step 2/9 : RUN apt-get update && apt-get install -y nginx imagemagick graphviz ---> Using cache ---> 32ff2dce1af3 Step 3/9 : RUN pip install --upgrade pip ---> Using cache ---> 4e1b65ea3a65 Step 4/9 : RUN pip install gevent gunicorn flask tensorflow_hub seqeval graphviz nltk spacy tqdm ---> Using cache ---> d97c22f6de86 Step 5/9 : RUN python -m spacy download en_core_web_sm ---> Using cache ---> 14c8854a1901 Step 6/9 : RUN python -m spacy download en ---> Using cache ---> 185661d9e15d Step 7/9 : ENV PATH=\"/opt/program:${PATH}\" ---> Using cache ---> b5d5c6867074 Step 8/9 : COPY sagemaker_keras_text_classification /opt/program ---> Using cache ---> ac73b50bd646 Step 9/9 : WORKDIR /opt/program ---> Using cache ---> c5fe52a83024 Successfully built c5fe52a83024 Successfully tagged sagemaker-keras-text-classification:latest \u001b[34mINFO: \u001b[0m Starting build... Getting image source signatures Copying blob sha256:87e513ddb4a6ce37dabf3de74b0284d49e08f4d7a3f0de393e6a533577e00f11 ... Copying config sha256:77b2a54a3da8891391f609455182127c0944edb40397fbaf24f9ec80a9be5460 Writing manifest to image destination Storing signatures 2020/06/08 22:14:47 info unpack layer: sha256:647dce8a9de5ada5719e82c2ff5408867fcaa83145665bea4103d3705c2326b1 ... 2020/06/08 22:14:49 info unpack layer: sha256:1df727cf7f1435f496890edded1650193af403065eff27929a8b374d5b36d743 2020/06/08 22:14:49 info unpack layer: sha256:df2ccfca12a78a5c880fd30514c57c84f250a81c223915e124cff93833f6b5d2 2020/06/08 22:14:49 info unpack layer: sha256:88f2c64e66817e60a415e82323d1a2d3f19ca75eb4ea9ae7692a2fccc09c2de5 \u001b[34mINFO: \u001b[0m Creating SIF file... \u001b[34mINFO: \u001b[0m Build complete: local_sagemaker-keras-text-classification.sif Training Text Classifier %%sh cd container ################################ ########## Local Test ########## ################################ cd ../data cp -a . ../container/local_test/test_dir/input/data/training/ cd ../container cd local_test ### Train sifname=\"local_sagemaker-keras-text-classification.sif\" ./train_local.sh ../${sifname} Starting the training. TITLE ... TIMESTAMP 1 Fed official says weak data caused by weather,... ... 1394470370698 2 Fed's Charles Plosser sees high bar for change... ... 1394470371207 3 US open: Stocks fall after Fed official hints ... ... 1394470371550 4 Fed risks falling 'behind the curve', Charles ... ... 1394470371793 5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... ... 1394470372027 [5 rows x 7 columns] Found 65990 unique tokens. Shape of data tensor: (422417, 100) Shape of label tensor: (422417, 4) x_train shape: (337933, 100) Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 100) 1000000 _________________________________________________________________ flatten (Flatten) (None, 10000) 0 _________________________________________________________________ dense (Dense) (None, 2) 20002 _________________________________________________________________ dense_1 (Dense) (None, 4) 12 ================================================================= Total params: 1,020,014 Trainable params: 1,020,014 Non-trainable params: 0 _________________________________________________________________ Train on 337933 samples, validate on 84484 samples Epoch 1/6 337933/337933 - 25s - loss: 0.6788 - acc: 0.7409 - val_loss: 0.6146 - val_acc: 0.7757 Epoch 2/6 337933/337933 - 25s - loss: 0.5958 - acc: 0.7824 - val_loss: 0.5889 - val_acc: 0.7840 Epoch 3/6 337933/337933 - 25s - loss: 0.5778 - acc: 0.7882 - val_loss: 0.5755 - val_acc: 0.7893 Epoch 4/6 337933/337933 - 25s - loss: 0.5707 - acc: 0.7904 - val_loss: 0.5697 - val_acc: 0.7918 Epoch 5/6 337933/337933 - 25s - loss: 0.5673 - acc: 0.7918 - val_loss: 0.5684 - val_acc: 0.7915 Epoch 6/6 337933/337933 - 25s - loss: 0.5648 - acc: 0.7920 - val_loss: 0.5657 - val_acc: 0.7923 Training complete. Now saving model to: /opt/ml/model Test headline: What Improved Tech Means for Electric, Self-Driving and Flying Cars Predicted category: t Data Exploration import pandas as pd import tensorflow as tf import re import numpy as np import os from tensorflow.python.keras.preprocessing.text import Tokenizer from tensorflow.python.keras.preprocessing.sequence import pad_sequences from tensorflow.python.keras.utils import to_categorical column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"] news_dataset = pd.read_csv(os.path.join('./data', 'newsCorpora.csv'), names=column_names, header=None, delimiter='\\t') news_dataset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP 1 Fed official says weak data caused by weather,... http://www.latimes.com/business/money/la-fi-mo... Los Angeles Times b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.latimes.com 1394470370698 2 Fed's Charles Plosser sees high bar for change... http://www.livemint.com/Politics/H2EvwJSK2VE6O... Livemint b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.livemint.com 1394470371207 3 US open: Stocks fall after Fed official hints ... http://www.ifamagazine.com/news/us-open-stocks... IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371550 4 Fed risks falling 'behind the curve', Charles ... http://www.ifamagazine.com/news/fed-risks-fall... IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371793 5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... http://www.moneynews.com/Economy/federal-reser... Moneynews b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.moneynews.com 1394470372027 news_dataset.groupby(['CATEGORY']).size() CATEGORY b 115967 e 152469 m 45639 t 108344 dtype: int64 Local Test cd container sh build_singularity_local_test.sh Train text classifier on Lawrencium Upload the Singularity image and training data sftp lrc-xfer.lbl.gov put local_sagemaker-keras-text-classification.sif put -r local_test Run the Singularity image on Lawrencium compute-node ssh lrc-login.lbl.gov cd local_test srun -N 1 -p lr4 -A $ACCOUNT -t 1:0:0 -q lr_normal --pty bash sh train_local.sh ../local_sagemaker-keras-text-classification.sif References https://aws.amazon.com/releasenotes/available-deep-learning-containers-images https://github.com/aws-samples/amazon-sagemaker-keras-text-classification https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification https://sylabs.io/guides/3.5/user-guide/","title":"Build Singularity container using AWS DL docker images"},{"location":"singularity_docker/#text-classification-using-aws-deep-learning-docker-containers","text":"","title":"Text Classification Using AWS Deep Learning Docker Containers"},{"location":"singularity_docker/#outline","text":"1) Building the Singularity container using available AWS Deep-Learning docker containers 2) Local Test 3) Train text classifier on Lawrencium: Upload the Singularity image and training data Run the Singularity image on compute nodes","title":"Outline"},{"location":"singularity_docker/#building-the-singularity-container-using-available-aws-deep-learning-docker-containers","text":"AWS Deep-Learning images The following shell code shows how to build the container image using docker and convert the container image to a Singularity image.","title":"Building the Singularity container using available AWS Deep-Learning docker containers"},{"location":"singularity_docker/#download-the-github-repository-for-this-tutorial","text":"%%sh git clone https://github.com/lbnl-science-it/singularity_aws_dl_container.git cd singularity_aws_dl_container","title":"Download the GitHub repository for this tutorial"},{"location":"singularity_docker/#download-and-unzip-the-dataset","text":"%%sh cd container #################################################### ########## Download and unzip the dataset ########## #################################################### cd ../data/ wget https://danilop.s3-eu-west-1.amazonaws.com/reInvent-Workshop-Data-Backup.zip && unzip reInvent-Workshop-Data-Backup.zip mv reInvent-Workshop-Data-Backup/* ./ rm -rf reInvent-Workshop-Data-Backup reInvent-Workshop-Data-Backup.zip cd ../container/","title":"Download and unzip the dataset"},{"location":"singularity_docker/#build-the-sagemaker-container-convert-it-to-singularity-image","text":"%%sh cd container ################################################################################### ######### Build the SageMaker Container & Convert it to Singularity image ######### ################################################################################### algorithm_name=sagemaker-keras-text-classification chmod +x sagemaker_keras_text_classification/train chmod +x sagemaker_keras_text_classification/serve ## Get the region defined in the current configuration region=$(aws configure get region) fullname=\"local_${algorithm_name}:latest\" ## Get the login command from ECR and execute it directly $(aws ecr get-login --no-include-email --region ${region} --registry-ids 763104351884) ## Build the docker image locally with the image name ## In the \"Dockerfile\", modify the source image to select one of the available deep learning docker containers images: ## https://aws.amazon.com/releasenotes/available-deep-learning-containers-images docker build -t ${algorithm_name} . docker tag ${algorithm_name} ${fullname} ## Build Singularity image from local docker image sifname=\"local_sagemaker-keras-text-classification.sif\" sudo singularity build ${sifname} docker-daemon:${fullname} Login Succeeded Sending build context to Docker daemon 456.3MB Step 1/9 : FROM 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:1.14.0-cpu-py36-ubuntu16.04 ---> e6a210ff54e4 Step 2/9 : RUN apt-get update && apt-get install -y nginx imagemagick graphviz ---> Using cache ---> 32ff2dce1af3 Step 3/9 : RUN pip install --upgrade pip ---> Using cache ---> 4e1b65ea3a65 Step 4/9 : RUN pip install gevent gunicorn flask tensorflow_hub seqeval graphviz nltk spacy tqdm ---> Using cache ---> d97c22f6de86 Step 5/9 : RUN python -m spacy download en_core_web_sm ---> Using cache ---> 14c8854a1901 Step 6/9 : RUN python -m spacy download en ---> Using cache ---> 185661d9e15d Step 7/9 : ENV PATH=\"/opt/program:${PATH}\" ---> Using cache ---> b5d5c6867074 Step 8/9 : COPY sagemaker_keras_text_classification /opt/program ---> Using cache ---> ac73b50bd646 Step 9/9 : WORKDIR /opt/program ---> Using cache ---> c5fe52a83024 Successfully built c5fe52a83024 Successfully tagged sagemaker-keras-text-classification:latest \u001b[34mINFO: \u001b[0m Starting build... Getting image source signatures Copying blob sha256:87e513ddb4a6ce37dabf3de74b0284d49e08f4d7a3f0de393e6a533577e00f11 ... Copying config sha256:77b2a54a3da8891391f609455182127c0944edb40397fbaf24f9ec80a9be5460 Writing manifest to image destination Storing signatures 2020/06/08 22:14:47 info unpack layer: sha256:647dce8a9de5ada5719e82c2ff5408867fcaa83145665bea4103d3705c2326b1 ... 2020/06/08 22:14:49 info unpack layer: sha256:1df727cf7f1435f496890edded1650193af403065eff27929a8b374d5b36d743 2020/06/08 22:14:49 info unpack layer: sha256:df2ccfca12a78a5c880fd30514c57c84f250a81c223915e124cff93833f6b5d2 2020/06/08 22:14:49 info unpack layer: sha256:88f2c64e66817e60a415e82323d1a2d3f19ca75eb4ea9ae7692a2fccc09c2de5 \u001b[34mINFO: \u001b[0m Creating SIF file... \u001b[34mINFO: \u001b[0m Build complete: local_sagemaker-keras-text-classification.sif","title":"Build the SageMaker Container &amp; Convert it to Singularity image"},{"location":"singularity_docker/#training-text-classifier","text":"%%sh cd container ################################ ########## Local Test ########## ################################ cd ../data cp -a . ../container/local_test/test_dir/input/data/training/ cd ../container cd local_test ### Train sifname=\"local_sagemaker-keras-text-classification.sif\" ./train_local.sh ../${sifname} Starting the training. TITLE ... TIMESTAMP 1 Fed official says weak data caused by weather,... ... 1394470370698 2 Fed's Charles Plosser sees high bar for change... ... 1394470371207 3 US open: Stocks fall after Fed official hints ... ... 1394470371550 4 Fed risks falling 'behind the curve', Charles ... ... 1394470371793 5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... ... 1394470372027 [5 rows x 7 columns] Found 65990 unique tokens. Shape of data tensor: (422417, 100) Shape of label tensor: (422417, 4) x_train shape: (337933, 100) Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 100) 1000000 _________________________________________________________________ flatten (Flatten) (None, 10000) 0 _________________________________________________________________ dense (Dense) (None, 2) 20002 _________________________________________________________________ dense_1 (Dense) (None, 4) 12 ================================================================= Total params: 1,020,014 Trainable params: 1,020,014 Non-trainable params: 0 _________________________________________________________________ Train on 337933 samples, validate on 84484 samples Epoch 1/6 337933/337933 - 25s - loss: 0.6788 - acc: 0.7409 - val_loss: 0.6146 - val_acc: 0.7757 Epoch 2/6 337933/337933 - 25s - loss: 0.5958 - acc: 0.7824 - val_loss: 0.5889 - val_acc: 0.7840 Epoch 3/6 337933/337933 - 25s - loss: 0.5778 - acc: 0.7882 - val_loss: 0.5755 - val_acc: 0.7893 Epoch 4/6 337933/337933 - 25s - loss: 0.5707 - acc: 0.7904 - val_loss: 0.5697 - val_acc: 0.7918 Epoch 5/6 337933/337933 - 25s - loss: 0.5673 - acc: 0.7918 - val_loss: 0.5684 - val_acc: 0.7915 Epoch 6/6 337933/337933 - 25s - loss: 0.5648 - acc: 0.7920 - val_loss: 0.5657 - val_acc: 0.7923 Training complete. Now saving model to: /opt/ml/model Test headline: What Improved Tech Means for Electric, Self-Driving and Flying Cars Predicted category: t","title":"Training Text Classifier"},{"location":"singularity_docker/#data-exploration","text":"import pandas as pd import tensorflow as tf import re import numpy as np import os from tensorflow.python.keras.preprocessing.text import Tokenizer from tensorflow.python.keras.preprocessing.sequence import pad_sequences from tensorflow.python.keras.utils import to_categorical column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"] news_dataset = pd.read_csv(os.path.join('./data', 'newsCorpora.csv'), names=column_names, header=None, delimiter='\\t') news_dataset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP 1 Fed official says weak data caused by weather,... http://www.latimes.com/business/money/la-fi-mo... Los Angeles Times b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.latimes.com 1394470370698 2 Fed's Charles Plosser sees high bar for change... http://www.livemint.com/Politics/H2EvwJSK2VE6O... Livemint b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.livemint.com 1394470371207 3 US open: Stocks fall after Fed official hints ... http://www.ifamagazine.com/news/us-open-stocks... IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371550 4 Fed risks falling 'behind the curve', Charles ... http://www.ifamagazine.com/news/fed-risks-fall... IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371793 5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... http://www.moneynews.com/Economy/federal-reser... Moneynews b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.moneynews.com 1394470372027 news_dataset.groupby(['CATEGORY']).size() CATEGORY b 115967 e 152469 m 45639 t 108344 dtype: int64","title":"Data Exploration"},{"location":"singularity_docker/#local-test","text":"cd container sh build_singularity_local_test.sh","title":"Local Test"},{"location":"singularity_docker/#train-text-classifier-on-lawrencium","text":"Upload the Singularity image and training data sftp lrc-xfer.lbl.gov put local_sagemaker-keras-text-classification.sif put -r local_test Run the Singularity image on Lawrencium compute-node ssh lrc-login.lbl.gov cd local_test srun -N 1 -p lr4 -A $ACCOUNT -t 1:0:0 -q lr_normal --pty bash sh train_local.sh ../local_sagemaker-keras-text-classification.sif","title":"Train text classifier on Lawrencium"},{"location":"singularity_docker/#references","text":"https://aws.amazon.com/releasenotes/available-deep-learning-containers-images https://github.com/aws-samples/amazon-sagemaker-keras-text-classification https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification https://sylabs.io/guides/3.5/user-guide/","title":"References"},{"location":"sparc_on_gcp/","text":"Running SpaRC on Google Cloud Dataproc Building SpaRC: Open a new Cloud Shell and run: git clone https://github.com/shawfdong/sparc.git cd sparc wget https://piccolo.link/sbt-1.3.10.tgz tar xvzf sbt-1.3.10.tgz ./sbt/bin/sbt assembly A jar file should be created at: target/scala-2.11/LocalCluster-assembly-0.2.jar Upload Data to Google Cloud Storage Navigate to Storage and select Storage > Browser . Click Create Bucket . Specify your project name as the bucket name . Click Create . Copy the compiled SpaRC LocalCluster-assembly-0.2.jar and a sample input file sample_small.seq to the project bucket you just created, by running the below in Cloud Shell: gsutil cp target/scala-2.11/LocalCluster-assembly-0.2.jar gs://$DEVSHELL_PROJECT_ID cd data/small cp sample.seq sample_small.seq gsutil cp sample_small.seq gs://$DEVSHELL_PROJECT_ID Launch Dataproc Run SpaRC job on Dataproc In the Dataproc console, click Jobs . Click Submit job . For Job type , select Spark ; for Main class or jar and Jar files , specify the location of the SpaRC jar file you uploaded to your bucket. Your bucket-name is your project name : gs://<my-project-name>/LocalCluster-assembly-0.2.jar . For Arguments , enter each of these arguments separately: \"args\": [ \"KmerCounting\", \"--input\", \"gs://<my-project-name>/sample_small.seq\", \"--output\", \"test.log\", \"--kmer_length\", \"31\" ] For Properties , enter these Key-Value pairs separately: \"properties\": { \"spark.executor.extraClassPath\": \"gs://<my-project-name>/LocalCluster-assembly-0.2.jar\", \"spark.driver.maxResultSize\": \"8g\", \"spark.network.timeout\": \"360000\", \"spark.default.parallelism\": \"4\", \"spark.eventLog.enabled\": \"false\" } Click Submit","title":"SpaRC on GCP"},{"location":"sparc_on_gcp/#running-sparc-on-google-cloud-dataproc","text":"","title":"Running SpaRC on Google Cloud Dataproc"},{"location":"sparc_on_gcp/#building-sparc","text":"Open a new Cloud Shell and run: git clone https://github.com/shawfdong/sparc.git cd sparc wget https://piccolo.link/sbt-1.3.10.tgz tar xvzf sbt-1.3.10.tgz ./sbt/bin/sbt assembly A jar file should be created at: target/scala-2.11/LocalCluster-assembly-0.2.jar","title":"Building SpaRC:"},{"location":"sparc_on_gcp/#upload-data-to-google-cloud-storage","text":"Navigate to Storage and select Storage > Browser . Click Create Bucket . Specify your project name as the bucket name . Click Create . Copy the compiled SpaRC LocalCluster-assembly-0.2.jar and a sample input file sample_small.seq to the project bucket you just created, by running the below in Cloud Shell: gsutil cp target/scala-2.11/LocalCluster-assembly-0.2.jar gs://$DEVSHELL_PROJECT_ID cd data/small cp sample.seq sample_small.seq gsutil cp sample_small.seq gs://$DEVSHELL_PROJECT_ID","title":"Upload Data to Google Cloud Storage"},{"location":"sparc_on_gcp/#launch-dataproc","text":"","title":"Launch Dataproc"},{"location":"sparc_on_gcp/#run-sparc-job-on-dataproc","text":"In the Dataproc console, click Jobs . Click Submit job . For Job type , select Spark ; for Main class or jar and Jar files , specify the location of the SpaRC jar file you uploaded to your bucket. Your bucket-name is your project name : gs://<my-project-name>/LocalCluster-assembly-0.2.jar . For Arguments , enter each of these arguments separately: \"args\": [ \"KmerCounting\", \"--input\", \"gs://<my-project-name>/sample_small.seq\", \"--output\", \"test.log\", \"--kmer_length\", \"31\" ] For Properties , enter these Key-Value pairs separately: \"properties\": { \"spark.executor.extraClassPath\": \"gs://<my-project-name>/LocalCluster-assembly-0.2.jar\", \"spark.driver.maxResultSize\": \"8g\", \"spark.network.timeout\": \"360000\", \"spark.default.parallelism\": \"4\", \"spark.eventLog.enabled\": \"false\" } Click Submit","title":"Run SpaRC job on Dataproc"},{"location":"sparc_on_lawrencium/","text":"Running SpaRC on Lawrencium SpaRC is an Apache Spark-based scalable genomic sequence clustering application . SpaRC has been running successfully on AWS EMR , as well as on the Bridges supercomputer at PSC. In this tutorial, I describe how to run SpaRC on Lawrencium . Spark On Demand on Lawrencium Users can run Spark jobs on Lawrencium in Spark On Demand (SOD) fashion, in which a standalone Spark cluster will be created on demand in a Slurm job. Note that the Spark cluster will be running in standalone mode , so there will be no YARN cluster manager, nor HDFS. In lieu of HDFS, we'll use Lustre scratch for storage. As of this writing, there is only Spark 2.1.0 available on Lawrencium. We may install a more up-to-date version in the near future. Building SpaRC You'll build SpaRC against Spark 2.1.0. The source code of SpaRC is hosted on Bitbucket at https://bitbucket.org/LizhenShi/sparc . I don't have write access to the repo, so I imported it to GitHub, at https://github.com/shawfdong/sparc . I've added a new file build.sbt.spark2.1.0 to the GitHub repo, which, as the name suggests, will be used to build SpaRC against Spark 2.1.0. Note that you can't build SpaRC on the login nodes of Lawrencium, because rsync (which is required by sbt) is disabled there. You'll have to use the data transfer node lrc-xfer.lbl.gov : $ ssh lrc-xfer.lbl.gov Download and unpack the Scala build tool sbt : $ wget https://piccolo.link/sbt-1.3.10.tgz $ tar xvz sbt-1.3.10.tgz Load the module for JDK 1.8.0: $ export MODULEPATH=$MODULEPATH:/global/software/sl-7.x86_64/modfiles/langs $ module load java $ java -version java version \"1.8.0_121\" Java(TM) SE Runtime Environment (build 1.8.0_121-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) Clone and build SpaRC: $ git clone https://github.com/shawfdong/sparc.git $ cd sparc $ cp build.sbt.spark2.1.0 build.sbt $ ~/sbt/bin/sbt assembly This will create a fat jar file ~/sparc/target/scala-2.11/LocalCluster-assembly-0.2.jar . Copy it to your Lustre scratch space: $ cp target/scala-2.11/LocalCluster-assembly-0.2.jar /global/scratch/$USER/ While you are at it, also download a sample sequence data file: $ cd /global/scratch/$USER $ curl http://s3.amazonaws.com/share.jgi-ga.org/sparc_example/illumina_02G_merged_with_id.seq.gz -o sample.seq.gz $ gunzip sample.seq.gz $ wc -l sample.seq 6343345 sample.seq $ head -2 sample.seq 6 HISEQ13:204:C8T6VANXX:1:1101:3726:1992 NATATTCCCGTTCTGATATTGCGTTAAGTCGTTCCCCTAAGCCGGCCCTCCTTATCGAGCGCGCCGGCTTTTTTTGCCATGTTCAGCGAATCACAGGACAAGATACTTCACCTAACGTAGTAGATGGTTCTATGCTTAAGGGCAAGGTGTNTTAATCTCGATATCCGCCTGTTTTAATAAATCAGCGACGAAGCGATGGGAGGATAAGCGCTCGTCAAAAACCACGCGCTTTTTTTCTAAGGTGGGTAAGTTCAAGGTAACACCCCCACTATGCCTATGAGTGAATTGGTAACACCTTGCC 60 HISEQ13:204:C8T6VANXX:1:1101:4370:1919 NCGTGCGCCCATCTCCGTGGCTAAACAGCTTGAGGTGGAAATTCGCCAGTGGATACAGCAGCATGCAGCGACAGGCGGGCGTCGCCTCCCTTCGATACGCCATTTAGCAGCAACACATAACGTCAGCCGCAATGCAGTCATTGAAGCTTANGTAAGGTCTTCTCCTTCGCGCCAATCGTTAGGTAACCAGCCGCAGCCCAGTTTCAATGACTGTTCATCGGTGTTAAACACGCCCCATAAGCCATTCGTCACTTCTTCCAATGGCGTTGATGACGCGGGTTGAACCAGTTTCAGCGCGTTA Now you can exit from lrc-xfer.lbl.gov . Alternatively , you could build SpaRC on your local computer, then upload the assembled fat jar file to your Lustre scratch space on Lawrencium. Running SpaRC interactively SSH to a Lawrencium login node: $ ssh lrc-login.lbl.gov For demonstration purpose, request 2 nodes from the lr6 partition for the interactive job: $ cd /global/scratch/$USER $ srun -p lr6 --qos=lr_normal -N 2 -t 1:00:0 --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> --pty bash When the job starts, you'll be given exclusive allocation to 2 compute nodes in the lr6 partition, each one of which has 2x 16-core Intel Xeon Gold 6130 CPUs and 96 GB memory (so in total, there will be 64 cores and 192GB memory available in your Spark cluster). And you'll be dropped to a bash shell on one of the compute nodes. Start Spark On Demand (SOD): $ source /global/home/groups/allhands/bin/spark_helper.sh $ spark-start Run the first Spark job on SOD (you might want to tune the values for --executor-cores , --num-executors and --executor-memory ): $ SCRATCH=/global/scratch/$USER $ JAR=$SCRATCH/LocalCluster-assembly-0.2.jar $ OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" $ OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" $ spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C Run the second Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 Run the third Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 Run the fifth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 Once you are done, don't forget to stop Spark On Demand and exit from the interactive job: $ spark-stop $ exit Running SpaRC in batch mode To run SpaRC in batch mode, write a Slurm job script and call it sparc.slurm : #!/bin/bash #SBATCH --job-name=sparc #SBATCH --partition=lr6 #SBATCH --qos=lr_normal #SBATCH --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> #SBATCH --nodes=2 #SBATCH --time=01:00:00 source /global/home/groups/allhands/bin/spark_helper.sh # Start Spark On Demand spark-start SCRATCH=/global/scratch/$USER JAR=$SCRATCH/LocalCluster-assembly-0.2.jar OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" # 1st Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C # 2nd Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 # 3rd Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: # 4th Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 # 5th Spark job spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 # Stop Spark On Demand spark-stop Then submit the job with: sbatch sparc.slurm Known issues and future improvements Presumably, there is a switch -i to spark-start that would enable communications over the IPoIB network. But it doesn't work! Spark 2.1.0 is a bit old. References https://academic.oup.com/bioinformatics/article/35/5/760/5078476 https://peerj.com/articles/8966/","title":"SpaRC on Lawrencium"},{"location":"sparc_on_lawrencium/#running-sparc-on-lawrencium","text":"SpaRC is an Apache Spark-based scalable genomic sequence clustering application . SpaRC has been running successfully on AWS EMR , as well as on the Bridges supercomputer at PSC. In this tutorial, I describe how to run SpaRC on Lawrencium .","title":"Running SpaRC on Lawrencium"},{"location":"sparc_on_lawrencium/#spark-on-demand-on-lawrencium","text":"Users can run Spark jobs on Lawrencium in Spark On Demand (SOD) fashion, in which a standalone Spark cluster will be created on demand in a Slurm job. Note that the Spark cluster will be running in standalone mode , so there will be no YARN cluster manager, nor HDFS. In lieu of HDFS, we'll use Lustre scratch for storage. As of this writing, there is only Spark 2.1.0 available on Lawrencium. We may install a more up-to-date version in the near future.","title":"Spark On Demand on Lawrencium"},{"location":"sparc_on_lawrencium/#building-sparc","text":"You'll build SpaRC against Spark 2.1.0. The source code of SpaRC is hosted on Bitbucket at https://bitbucket.org/LizhenShi/sparc . I don't have write access to the repo, so I imported it to GitHub, at https://github.com/shawfdong/sparc . I've added a new file build.sbt.spark2.1.0 to the GitHub repo, which, as the name suggests, will be used to build SpaRC against Spark 2.1.0. Note that you can't build SpaRC on the login nodes of Lawrencium, because rsync (which is required by sbt) is disabled there. You'll have to use the data transfer node lrc-xfer.lbl.gov : $ ssh lrc-xfer.lbl.gov Download and unpack the Scala build tool sbt : $ wget https://piccolo.link/sbt-1.3.10.tgz $ tar xvz sbt-1.3.10.tgz Load the module for JDK 1.8.0: $ export MODULEPATH=$MODULEPATH:/global/software/sl-7.x86_64/modfiles/langs $ module load java $ java -version java version \"1.8.0_121\" Java(TM) SE Runtime Environment (build 1.8.0_121-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) Clone and build SpaRC: $ git clone https://github.com/shawfdong/sparc.git $ cd sparc $ cp build.sbt.spark2.1.0 build.sbt $ ~/sbt/bin/sbt assembly This will create a fat jar file ~/sparc/target/scala-2.11/LocalCluster-assembly-0.2.jar . Copy it to your Lustre scratch space: $ cp target/scala-2.11/LocalCluster-assembly-0.2.jar /global/scratch/$USER/ While you are at it, also download a sample sequence data file: $ cd /global/scratch/$USER $ curl http://s3.amazonaws.com/share.jgi-ga.org/sparc_example/illumina_02G_merged_with_id.seq.gz -o sample.seq.gz $ gunzip sample.seq.gz $ wc -l sample.seq 6343345 sample.seq $ head -2 sample.seq 6 HISEQ13:204:C8T6VANXX:1:1101:3726:1992 NATATTCCCGTTCTGATATTGCGTTAAGTCGTTCCCCTAAGCCGGCCCTCCTTATCGAGCGCGCCGGCTTTTTTTGCCATGTTCAGCGAATCACAGGACAAGATACTTCACCTAACGTAGTAGATGGTTCTATGCTTAAGGGCAAGGTGTNTTAATCTCGATATCCGCCTGTTTTAATAAATCAGCGACGAAGCGATGGGAGGATAAGCGCTCGTCAAAAACCACGCGCTTTTTTTCTAAGGTGGGTAAGTTCAAGGTAACACCCCCACTATGCCTATGAGTGAATTGGTAACACCTTGCC 60 HISEQ13:204:C8T6VANXX:1:1101:4370:1919 NCGTGCGCCCATCTCCGTGGCTAAACAGCTTGAGGTGGAAATTCGCCAGTGGATACAGCAGCATGCAGCGACAGGCGGGCGTCGCCTCCCTTCGATACGCCATTTAGCAGCAACACATAACGTCAGCCGCAATGCAGTCATTGAAGCTTANGTAAGGTCTTCTCCTTCGCGCCAATCGTTAGGTAACCAGCCGCAGCCCAGTTTCAATGACTGTTCATCGGTGTTAAACACGCCCCATAAGCCATTCGTCACTTCTTCCAATGGCGTTGATGACGCGGGTTGAACCAGTTTCAGCGCGTTA Now you can exit from lrc-xfer.lbl.gov . Alternatively , you could build SpaRC on your local computer, then upload the assembled fat jar file to your Lustre scratch space on Lawrencium.","title":"Building SpaRC"},{"location":"sparc_on_lawrencium/#running-sparc-interactively","text":"SSH to a Lawrencium login node: $ ssh lrc-login.lbl.gov For demonstration purpose, request 2 nodes from the lr6 partition for the interactive job: $ cd /global/scratch/$USER $ srun -p lr6 --qos=lr_normal -N 2 -t 1:00:0 --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> --pty bash When the job starts, you'll be given exclusive allocation to 2 compute nodes in the lr6 partition, each one of which has 2x 16-core Intel Xeon Gold 6130 CPUs and 96 GB memory (so in total, there will be 64 cores and 192GB memory available in your Spark cluster). And you'll be dropped to a bash shell on one of the compute nodes. Start Spark On Demand (SOD): $ source /global/home/groups/allhands/bin/spark_helper.sh $ spark-start Run the first Spark job on SOD (you might want to tune the values for --executor-cores , --num-executors and --executor-memory ): $ SCRATCH=/global/scratch/$USER $ JAR=$SCRATCH/LocalCluster-assembly-0.2.jar $ OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" $ OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" $ spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C Run the second Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 Run the third Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 Run the fifth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 Once you are done, don't forget to stop Spark On Demand and exit from the interactive job: $ spark-stop $ exit","title":"Running SpaRC interactively"},{"location":"sparc_on_lawrencium/#running-sparc-in-batch-mode","text":"To run SpaRC in batch mode, write a Slurm job script and call it sparc.slurm : #!/bin/bash #SBATCH --job-name=sparc #SBATCH --partition=lr6 #SBATCH --qos=lr_normal #SBATCH --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> #SBATCH --nodes=2 #SBATCH --time=01:00:00 source /global/home/groups/allhands/bin/spark_helper.sh # Start Spark On Demand spark-start SCRATCH=/global/scratch/$USER JAR=$SCRATCH/LocalCluster-assembly-0.2.jar OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" # 1st Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C # 2nd Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 # 3rd Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: # 4th Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 # 5th Spark job spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 # Stop Spark On Demand spark-stop Then submit the job with: sbatch sparc.slurm","title":"Running SpaRC in batch mode"},{"location":"sparc_on_lawrencium/#known-issues-and-future-improvements","text":"Presumably, there is a switch -i to spark-start that would enable communications over the IPoIB network. But it doesn't work! Spark 2.1.0 is a bit old.","title":"Known issues and future improvements"},{"location":"sparc_on_lawrencium/#references","text":"https://academic.oup.com/bioinformatics/article/35/5/760/5078476 https://peerj.com/articles/8966/","title":"References"},{"location":"storage/","text":"","title":"Storage Resources"},{"location":"support/","text":"Science General Support Science IT supports provision of all services and capabilities offered by the IT Division at LBL -- we are a one-stop shop for any need you may have including: Provision of virtual resources through our Science Virtual Machine program Help with data management plans and research needs through the LBL Research Library Assistance with IT requirements development, program management, architecture development and procurement support Identifying software needs, resources and deployment options","title":"Science General Support"},{"location":"support/#science-general-support","text":"Science IT supports provision of all services and capabilities offered by the IT Division at LBL -- we are a one-stop shop for any need you may have including: Provision of virtual resources through our Science Virtual Machine program Help with data management plans and research needs through the LBL Research Library Assistance with IT requirements development, program management, architecture development and procurement support Identifying software needs, resources and deployment options","title":"Science General Support"}]}