{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Whether you need a virtual server, help deciding on a data storage solution, help with procuring lab equipment, or access to the Lab's Lawrencium HPC, the place to start is with an email to scienceit@lbl.gov! Initial consultations are always free We have many low cost services We support on and off premises capabilities - if the right answer isn't available at LBL, we can find what you need at UCB, with external or cloud services providers (AWS, GCP) or elsewhere! Free drop-in office hours are available every Wednesday from 10:30am to Noon in 50B-3209","title":"Home"},{"location":"#getting-started","text":"Whether you need a virtual server, help deciding on a data storage solution, help with procuring lab equipment, or access to the Lab's Lawrencium HPC, the place to start is with an email to scienceit@lbl.gov! Initial consultations are always free We have many low cost services We support on and off premises capabilities - if the right answer isn't available at LBL, we can find what you need at UCB, with external or cloud services providers (AWS, GCP) or elsewhere! Free drop-in office hours are available every Wednesday from 10:30am to Noon in 50B-3209","title":"Getting Started"},{"location":"cloud/","text":"Cloud Resources LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP). This program provides the following benefits: No charge to have an account in the program Charges only for actual usage of cloud services like storage or compute Monthly billing for cloud usage are direct to your PID via recharge mechanism Simple enrollment process. De-enrollment only changes the billing setup in your account, and your account will continue to be active. You maintain complete control of your own AWS or GCP dashboard, and your data, tools, and services in your account are only accessible by you. Both AWS and GCP make discounts available to LBNL users for using their services. Generally speaking, costs for using GCP will be lower than AWS due to lower pricing and additional discounts. The discounts are: Type AWS GCP Overall 7% 13% Data Egress 15% 25% For AWS, restrictions apply Currently there are over 125 people at LBNL with cloud accounts on AWS and GCP. Most commonly people use virtual machines and storage, however there are many advanced users using containerization, machine learning, AI, and data visualization services on both platforms. Maybe list a few of the more interesting projects using AWS and GCP here? To set up a cloud account on either AWS or GCP, please send email to scienceit@lbl.gov to start the process.","title":"Cloud Resources"},{"location":"cloud/#cloud-resources","text":"LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP). This program provides the following benefits: No charge to have an account in the program Charges only for actual usage of cloud services like storage or compute Monthly billing for cloud usage are direct to your PID via recharge mechanism Simple enrollment process. De-enrollment only changes the billing setup in your account, and your account will continue to be active. You maintain complete control of your own AWS or GCP dashboard, and your data, tools, and services in your account are only accessible by you. Both AWS and GCP make discounts available to LBNL users for using their services. Generally speaking, costs for using GCP will be lower than AWS due to lower pricing and additional discounts. The discounts are: Type AWS GCP Overall 7% 13% Data Egress 15% 25% For AWS, restrictions apply Currently there are over 125 people at LBNL with cloud accounts on AWS and GCP. Most commonly people use virtual machines and storage, however there are many advanced users using containerization, machine learning, AI, and data visualization services on both platforms. Maybe list a few of the more interesting projects using AWS and GCP here? To set up a cloud account on either AWS or GCP, please send email to scienceit@lbl.gov to start the process.","title":"Cloud Resources"},{"location":"compute/","text":"Lawrencium Cluster Overview Detailed information of Lawrencium Accounts on Lawrencium Three types of Project Accounts PI Computing Allowance (PCA) account: free 300K SUs per year (pc_xxx) Condo account: PIs buy in compute nodes to the general condo pool (lr_xxx) Recharge account: with minimal recharge rate ~ $0.01/SU (ac_xxx) User accounts User account request User agreement consent https://sites.google.com/a/lbl.gov/hpc/getting-an-account Software Module Farm Module commands module purge : clear user\u2019s work environment module avail : check available software packages module load xxx : load a package module list : check currently loaded software Users may install their own software https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide SLURM: Resource Manager & Job Scheduler Job Submission Get help with the complete command options sbatch --help sbatch: submit a job to the batch queue system sbatch myjob.sh srun: request an interactive node(s) and login automatically srun -A ac_xxx -p lr5 -q lr_normal -t 1:0:0 --pty bash salloc : request an interactive node(s) salloc \u2013A pc_xxx \u2013p lr6 \u2013q lr_debug \u2013t 0:30:0 Job Monitoring sinfo: check status of partitions and nodes (idle, allocated, drain, down) sinfo \u2013r \u2013p lr6 squeue: check jobs in the batch queuing system (R or PD) squeue \u2013u $USER sacct: check job information or history sacct -X -o 'jobid,user,partition,nodelist,stat' scancel : cancel a job scancel jobID Check slurm association, such as qos, account, partition sacctmgr show association user=UserName -p perceus-00|ac_test|UserName|lr6|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|ac_test|UserName|lr5|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|pc_test|UserName|lr4|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|lr_test|UserName|lr3|1||||||||||||lr_debug,lr_lowprio,lr_normal,te perceus-00|scs|UserName|es1|1||||||||||||es_debug,es_lowprio,es_normal||| https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions Data Transfer Node & Visualization Node Designate data transfer node lrc-xfer.lbl.gov scp -r /your/source/file $USER@lrc-xder.lbl.gov:/cluster/path rsync -avzh /your/source/file $USER @lrc-xfer.lbl.gov:/cluster/path Globus Online provide secured unified interface for data transfer endpoint lbn#lrc, Globus Connect, AWS S3 connector Visualization and remote desktop node viz.lbl.gov Detailed information click here JupyterHub LRC JupyterHub Virtual Machine Services More information of VM click here","title":"Compute Resources"},{"location":"compute/#lawrencium-cluster-overview","text":"Detailed information of Lawrencium","title":"Lawrencium Cluster Overview"},{"location":"compute/#accounts-on-lawrencium","text":"","title":"Accounts on Lawrencium"},{"location":"compute/#three-types-of-project-accounts","text":"PI Computing Allowance (PCA) account: free 300K SUs per year (pc_xxx) Condo account: PIs buy in compute nodes to the general condo pool (lr_xxx) Recharge account: with minimal recharge rate ~ $0.01/SU (ac_xxx)","title":"Three types of Project Accounts"},{"location":"compute/#user-accounts","text":"User account request User agreement consent https://sites.google.com/a/lbl.gov/hpc/getting-an-account","title":"User accounts"},{"location":"compute/#software-module-farm","text":"","title":"Software Module Farm"},{"location":"compute/#module-commands","text":"module purge : clear user\u2019s work environment module avail : check available software packages module load xxx : load a package module list : check currently loaded software Users may install their own software https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide","title":"Module commands"},{"location":"compute/#slurm-resource-manager-job-scheduler","text":"","title":"SLURM: Resource Manager &amp; Job Scheduler"},{"location":"compute/#job-submission","text":"Get help with the complete command options sbatch --help sbatch: submit a job to the batch queue system sbatch myjob.sh srun: request an interactive node(s) and login automatically srun -A ac_xxx -p lr5 -q lr_normal -t 1:0:0 --pty bash salloc : request an interactive node(s) salloc \u2013A pc_xxx \u2013p lr6 \u2013q lr_debug \u2013t 0:30:0","title":"Job Submission"},{"location":"compute/#job-monitoring","text":"sinfo: check status of partitions and nodes (idle, allocated, drain, down) sinfo \u2013r \u2013p lr6 squeue: check jobs in the batch queuing system (R or PD) squeue \u2013u $USER sacct: check job information or history sacct -X -o 'jobid,user,partition,nodelist,stat' scancel : cancel a job scancel jobID Check slurm association, such as qos, account, partition sacctmgr show association user=UserName -p perceus-00|ac_test|UserName|lr6|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|ac_test|UserName|lr5|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|pc_test|UserName|lr4|1||||||||||||lr_debug,lr_lowprio,lr_normal||| perceus-00|lr_test|UserName|lr3|1||||||||||||lr_debug,lr_lowprio,lr_normal,te perceus-00|scs|UserName|es1|1||||||||||||es_debug,es_lowprio,es_normal||| https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions","title":"Job Monitoring"},{"location":"compute/#data-transfer-node-visualization-node","text":"Designate data transfer node lrc-xfer.lbl.gov scp -r /your/source/file $USER@lrc-xder.lbl.gov:/cluster/path rsync -avzh /your/source/file $USER @lrc-xfer.lbl.gov:/cluster/path Globus Online provide secured unified interface for data transfer endpoint lbn#lrc, Globus Connect, AWS S3 connector Visualization and remote desktop node viz.lbl.gov Detailed information click here","title":"Data Transfer Node &amp; Visualization Node"},{"location":"compute/#jupyterhub","text":"LRC JupyterHub","title":"JupyterHub"},{"location":"compute/#virtual-machine-services","text":"More information of VM click here","title":"Virtual Machine Services"},{"location":"consult/","text":"Consulting & Training Science IT provides on-demand and long-term \"embedded\" research consulting support, depending on your needs. We also support hiring of interns for science programs through the Science IT Intern program, as well as collaborative evaluation & support for hiring of staff to meet science project IT needs. One email to scienceit@lbl.gov is all you need to do to get the help you want. We provide a wide range of training opportunities -- all at no cost to science users. Via our partnerships with UCB's D-Lab, NSF's XSEDE, and AWS/GCP training programs we offer brown-bags, technical sessions, and custom opportunities to grow your IT skills. For more information, and to register for upcoming events, please see ittraining.lbl.gov.","title":"Consulting & Training"},{"location":"consult/#consulting-training","text":"Science IT provides on-demand and long-term \"embedded\" research consulting support, depending on your needs. We also support hiring of interns for science programs through the Science IT Intern program, as well as collaborative evaluation & support for hiring of staff to meet science project IT needs. One email to scienceit@lbl.gov is all you need to do to get the help you want. We provide a wide range of training opportunities -- all at no cost to science users. Via our partnerships with UCB's D-Lab, NSF's XSEDE, and AWS/GCP training programs we offer brown-bags, technical sessions, and custom opportunities to grow your IT skills. For more information, and to register for upcoming events, please see ittraining.lbl.gov.","title":"Consulting &amp; Training"},{"location":"container-101/","text":"Container 101 on Lawrencium; June 10, 2020; Wei Feinstein Outline Container technology overview Build singularity containers Run singularity containers on Lawrencium Containerization Standardized packaging of software and dependencies Portable, shareable, and reproducible. Your application brings its environment with it. Share the same OS kernel Applications Package an analysis pipeline so that it runs on your laptop, in the cloud, and in HPC environment to produce the same result. Publish a paper and include a link to a container with all of the data and software that you used so that others can easily reproduce your results. Install and run an application that requires a complicated stack of dependencies Legacy codes require outdated OS Container vs. Virtual Machine Singularity Technology Open-source computer software that encapsulates an application and all its dependencies into a single image Bring containers and reproducibility to scientific computing and HPC Developed by Greg Kurtzer Typically users have a build system as root users, but may not be root users on a production system Docker Bring containerization to the community-scale Rich image repository Widely used by scientific communities Compose for defining multi-container, recipe/definition file to build docker images Security concerns not ideal for the HPC environment Learn more docker Singularity Workflow Install Singularity on a local machine Build Singularity images locally with a root permission Transfer images to LRC clusters Run containers on the cluster Root privilege is not permitted Singularity Installation OS platforms - Linux - Mac - Window Refer to instructions here for details Test your installation: $ singularity --version singularity version 3.2.1-1 $ singularity run docker://godlovedc/lolcow ______________________________________ / A tall, dark stranger will have more \\ \\ fun than you. / -------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Create Singularity Containers Build directly from pre-built docker images Docker hub Sylabs Cloud and Singularity hub More involved using docker images from other external resources Nvidia HPC containers Biocontainers AWS Build using definition files or recipes Singularity pull directly from docker:// & shub:// No root/sudo privilege is needed Create/download immutable squashfs images/containers singularity pull --help Docker Hub: Pull a container from Docker Hub. $ singularity pull docker://ubuntu:18.04 $ singularity pull docker://gcc:7.2.0 Singularity Hub: If no tag is specified, the master branch of the repository is used $ singularity pull hello-world.sif shub://singularityhub/hello-world Run Singularity with shell, run, exec shell sub-command: invokes an interactive shell within a container singularity shell hello-world.sif run sub-command: executes the container\u2019s runscript singularity run hello-world.sif exec sub-command: execute an arbitrary command within container singularity exec hello-world.sif cat /etc/os-release Singularity pull when Docker Containers Provided by Other External docker repositories Nvidia HPC containers Biocontainers AWS Steps to build singularity containers from NGC: Pull a docker image locally (e.g. pgi compiler from NGC) $ docker pull nvcr.io/hpc/pgi-compilers:ce ..... $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nvcr.io/hpc/pgi-compiler ce c13ce6cf7f66 6 months ago 9.9GB openmpi3.1 latest 08a5518bb344 9 months ago 14.3GB registry 2 f32a97de94e1 15 months ago 25.8MB ... Steps to build singularity containers from NGC: Push to the Docker Hub (docker login) or simply use your local docker images Singularity build from local registry $ singularity build pgi.sif docker-daemon://nvcr.io/hpc/pgi-compilers:ce $ ls Dokerfile hello-world.sif pgi.sif saxpy.c Singularity Compile OpenACC code $ singularity exec pgi.sif pgcc -o saxpy -acc -Minfo saxpy.c saxpy: 9, Loop is parallelizable Generating Tesla code 9, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */ 9, FMA (fused multiply-add) instruction(s) generated ... Execute binary ./saxpy $ singularity exec pgi.sif ./saxpy y[0] = 2.000000 Another example of using docker containers from AWS Singularity build Root/sudo privilege is needed singularity build --help Build from a definition file sudo singularity --debug build mycontainer.sif Singularity Definition File/Recipe Bootstrap: docker #library, docker, shub, localimage, yum, debootstrap, arch, busybox, zypper From: ubuntu ## used singularity run-help %help Hello. I'm in the container. ## executed on host after the base OS is installed. %setup touch ${SINGULARITY_ROOTFS}/tacos.txt echo \"I love avocado\" >> avocados.txt # copy files from your host system into the container %files avocados.txt /opt %environment export NAME=avocado ## executed within the container after the base OS is installed at build time #install new software and libraries, config files, directories, etc %post echo 'export Avocado=TRUE' >> $SINGULARITY_ENVIRONMENT ## executed when the container image is run: singularity run %runscript echo \"Hello! Arguments received: $* \\n\" exec echo \"$@\" More information of singularity recipes Singularity Build Rewritable Sandbox Can be built from a recipe or existing container Used to develop, test, and make changes, then build or convert it into a standard image sudo singularity build --sandbox gccbox docker://gcc:7.2.0 sudo singularity build --sandbox test-box Singularity When you want to alter your image, you can use commands like shell, exec, run, with the --writable option sudo singularity shell --writable test-box Convert a sandbox to an immutable final container: sudo singularity build test-box.sif test-box Inspect Containers To check how a image is built, running script and environment variables.. singularity inspect [options] image_name --labels --runscript --deffile --environment e.g. singularity inspect --deffile mycontainer.sif Singularity Python (spython) Python API for Singularity containers Convert Dockerfile to Singularity def spython recipe Dockerfile > dock2sif.def Run Singularity Containers on Lawrencium File transfer to LRC cluster scp xxx.sif $USER@lrc-xfer.lbl.gov:/your/path/on/cluster Run your container interactively Request an interactive compute node singularity shell/run/exec container.sif Submit a slurm job Job Submission Example #!/bin/bash -l #SBATCH --job-name=container-test #SBATCH --partition=lr5 #SBATCH --account=ac_xxx #SBATCH --qos=lr_normal #SBATCH --nodes=1 #SBATCH --time=1-2:0:0 cd $SLURM_SUBMIT_DIR singularity exec mycontainer.sif cat /etc/os-release Container Bind Path Singularity allow mapping directories on host to directories within container Easy data access within containers System-defined bind paths on LRC /global/home/users/ /global/scratch/ User can define own bind paths: e.g.: mount /host/path/ on the host to /container/path inside the container -B /host/path/:/container/path singularity run --nv -B /global/home/users/$USER:/tmp pytorch_19_12_py3.sif ls /tmp Run GPU Containers Singularity supports NVIDIA\u2019s CUDA GPU compute framework or AMD\u2019s ROCm solution --nv enables NVIDIA GPU support in Singularity Remember to request a GPU node from the ES1 partition singularity exec --nv pytorch_19_12_py3.sif python -c \"import torch; print(torch.__version__)\" 1.4.0a0+a5b4d78 Run MPI Containers If launch on multiple nodes, MPI libraries on the host and inside the container need to match If launch on one node, only MPI library inside the container is called Exercise 1) singularity pull hello-world.sif shub://singularityhub/hello-world 2) generate hello-world.def or generate a sandbox to start with 3) add /data directory inside the container 4) build a new image hello-world-new.sif 5) bind /home/$USER on host to /data inside container 6) ls /data inside the new container Getting help Virtual Office Hours: Time: 10:30am - noon (Wednesdays) Request online Sending us tickets at hpcshelp@lbl.gov More information, documents, tips of how to use LBNL Supercluster http://scs.lbl.gov/ DLab consulting: https://dlab.berkeley.edu/consulting Please fill out Training Survey to get your comments and help us improve.","title":"Container 101 on Lawrencium"},{"location":"container-101/#outline","text":"Container technology overview Build singularity containers Run singularity containers on Lawrencium","title":"Outline"},{"location":"container-101/#containerization","text":"Standardized packaging of software and dependencies Portable, shareable, and reproducible. Your application brings its environment with it. Share the same OS kernel","title":"Containerization"},{"location":"container-101/#applications","text":"Package an analysis pipeline so that it runs on your laptop, in the cloud, and in HPC environment to produce the same result. Publish a paper and include a link to a container with all of the data and software that you used so that others can easily reproduce your results. Install and run an application that requires a complicated stack of dependencies Legacy codes require outdated OS","title":"Applications"},{"location":"container-101/#container-vs-virtual-machine","text":"","title":"Container vs. Virtual Machine"},{"location":"container-101/#singularity-technology","text":"Open-source computer software that encapsulates an application and all its dependencies into a single image Bring containers and reproducibility to scientific computing and HPC Developed by Greg Kurtzer Typically users have a build system as root users, but may not be root users on a production system","title":"Singularity Technology"},{"location":"container-101/#docker","text":"Bring containerization to the community-scale Rich image repository Widely used by scientific communities Compose for defining multi-container, recipe/definition file to build docker images Security concerns not ideal for the HPC environment Learn more docker","title":"Docker"},{"location":"container-101/#singularity-workflow","text":"Install Singularity on a local machine Build Singularity images locally with a root permission Transfer images to LRC clusters Run containers on the cluster Root privilege is not permitted","title":"Singularity Workflow"},{"location":"container-101/#singularity-installation","text":"OS platforms - Linux - Mac - Window Refer to instructions here for details Test your installation: $ singularity --version singularity version 3.2.1-1 $ singularity run docker://godlovedc/lolcow ______________________________________ / A tall, dark stranger will have more \\ \\ fun than you. / -------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Singularity Installation"},{"location":"container-101/#create-singularity-containers","text":"Build directly from pre-built docker images Docker hub Sylabs Cloud and Singularity hub More involved using docker images from other external resources Nvidia HPC containers Biocontainers AWS Build using definition files or recipes","title":"Create Singularity Containers"},{"location":"container-101/#singularity-pull-directly-from-docker-shub","text":"No root/sudo privilege is needed Create/download immutable squashfs images/containers singularity pull --help Docker Hub: Pull a container from Docker Hub. $ singularity pull docker://ubuntu:18.04 $ singularity pull docker://gcc:7.2.0 Singularity Hub: If no tag is specified, the master branch of the repository is used $ singularity pull hello-world.sif shub://singularityhub/hello-world","title":"Singularity pull directly from docker:// &amp; shub://"},{"location":"container-101/#run-singularity-with-shell-run-exec","text":"shell sub-command: invokes an interactive shell within a container singularity shell hello-world.sif run sub-command: executes the container\u2019s runscript singularity run hello-world.sif exec sub-command: execute an arbitrary command within container singularity exec hello-world.sif cat /etc/os-release","title":"Run Singularity with shell, run, exec"},{"location":"container-101/#singularity-pull-when-docker-containers-provided-by-other-external-docker-repositories","text":"Nvidia HPC containers Biocontainers AWS","title":"Singularity pull when Docker Containers Provided by Other External docker repositories"},{"location":"container-101/#steps-to-build-singularity-containers-from-ngc","text":"Pull a docker image locally (e.g. pgi compiler from NGC) $ docker pull nvcr.io/hpc/pgi-compilers:ce ..... $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nvcr.io/hpc/pgi-compiler ce c13ce6cf7f66 6 months ago 9.9GB openmpi3.1 latest 08a5518bb344 9 months ago 14.3GB registry 2 f32a97de94e1 15 months ago 25.8MB ...","title":"Steps to build singularity containers from NGC:"},{"location":"container-101/#steps-to-build-singularity-containers-from-ngc_1","text":"Push to the Docker Hub (docker login) or simply use your local docker images Singularity build from local registry $ singularity build pgi.sif docker-daemon://nvcr.io/hpc/pgi-compilers:ce $ ls Dokerfile hello-world.sif pgi.sif saxpy.c Singularity Compile OpenACC code $ singularity exec pgi.sif pgcc -o saxpy -acc -Minfo saxpy.c saxpy: 9, Loop is parallelizable Generating Tesla code 9, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */ 9, FMA (fused multiply-add) instruction(s) generated ... Execute binary ./saxpy $ singularity exec pgi.sif ./saxpy y[0] = 2.000000 Another example of using docker containers from AWS","title":"Steps to build singularity containers from NGC:"},{"location":"container-101/#singularity-build","text":"Root/sudo privilege is needed singularity build --help Build from a definition file sudo singularity --debug build mycontainer.sif Singularity","title":"Singularity build"},{"location":"container-101/#definition-filerecipe","text":"Bootstrap: docker #library, docker, shub, localimage, yum, debootstrap, arch, busybox, zypper From: ubuntu ## used singularity run-help %help Hello. I'm in the container. ## executed on host after the base OS is installed. %setup touch ${SINGULARITY_ROOTFS}/tacos.txt echo \"I love avocado\" >> avocados.txt # copy files from your host system into the container %files avocados.txt /opt %environment export NAME=avocado ## executed within the container after the base OS is installed at build time #install new software and libraries, config files, directories, etc %post echo 'export Avocado=TRUE' >> $SINGULARITY_ENVIRONMENT ## executed when the container image is run: singularity run %runscript echo \"Hello! Arguments received: $* \\n\" exec echo \"$@\" More information of singularity recipes","title":"Definition File/Recipe"},{"location":"container-101/#singularity-build-rewritable-sandbox","text":"Can be built from a recipe or existing container Used to develop, test, and make changes, then build or convert it into a standard image sudo singularity build --sandbox gccbox docker://gcc:7.2.0 sudo singularity build --sandbox test-box Singularity When you want to alter your image, you can use commands like shell, exec, run, with the --writable option sudo singularity shell --writable test-box Convert a sandbox to an immutable final container: sudo singularity build test-box.sif test-box","title":"Singularity Build Rewritable Sandbox"},{"location":"container-101/#inspect-containers","text":"To check how a image is built, running script and environment variables.. singularity inspect [options] image_name --labels --runscript --deffile --environment e.g. singularity inspect --deffile mycontainer.sif","title":"Inspect Containers"},{"location":"container-101/#singularity-python-spython","text":"Python API for Singularity containers Convert Dockerfile to Singularity def spython recipe Dockerfile > dock2sif.def","title":"Singularity Python (spython)"},{"location":"container-101/#run-singularity-containers-on-lawrencium","text":"File transfer to LRC cluster scp xxx.sif $USER@lrc-xfer.lbl.gov:/your/path/on/cluster Run your container interactively Request an interactive compute node singularity shell/run/exec container.sif Submit a slurm job","title":"Run Singularity Containers on Lawrencium"},{"location":"container-101/#job-submission-example","text":"#!/bin/bash -l #SBATCH --job-name=container-test #SBATCH --partition=lr5 #SBATCH --account=ac_xxx #SBATCH --qos=lr_normal #SBATCH --nodes=1 #SBATCH --time=1-2:0:0 cd $SLURM_SUBMIT_DIR singularity exec mycontainer.sif cat /etc/os-release","title":"Job Submission Example"},{"location":"container-101/#container-bind-path","text":"Singularity allow mapping directories on host to directories within container Easy data access within containers System-defined bind paths on LRC /global/home/users/ /global/scratch/ User can define own bind paths: e.g.: mount /host/path/ on the host to /container/path inside the container -B /host/path/:/container/path singularity run --nv -B /global/home/users/$USER:/tmp pytorch_19_12_py3.sif ls /tmp","title":"Container Bind Path"},{"location":"container-101/#run-gpu-containers","text":"Singularity supports NVIDIA\u2019s CUDA GPU compute framework or AMD\u2019s ROCm solution --nv enables NVIDIA GPU support in Singularity Remember to request a GPU node from the ES1 partition singularity exec --nv pytorch_19_12_py3.sif python -c \"import torch; print(torch.__version__)\" 1.4.0a0+a5b4d78","title":"Run GPU Containers"},{"location":"container-101/#run-mpi-containers","text":"If launch on multiple nodes, MPI libraries on the host and inside the container need to match If launch on one node, only MPI library inside the container is called","title":"Run MPI Containers"},{"location":"container-101/#exercise","text":"1) singularity pull hello-world.sif shub://singularityhub/hello-world 2) generate hello-world.def or generate a sandbox to start with 3) add /data directory inside the container 4) build a new image hello-world-new.sif 5) bind /home/$USER on host to /data inside container 6) ls /data inside the new container","title":"Exercise"},{"location":"container-101/#getting-help","text":"Virtual Office Hours: Time: 10:30am - noon (Wednesdays) Request online Sending us tickets at hpcshelp@lbl.gov More information, documents, tips of how to use LBNL Supercluster http://scs.lbl.gov/ DLab consulting: https://dlab.berkeley.edu/consulting Please fill out Training Survey to get your comments and help us improve.","title":"Getting help"},{"location":"data/","text":"Data Management & Storage Science IT can help you design the right data management solution supporting your research, and we have a wide variety of options supporting scientific data collection, storage, and analytics. These include: Our Science Project Storage Service ($25/Mo/TB) for performance data storage on demand, including free data movement and migration Our Compellent and Lawrencium storage condo option for those storing fixed amounts of data for longer than 6 months Access to cloud storage and assistance with local storage provision Help with data migration, including Globus and unlimited, no cost GDrive data storage","title":"Data Management & Storage"},{"location":"data/#data-management-storage","text":"Science IT can help you design the right data management solution supporting your research, and we have a wide variety of options supporting scientific data collection, storage, and analytics. These include: Our Science Project Storage Service ($25/Mo/TB) for performance data storage on demand, including free data movement and migration Our Compellent and Lawrencium storage condo option for those storing fixed amounts of data for longer than 6 months Access to cloud storage and assistance with local storage provision Help with data migration, including Globus and unlimited, no cost GDrive data storage","title":"Data Management &amp; Storage"},{"location":"hpc/","text":"High-Performance Computing Science IT supports and operates LBL's institutional high performance compute resource called Lawrencium. Lawrencium is a 55,000 core cluster which consist of shared nodes and PI-contributed condo nodes currently supporting compute needs across virtually every scientific discipline studied at LBL. For account information (including access to our PI Grant program) please see scs.lbl.gov. Support for both shared and private compute node hosting GPU nodes available now (Dual V100 with NVLink or Quad GTX1080Ti nodes) supporting machine learning and other needs High Speed Network connections between nodes - FDR & EDR Infiniband interconnect Users may also directly purchase service units ($0.01) on-demand","title":"High-Performance Computing"},{"location":"hpc/#high-performance-computing","text":"Science IT supports and operates LBL's institutional high performance compute resource called Lawrencium. Lawrencium is a 55,000 core cluster which consist of shared nodes and PI-contributed condo nodes currently supporting compute needs across virtually every scientific discipline studied at LBL. For account information (including access to our PI Grant program) please see scs.lbl.gov. Support for both shared and private compute node hosting GPU nodes available now (Dual V100 with NVLink or Quad GTX1080Ti nodes) supporting machine learning and other needs High Speed Network connections between nodes - FDR & EDR Infiniband interconnect Users may also directly purchase service units ($0.01) on-demand","title":"High-Performance Computing"},{"location":"jupyter_gpu/","text":"Jupyter Notebook on Lawrencium GPU node https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/1.12.0-py36 $ start_jupyter.py Login from visualization node via VNC Viewer: https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.VERSION) 1.12.0 print(date.today()) 2020-04-30 Test if TF can access a GPU tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True Print the name of the GPU device tf.test.gpu_device_name() '/device:GPU:0' Run the code on a GPU or CPU device_name = tf.test.gpu_device_name() #device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750466000.0 Shape: (3000, 3000) Device: /device:GPU:0 Time taken: 0:00:00.044407 #device_name = tf.test.gpu_device_name() device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750241300.0 Shape: (3000, 3000) Device: /device:CPU:0 Time taken: 0:00:00.435257","title":"TensorFlow 1.12"},{"location":"jupyter_gpu/#jupyter-notebook-on-lawrencium-gpu-node","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/1.12.0-py36 $ start_jupyter.py","title":"Jupyter Notebook on Lawrencium GPU node"},{"location":"jupyter_gpu/#login-from-visualization-node-via-vnc-viewer","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.VERSION) 1.12.0 print(date.today()) 2020-04-30","title":"Login from visualization node via VNC Viewer:"},{"location":"jupyter_gpu/#test-if-tf-can-access-a-gpu","text":"tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True","title":"Test if TF can access a GPU"},{"location":"jupyter_gpu/#print-the-name-of-the-gpu-device","text":"tf.test.gpu_device_name() '/device:GPU:0'","title":"Print the name of the GPU device"},{"location":"jupyter_gpu/#run-the-code-on-a-gpu-or-cpu","text":"device_name = tf.test.gpu_device_name() #device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750466000.0 Shape: (3000, 3000) Device: /device:GPU:0 Time taken: 0:00:00.044407 #device_name = tf.test.gpu_device_name() device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750241300.0 Shape: (3000, 3000) Device: /device:CPU:0 Time taken: 0:00:00.435257","title":"Run the code on a GPU or CPU"},{"location":"jupyter_gpu_tensorflow_2.0/","text":"Jupyter Notebook on Lawrencium GPU node https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/2.0-py36 $ start_jupyter.py Login from visualization node via VNC Viewer: https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.compat.v1.VERSION) 2.0.0 print(date.today()) 2020-05-11 Test if TF can access a GPU tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True Print the name of the GPU device tf.test.gpu_device_name() '/device:GPU:0'","title":"TensorFlow 2.0"},{"location":"jupyter_gpu_tensorflow_2.0/#jupyter-notebook-on-lawrencium-gpu-node","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/2.0-py36 $ start_jupyter.py","title":"Jupyter Notebook on Lawrencium GPU node"},{"location":"jupyter_gpu_tensorflow_2.0/#login-from-visualization-node-via-vnc-viewer","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.compat.v1.VERSION) 2.0.0 print(date.today()) 2020-05-11","title":"Login from visualization node via VNC Viewer:"},{"location":"jupyter_gpu_tensorflow_2.0/#test-if-tf-can-access-a-gpu","text":"tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True","title":"Test if TF can access a GPU"},{"location":"jupyter_gpu_tensorflow_2.0/#print-the-name-of-the-gpu-device","text":"tf.test.gpu_device_name() '/device:GPU:0'","title":"Print the name of the GPU device"},{"location":"jupyter_gpu_tensorflow_2.1/","text":"Jupyter Notebook on Lawrencium GPU node https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/2.1.0-py37 $ start_jupyter.py Login from visualization node via VNC Viewer: https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0] print(tf.compat.v1.VERSION) 2.1.0 print(date.today()) 2020-05-13 Test if TF can access a GPU tf.config.list_physical_devices('GPU') [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')] Print the name of the GPU device tf.test.gpu_device_name() '/device:GPU:0'","title":"TensorFlow 2.1"},{"location":"jupyter_gpu_tensorflow_2.1/#jupyter-notebook-on-lawrencium-gpu-node","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook $ srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash $ module load ml/tensorflow/2.1.0-py37 $ start_jupyter.py","title":"Jupyter Notebook on Lawrencium GPU node"},{"location":"jupyter_gpu_tensorflow_2.1/#login-from-visualization-node-via-vnc-viewer","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop import sys import numpy as np import tensorflow as tf from datetime import date from datetime import datetime print(sys.version) 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0] print(tf.compat.v1.VERSION) 2.1.0 print(date.today()) 2020-05-13","title":"Login from visualization node via VNC Viewer:"},{"location":"jupyter_gpu_tensorflow_2.1/#test-if-tf-can-access-a-gpu","text":"tf.config.list_physical_devices('GPU') [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]","title":"Test if TF can access a GPU"},{"location":"jupyter_gpu_tensorflow_2.1/#print-the-name-of-the-gpu-device","text":"tf.test.gpu_device_name() '/device:GPU:0'","title":"Print the name of the GPU device"},{"location":"jupyterhub_gpu/","text":"Jupyter Notebook via Jupyterhub https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook select ES1 partition for GPU Select a TensorFlow kernel : Python3.6 TF-1.12 import sys import os import getpass import numpy as np from datetime import datetime from datetime import date system('hostname') ['n0009.es1'] import tensorflow as tf print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.VERSION) 1.12.0 print(date.today()) 2020-05-21 Test if TF can access a GPU tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True Print the name of the GPU device tf.test.gpu_device_name() '/device:GPU:0' Run the code on a GPU or CPU device_name = tf.test.gpu_device_name() #device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750205000.0 Shape: (3000, 3000) Device: /device:GPU:0 Time taken: 0:00:00.043736 #device_name = tf.test.gpu_device_name() device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6749143000.0 Shape: (3000, 3000) Device: /device:CPU:0 Time taken: 0:00:00.765788","title":"JupyterHub"},{"location":"jupyterhub_gpu/#jupyter-notebook-via-jupyterhub","text":"https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/jupyter-notebook select ES1 partition for GPU Select a TensorFlow kernel : Python3.6 TF-1.12 import sys import os import getpass import numpy as np from datetime import datetime from datetime import date system('hostname') ['n0009.es1'] import tensorflow as tf print(sys.version) 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) [GCC 7.3.0] print(tf.VERSION) 1.12.0 print(date.today()) 2020-05-21","title":"Jupyter Notebook via Jupyterhub"},{"location":"jupyterhub_gpu/#test-if-tf-can-access-a-gpu","text":"tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) True","title":"Test if TF can access a GPU"},{"location":"jupyterhub_gpu/#print-the-name-of-the-gpu-device","text":"tf.test.gpu_device_name() '/device:GPU:0' Run the code on a GPU or CPU device_name = tf.test.gpu_device_name() #device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6750205000.0 Shape: (3000, 3000) Device: /device:GPU:0 Time taken: 0:00:00.043736 #device_name = tf.test.gpu_device_name() device_name = '/device:CPU:0' shape = (3000, 3000) with tf.device(device_name): random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation) startTime = datetime.now() with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session: result = session.run(sum_operation) print(result) print(\"Shape:\", shape, \"Device:\", device_name) print(\"Time taken:\", datetime.now() - startTime) 6749143000.0 Shape: (3000, 3000) Device: /device:CPU:0 Time taken: 0:00:00.765788","title":"Print the name of the GPU device"},{"location":"rl_mountain_car_coach_gymEnv/","text":"Mountain Car with Amazon SageMaker RL AWS SageMaker Example: https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_mountain_car_coach_gymEnv Overview Mountain Car is a classic control Reinforcement Learning problem that was first introduced by A. Moore in 1991 [1]. An under-powered car is tasked with climbing a steep mountain, and is only successful when it reaches the top. Luckily there's another mountain on the opposite side which can be used to gain momentum, and launch the car to the peak. It can be tricky to find this optimal solution due to the sparsity of the reward. Complex exploration strategies can be used to incentivise exploration of the mountain, but to keep things simple in this example we extend the amount of time in each episode from Open AI Gym's default of 200 environment steps to 10,000 steps, showing how to customise environments. We consider two variants in this example: PatientMountainCar for discrete actions and PatientContinuousMountainCar for continuous actions. PatientMountainCar Objective: Get the car to the top of the right hand side mountain. Environment(s): Open AI Gym's MountainCar-v0 that is extended to 10,000 steps per episode. State: Car's horizontal position and velocity (can be negative). Action: Direction of push (left, nothing or right). Reward: -1 for every environment step until success, which incentivises quick solutions. PatientContinuousMountainCar Objective: Get the car to the top of the right hand side mountain. Environment(s): Open AI Gym's MountainCarContinuous-v0 that is extended to 10,000 steps per episode. State: Car's horizontal position and velocity (can be negative). Action: Mmagnitude of push (if negative push to left, if positive push to right). Reward: +100 for reaching top of the right hand side mountain, minus the squared sum of actions from start to end. [1] A. Moore, Efficient Memory-Based Learning for Robot Control, PhD thesis, University of Cambridge, November 1990. Prerequisites Create an Amazon SageMaker Notebook Open JupyterLab Clone the sample code In JupyterLab , click the Terminal icon to open a new terminal To clone the tutorial repository , run the following command git clone https://github.com/lbnl-science-it/aws-sagemaker-rl.git In JupyterLab , navigate to aws-sagemaker-rl and open rl_mountain_car_coach_gymEnv.ipynb Contents rl_mountain_car_coach_gym.ipynb : notebook used for training Mountain Car policy src/patient_envs.py : custom environments defined here. src/train-coach.py : launcher for coach training src/preset-mountain-car-continuous-clipped-ppo.py : coach preset for Clipped PPO. src/preset-mountain-car-dqn.py : coach preset for DQN. Imports To get started, we'll import the Python libraries we need, set up the environment with a few prerequisites for permissions and configurations. import sagemaker import boto3 import sys import os import glob import re import subprocess import numpy as np from IPython.display import HTML import time from time import gmtime, strftime sys.path.append(\"common\") from misc import get_execution_role, wait_for_s3_object from sagemaker.rl import RLEstimator, RLToolkit, RLFramework Setup S3 bucket Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. sage_session = sagemaker.session.Session() s3_bucket = sage_session.default_bucket() s3_output_path = 's3://{}/'.format(s3_bucket) print(\"S3 bucket path: {}\".format(s3_output_path)) S3 bucket path: s3://sagemaker-us-west-2-485444084140/ Define Variables We define variables such as the job prefix for the training jobs and the image path for the container (only when this is BYOC). # create unique job name job_name_prefix = 'rl-mountain-car' Configure settings You can run your RL training jobs on a SageMaker notebook instance or on your own machine. In both of these scenarios, you can run in either 'local' (where you run the commands) or 'SageMaker' mode (on SageMaker training instances). 'local' mode uses the SageMaker Python SDK to run your code in Docker containers locally. It can speed up iterative testing and debugging while using the same familiar Python SDK interface. Just set local_mode = True . And when you're ready move to 'SageMaker' mode to scale things up. # run in local mode? local_mode = False Create an IAM role Either get the execution role when running from a SageMaker notebook instance role = sagemaker.get_execution_role() or, when running from local notebook instance, use utils method role = get_execution_role() to create an execution role. try: role = sagemaker.get_execution_role() except: role = get_execution_role() print(\"Using IAM role arn: {}\".format(role)) Install docker for local mode In order to work in local mode, you need to have docker installed. When running from you local machine, please make sure that you have docker or docker-compose (for local CPU machines) and nvidia-docker (for local GPU machines) installed. Alternatively, when running from a SageMaker notebook instance, you can simply run the following script to install dependenceis. Note, you can only run a single local notebook at one time. # only run from SageMaker notebook instance if local_mode: !/bin/bash ./common/setup.sh Setup the environment We create a file called src/patient_envs.py for our modified environments. We can create a custom environment class or create a function that returns our environment. Since we're using Open AI Gym environment and wrappers, we just create functions that take the classic control environments MountainCarEnv and Continuous_MountainCarEnv and wrap them with a TimeLimit where we specify the max_episode_steps to 10,000. !pygmentize src/patient_envs.py \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgym.envs.classic_control.mountain_car\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MountainCarEnv \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgym.envs.classic_control.continuous_mountain_car\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Continuous_MountainCarEnv \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgym.wrappers.time_limit\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TimeLimit \u001b[34mdef\u001b[39;49;00m \u001b[32mPatientMountainCar\u001b[39;49;00m(): env = MountainCarEnv() \u001b[34mreturn\u001b[39;49;00m TimeLimit(env, max_episode_steps=\u001b[34m10000\u001b[39;49;00m) \u001b[34mdef\u001b[39;49;00m \u001b[32mPatientContinuousMountainCar\u001b[39;49;00m(): env = Continuous_MountainCarEnv() \u001b[34mreturn\u001b[39;49;00m TimeLimit(env, max_episode_steps=\u001b[34m10000\u001b[39;49;00m) Configure the presets for RL algorithm The presets that configure the RL training jobs are defined in the \"preset-mountain-car-continuous-clipped-ppo.py\" file which is also uploaded on the /src directory. Also see \"preset-mountain-car-dqn.py\" for the discrete environment case. Using the preset file, you can define agent parameters to select the specific agent algorithm. You can also set the environment parameters, define the schedule and visualization parameters, and define the graph manager. The schedule presets will define the number of heat up steps, periodic evaluation steps, training steps between evaluations. These can be overridden at runtime by specifying the RLCOACH_PRESET hyperparameter. Additionally, it can be used to define custom hyperparameters. !pygmentize src/preset-mountain-car-continuous-clipped-ppo.py \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.agents.clipped_ppo_agent\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClippedPPOAgentParameters \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.architectures.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VisualizationParameters, PresetValidationParameters \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.core_types\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, RunPhase, \\ SelectedPhaseOnlyDumpFilter, MaxDumpFilter \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.environments.gym_environment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GymVectorEnvironment \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.exploration_policies.e_greedy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m EGreedyParameters \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.filters.observation.observation_normalization_filter\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ObservationNormalizationFilter \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.basic_rl_graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BasicRLGraphManager \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ScheduleParameters \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.schedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LinearSchedule \u001b[37m####################\u001b[39;49;00m \u001b[37m# Graph Scheduling #\u001b[39;49;00m \u001b[37m####################\u001b[39;49;00m schedule_params = ScheduleParameters() schedule_params.improve_steps = EnvironmentEpisodes(\u001b[34m100\u001b[39;49;00m) schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m) schedule_params.evaluation_steps = EnvironmentEpisodes(\u001b[34m1\u001b[39;49;00m) schedule_params.heatup_steps = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m) \u001b[37m#########\u001b[39;49;00m \u001b[37m# Agent #\u001b[39;49;00m \u001b[37m#########\u001b[39;49;00m agent_params = ClippedPPOAgentParameters() agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].learning_rate = \u001b[34m0.001\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].input_embedders_parameters[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].activation_function = \u001b[33m'\u001b[39;49;00m\u001b[33mtanh\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].input_embedders_parameters[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].scheme = [Dense(\u001b[34m32\u001b[39;49;00m)] agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].middleware_parameters.scheme = [Dense(\u001b[34m32\u001b[39;49;00m)] agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].middleware_parameters.activation_function = \u001b[33m'\u001b[39;49;00m\u001b[33mtanh\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].batch_size = \u001b[34m256\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].optimizer_epsilon = \u001b[34m1e-5\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].adam_optimizer_beta2 = \u001b[34m0.999\u001b[39;49;00m agent_params.algorithm.clip_likelihood_ratio_using_epsilon = \u001b[34m0.3\u001b[39;49;00m agent_params.algorithm.clipping_decay_schedule = LinearSchedule(\u001b[34m0.5\u001b[39;49;00m, \u001b[34m0.1\u001b[39;49;00m, \u001b[34m10000\u001b[39;49;00m * \u001b[34m50\u001b[39;49;00m) agent_params.algorithm.beta_entropy = \u001b[34m0\u001b[39;49;00m agent_params.algorithm.gae_lambda = \u001b[34m0.95\u001b[39;49;00m agent_params.algorithm.discount = \u001b[34m0.999\u001b[39;49;00m agent_params.algorithm.estimate_state_value_using_gae = \u001b[36mTrue\u001b[39;49;00m agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m) agent_params.algorithm.num_episodes_in_experience_replay = \u001b[34m100\u001b[39;49;00m agent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m) agent_params.algorithm.optimization_epochs = \u001b[34m10\u001b[39;49;00m agent_params.pre_network_filter.add_observation_filter(\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mnormalize_observation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, ObservationNormalizationFilter(name=\u001b[33m'\u001b[39;49;00m\u001b[33mnormalize_observation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)) \u001b[37m###############\u001b[39;49;00m \u001b[37m# Environment #\u001b[39;49;00m \u001b[37m###############\u001b[39;49;00m env_params = GymVectorEnvironment(level=\u001b[33m'\u001b[39;49;00m\u001b[33mpatient_envs:PatientContinuousMountainCar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[37m#################\u001b[39;49;00m \u001b[37m# Visualization #\u001b[39;49;00m \u001b[37m#################\u001b[39;49;00m vis_params = VisualizationParameters() vis_params.dump_gifs = \u001b[36mTrue\u001b[39;49;00m vis_params.video_dump_filters = [SelectedPhaseOnlyDumpFilter(RunPhase.TEST), MaxDumpFilter()] \u001b[37m########\u001b[39;49;00m \u001b[37m# Test #\u001b[39;49;00m \u001b[37m########\u001b[39;49;00m preset_validation_params = PresetValidationParameters() preset_validation_params.test = \u001b[36mTrue\u001b[39;49;00m preset_validation_params.min_reward_threshold = \u001b[34m150\u001b[39;49;00m preset_validation_params.max_episodes_to_achieve_reward = \u001b[34m250\u001b[39;49;00m graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params, schedule_params=schedule_params, vis_params=vis_params, preset_validation_params=preset_validation_params) Write the Training Code The training code is written in the file \u201ctrain-coach.py\u201d which is uploaded in the /src directory. We create a custom SageMakerCoachPresetLauncher which sets the default preset, maps and ties hyperparameters. !pygmentize src/train-coach.py \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_rl.coach_launcher\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerCoachPresetLauncher \u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyLauncher\u001b[39;49;00m(SageMakerCoachPresetLauncher): \u001b[34mdef\u001b[39;49;00m \u001b[32mdefault_preset_name\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m): \u001b[33m\"\"\"This points to a .py file that configures everything about the RL job.\u001b[39;49;00m \u001b[33m It can be overridden at runtime by specifying the RLCOACH_PRESET hyperparameter.\u001b[39;49;00m \u001b[33m \"\"\"\u001b[39;49;00m \u001b[34mreturn\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mpreset-mountaincarcontinuous-clippedppo.py\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mdef\u001b[39;49;00m \u001b[32mmap_hyperparameter\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, name, value): \u001b[33m\"\"\"Here we configure some shortcut names for hyperparameters that we expect to use frequently.\u001b[39;49;00m \u001b[33m Essentially anything in the preset file can be overridden through a hyperparameter with a name \u001b[39;49;00m \u001b[33m like \"rl.agent_params.algorithm.etc\". \u001b[39;49;00m \u001b[33m \"\"\"\u001b[39;49;00m \u001b[37m# maps from alias (key) to fully qualified coach parameter (value)\u001b[39;49;00m mapping = { \u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation_freq_env_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.steps_between_evaluation_periods:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation_episodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.evaluation_steps:EnvironmentEpisodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mimprove_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.improve_steps:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mdiscount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.discount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mgae_lambda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.gae_lambda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_freq_env_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.num_consecutive_playing_steps:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.network_wrappers.main.learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.network_wrappers.main.batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.optimization_epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m } \u001b[34mif\u001b[39;49;00m name \u001b[35min\u001b[39;49;00m mapping: \u001b[36mself\u001b[39;49;00m.apply_hyperparameter(mapping[name], value) \u001b[34melse\u001b[39;49;00m: \u001b[36msuper\u001b[39;49;00m().map_hyperparameter(name, value) \u001b[34mdef\u001b[39;49;00m \u001b[32mget_config_args\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, parser): args = \u001b[36msuper\u001b[39;49;00m().get_config_args(parser) \u001b[37m# Above line creates `self.hyperparameters` which is a collection of hyperparameters\u001b[39;49;00m \u001b[37m# that are to be added to graph manager when it's created. At this stage they are \u001b[39;49;00m \u001b[37m# fully qualified names since already passed through `map_hyperparameter`.\u001b[39;49;00m \u001b[37m# Keeps target network the same for all epochs of a single 'policy training' stage.\u001b[39;49;00m src_hp = \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.num_consecutive_playing_steps:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m target_hp = \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.num_steps_between_copying_online_weights_to_target:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict.get(src_hp, \u001b[36mFalse\u001b[39;49;00m): src_val = \u001b[36mint\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict[src_hp]) \u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict[target_hp] = src_val \u001b[37m# Evaluate after each 'policy training' stage\u001b[39;49;00m src_hp = \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.num_consecutive_playing_steps:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m target_hp = \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.steps_between_evaluation_periods:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict.get(src_hp, \u001b[36mFalse\u001b[39;49;00m): src_val = \u001b[36mint\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict[src_hp]) \u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict[target_hp] = src_val \u001b[34mreturn\u001b[39;49;00m args \u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: MyLauncher.train_main() Train the RL model using the Python SDK Script mode If you are using local mode, the training will run on the notebook instance. When using SageMaker for training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs. Specify the source directory where the environment, presets and training code is uploaded. Specify the entry point as the training code Specify the choice of RL toolkit and framework. This automatically resolves to the ECR path for the RL Container. Define the training parameters such as the instance count, job name, S3 path for output and job name. Specify the hyperparameters for the RL agent algorithm. The RLCOACH_PRESET can be used to specify the RL agent algorithm you want to use. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks. We use a variant of Proximal Policy Optimization (PPO) called Clipped PPO, which removes the need for complex KL divergence calculations. if local_mode: instance_type = 'local' else: instance_type = \"ml.m4.4xlarge\" estimator = RLEstimator(entry_point=\"train-coach.py\", source_dir='src', dependencies=[\"common/sagemaker_rl\"], toolkit=RLToolkit.COACH, toolkit_version='0.11.0', framework=RLFramework.MXNET, role=role, train_instance_type=instance_type, train_instance_count=1, output_path=s3_output_path, base_job_name=job_name_prefix, hyperparameters = { \"RLCOACH_PRESET\": \"preset-mountain-car-continuous-clipped-ppo\", # \"preset-mountain-car-dqn\", \"discount\": 0.995, \"gae_lambda\": 0.997, \"evaluation_episodes\": 3, # approx 100 episodes \"improve_steps\": 100000, # approx 5 episodes to start with \"training_freq_env_steps\": 75000, \"training_learning_rate\": 0.004, \"training_batch_size\": 256, # times number below by training_freq_env_steps to get total samples per policy training \"training_epochs\": 15, 'save_model': 1 } ) estimator.fit(wait=local_mode) Store intermediate training output and model checkpoints The output from the training job above is stored on S3. The intermediate folder contains gifs and metadata of the training. job_name=estimator._current_job_name print(\"Job name: {}\".format(job_name)) s3_url = \"s3://{}/{}\".format(s3_bucket,job_name) if local_mode: output_tar_key = \"{}/output.tar.gz\".format(job_name) else: output_tar_key = \"{}/output/output.tar.gz\".format(job_name) intermediate_folder_key = \"{}/output/intermediate/\".format(job_name) output_url = \"s3://{}/{}\".format(s3_bucket, output_tar_key) intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key) print(\"S3 job path: {}\".format(s3_url)) print(\"Output.tar.gz location: {}\".format(output_url)) print(\"Intermediate folder path: {}\".format(intermediate_url)) tmp_dir = \"/tmp/{}\".format(job_name) os.system(\"mkdir {}\".format(tmp_dir)) print(\"Create local folder {}\".format(tmp_dir)) Visualization Plot metrics for training job We can pull the reward metric of the training and plot it to see the performance of the model over time. %matplotlib inline import pandas as pd csv_file_name = \"worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\" key = os.path.join(intermediate_folder_key, csv_file_name) wait_for_s3_object(s3_bucket, key, tmp_dir) csv_file = \"{}/{}\".format(tmp_dir, csv_file_name) df = pd.read_csv(csv_file) df = df.dropna(subset=['Training Reward']) x_axis = 'Episode #' y_axis = 'Training Reward' if len(df) > 0: plt = df.plot(x=x_axis,y=y_axis, figsize=(12,5), legend=True, style='b-') plt.set_ylabel(y_axis) plt.set_xlabel(x_axis) Visualize the rendered gifs The latest gif file found in the gifs directory is displayed. You can replace the tmp.gif file below to visualize other files generated. key = os.path.join(intermediate_folder_key, 'gifs') wait_for_s3_object(s3_bucket, key, tmp_dir) print(\"Copied gifs files to {}\".format(tmp_dir)) glob_pattern = os.path.join(\"{}/*.gif\".format(tmp_dir)) gifs = [file for file in glob.iglob(glob_pattern, recursive=True)] extract_episode = lambda string: int(re.search('.*episode-(\\d*)_.*', string, re.IGNORECASE).group(1)) gifs.sort(key=extract_episode) print(\"GIFs found:\\n{}\".format(\"\\n\".join([os.path.basename(gif) for gif in gifs]))) GIFs found: 2020-04-22-06-41-49_episode-79_score--32.02112377668456.gif # visualize a specific episode gif_index = -1 # since we want last gif gif_filepath = gifs[gif_index] gif_filename = os.path.basename(gif_filepath) print(\"Selected GIF: {}\".format(gif_filename)) os.system(\"mkdir -p ./src/tmp/ && cp {} ./src/tmp/{}.gif\".format(gif_filepath, gif_filename)) HTML('<img src=\"./src/tmp/{}.gif\">'.format(gif_filename)) Selected GIF: 2020-04-22-06-41-49_episode-79_score--32.02112377668456.gif Evaluation of RL models We use the last checkpointed model to run evaluation for the RL Agent. Load checkpointed model Checkpointed data from the previously trained models will be passed on for evaluation / inference in the checkpoint channel. In local mode, we can simply use the local directory, whereas in the SageMaker mode, it needs to be moved to S3 first. wait_for_s3_object(s3_bucket, output_tar_key, tmp_dir) if not os.path.isfile(\"{}/output.tar.gz\".format(tmp_dir)): raise FileNotFoundError(\"File output.tar.gz not found\") os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir)) if local_mode: checkpoint_dir = \"{}/data/checkpoint\".format(tmp_dir) else: checkpoint_dir = \"{}/checkpoint\".format(tmp_dir) print(\"Checkpoint directory {}\".format(checkpoint_dir)) if local_mode: checkpoint_path = 'file://{}'.format(checkpoint_dir) print(\"Local checkpoint file path: {}\".format(checkpoint_path)) else: checkpoint_path = \"s3://{}/{}/checkpoint/\".format(s3_bucket, job_name) if not os.listdir(checkpoint_dir): raise FileNotFoundError(\"Checkpoint files not found under the path\") os.system(\"aws s3 cp --recursive {} {}\".format(checkpoint_dir, checkpoint_path)) print(\"S3 checkpoint file path: {}\".format(checkpoint_path)) Run the evaluation step Use the checkpointed model to run the evaluation step. estimator_eval = RLEstimator(role=role, source_dir='src/', dependencies=[\"common/sagemaker_rl\"], toolkit=RLToolkit.COACH, toolkit_version='0.11.0', framework=RLFramework.MXNET, entry_point=\"evaluate-coach.py\", train_instance_count=1, train_instance_type=instance_type, hyperparameters = { \"RLCOACH_PRESET\": \"preset-mountain-car-continuous-clipped-ppo\", \"evaluate_steps\": 10000*2 # evaluate on 2 episodes } ) estimator_eval.fit({'checkpoint': checkpoint_path}) Visualize the output Optionally, you can run the steps defined earlier to visualize the output Model deployment Since we specified MXNet when configuring the RLEstimator, the MXNet deployment container will be used for hosting. # Amazon SageMaker Pricing # ml.t2.medium $0.065 # ml.t2.large $0.1299 # ml.t2.xlarge $0.2598 # ml.m4.xlarge $0.28 predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.t2.large\", entry_point='deploy-mxnet-coach.py') Using already existing model: rl-mountain-car-2020-04-22-06-34-27-307 -----------! We can test the endpoint with 2 samples observations. Starting with the car on the right side, but starting to fall back down the hill. output = predictor.predict(np.array([0.5, -0.5])) action = output[1][0] action [-0.15561622381210327] We see the policy decides to move the car to the left (negative value). And similarly in the other direction. output = predictor.predict(np.array([-0.5, 0.5])) action = output[1][0] action [0.20560654997825623] Clean up endpoint #predictor.delete_endpoint() References https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_mountain_car_coach_gymEnv https://github.com/lbnl-science-it/aws-sagemaker-rl","title":"AWS SageMaker: Reinforcement Learning"},{"location":"rl_mountain_car_coach_gymEnv/#mountain-car-with-amazon-sagemaker-rl","text":"AWS SageMaker Example: https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_mountain_car_coach_gymEnv","title":"Mountain Car with Amazon SageMaker RL"},{"location":"rl_mountain_car_coach_gymEnv/#overview","text":"Mountain Car is a classic control Reinforcement Learning problem that was first introduced by A. Moore in 1991 [1]. An under-powered car is tasked with climbing a steep mountain, and is only successful when it reaches the top. Luckily there's another mountain on the opposite side which can be used to gain momentum, and launch the car to the peak. It can be tricky to find this optimal solution due to the sparsity of the reward. Complex exploration strategies can be used to incentivise exploration of the mountain, but to keep things simple in this example we extend the amount of time in each episode from Open AI Gym's default of 200 environment steps to 10,000 steps, showing how to customise environments. We consider two variants in this example: PatientMountainCar for discrete actions and PatientContinuousMountainCar for continuous actions.","title":"Overview"},{"location":"rl_mountain_car_coach_gymEnv/#patientmountaincar","text":"Objective: Get the car to the top of the right hand side mountain. Environment(s): Open AI Gym's MountainCar-v0 that is extended to 10,000 steps per episode. State: Car's horizontal position and velocity (can be negative). Action: Direction of push (left, nothing or right). Reward: -1 for every environment step until success, which incentivises quick solutions.","title":"PatientMountainCar"},{"location":"rl_mountain_car_coach_gymEnv/#patientcontinuousmountaincar","text":"Objective: Get the car to the top of the right hand side mountain. Environment(s): Open AI Gym's MountainCarContinuous-v0 that is extended to 10,000 steps per episode. State: Car's horizontal position and velocity (can be negative). Action: Mmagnitude of push (if negative push to left, if positive push to right). Reward: +100 for reaching top of the right hand side mountain, minus the squared sum of actions from start to end. [1] A. Moore, Efficient Memory-Based Learning for Robot Control, PhD thesis, University of Cambridge, November 1990.","title":"PatientContinuousMountainCar"},{"location":"rl_mountain_car_coach_gymEnv/#prerequisites","text":"","title":"Prerequisites"},{"location":"rl_mountain_car_coach_gymEnv/#create-an-amazon-sagemaker-notebook","text":"","title":"Create an Amazon SageMaker Notebook"},{"location":"rl_mountain_car_coach_gymEnv/#open-jupyterlab","text":"","title":"Open JupyterLab"},{"location":"rl_mountain_car_coach_gymEnv/#clone-the-sample-code","text":"In JupyterLab , click the Terminal icon to open a new terminal To clone the tutorial repository , run the following command git clone https://github.com/lbnl-science-it/aws-sagemaker-rl.git In JupyterLab , navigate to aws-sagemaker-rl and open rl_mountain_car_coach_gymEnv.ipynb","title":"Clone the sample code"},{"location":"rl_mountain_car_coach_gymEnv/#contents","text":"rl_mountain_car_coach_gym.ipynb : notebook used for training Mountain Car policy src/patient_envs.py : custom environments defined here. src/train-coach.py : launcher for coach training src/preset-mountain-car-continuous-clipped-ppo.py : coach preset for Clipped PPO. src/preset-mountain-car-dqn.py : coach preset for DQN.","title":"Contents"},{"location":"rl_mountain_car_coach_gymEnv/#imports","text":"To get started, we'll import the Python libraries we need, set up the environment with a few prerequisites for permissions and configurations. import sagemaker import boto3 import sys import os import glob import re import subprocess import numpy as np from IPython.display import HTML import time from time import gmtime, strftime sys.path.append(\"common\") from misc import get_execution_role, wait_for_s3_object from sagemaker.rl import RLEstimator, RLToolkit, RLFramework","title":"Imports"},{"location":"rl_mountain_car_coach_gymEnv/#setup-s3-bucket","text":"Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. sage_session = sagemaker.session.Session() s3_bucket = sage_session.default_bucket() s3_output_path = 's3://{}/'.format(s3_bucket) print(\"S3 bucket path: {}\".format(s3_output_path)) S3 bucket path: s3://sagemaker-us-west-2-485444084140/","title":"Setup S3 bucket"},{"location":"rl_mountain_car_coach_gymEnv/#define-variables","text":"We define variables such as the job prefix for the training jobs and the image path for the container (only when this is BYOC). # create unique job name job_name_prefix = 'rl-mountain-car'","title":"Define Variables"},{"location":"rl_mountain_car_coach_gymEnv/#configure-settings","text":"You can run your RL training jobs on a SageMaker notebook instance or on your own machine. In both of these scenarios, you can run in either 'local' (where you run the commands) or 'SageMaker' mode (on SageMaker training instances). 'local' mode uses the SageMaker Python SDK to run your code in Docker containers locally. It can speed up iterative testing and debugging while using the same familiar Python SDK interface. Just set local_mode = True . And when you're ready move to 'SageMaker' mode to scale things up. # run in local mode? local_mode = False","title":"Configure settings"},{"location":"rl_mountain_car_coach_gymEnv/#create-an-iam-role","text":"Either get the execution role when running from a SageMaker notebook instance role = sagemaker.get_execution_role() or, when running from local notebook instance, use utils method role = get_execution_role() to create an execution role. try: role = sagemaker.get_execution_role() except: role = get_execution_role() print(\"Using IAM role arn: {}\".format(role))","title":"Create an IAM role"},{"location":"rl_mountain_car_coach_gymEnv/#install-docker-for-local-mode","text":"In order to work in local mode, you need to have docker installed. When running from you local machine, please make sure that you have docker or docker-compose (for local CPU machines) and nvidia-docker (for local GPU machines) installed. Alternatively, when running from a SageMaker notebook instance, you can simply run the following script to install dependenceis. Note, you can only run a single local notebook at one time. # only run from SageMaker notebook instance if local_mode: !/bin/bash ./common/setup.sh","title":"Install docker for local mode"},{"location":"rl_mountain_car_coach_gymEnv/#setup-the-environment","text":"We create a file called src/patient_envs.py for our modified environments. We can create a custom environment class or create a function that returns our environment. Since we're using Open AI Gym environment and wrappers, we just create functions that take the classic control environments MountainCarEnv and Continuous_MountainCarEnv and wrap them with a TimeLimit where we specify the max_episode_steps to 10,000. !pygmentize src/patient_envs.py \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgym.envs.classic_control.mountain_car\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MountainCarEnv \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgym.envs.classic_control.continuous_mountain_car\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Continuous_MountainCarEnv \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgym.wrappers.time_limit\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TimeLimit \u001b[34mdef\u001b[39;49;00m \u001b[32mPatientMountainCar\u001b[39;49;00m(): env = MountainCarEnv() \u001b[34mreturn\u001b[39;49;00m TimeLimit(env, max_episode_steps=\u001b[34m10000\u001b[39;49;00m) \u001b[34mdef\u001b[39;49;00m \u001b[32mPatientContinuousMountainCar\u001b[39;49;00m(): env = Continuous_MountainCarEnv() \u001b[34mreturn\u001b[39;49;00m TimeLimit(env, max_episode_steps=\u001b[34m10000\u001b[39;49;00m)","title":"Setup the environment"},{"location":"rl_mountain_car_coach_gymEnv/#configure-the-presets-for-rl-algorithm","text":"The presets that configure the RL training jobs are defined in the \"preset-mountain-car-continuous-clipped-ppo.py\" file which is also uploaded on the /src directory. Also see \"preset-mountain-car-dqn.py\" for the discrete environment case. Using the preset file, you can define agent parameters to select the specific agent algorithm. You can also set the environment parameters, define the schedule and visualization parameters, and define the graph manager. The schedule presets will define the number of heat up steps, periodic evaluation steps, training steps between evaluations. These can be overridden at runtime by specifying the RLCOACH_PRESET hyperparameter. Additionally, it can be used to define custom hyperparameters. !pygmentize src/preset-mountain-car-continuous-clipped-ppo.py \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.agents.clipped_ppo_agent\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClippedPPOAgentParameters \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.architectures.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VisualizationParameters, PresetValidationParameters \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.core_types\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, RunPhase, \\ SelectedPhaseOnlyDumpFilter, MaxDumpFilter \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.environments.gym_environment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GymVectorEnvironment \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.exploration_policies.e_greedy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m EGreedyParameters \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.filters.observation.observation_normalization_filter\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ObservationNormalizationFilter \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.basic_rl_graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BasicRLGraphManager \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ScheduleParameters \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.schedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LinearSchedule \u001b[37m####################\u001b[39;49;00m \u001b[37m# Graph Scheduling #\u001b[39;49;00m \u001b[37m####################\u001b[39;49;00m schedule_params = ScheduleParameters() schedule_params.improve_steps = EnvironmentEpisodes(\u001b[34m100\u001b[39;49;00m) schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m) schedule_params.evaluation_steps = EnvironmentEpisodes(\u001b[34m1\u001b[39;49;00m) schedule_params.heatup_steps = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m) \u001b[37m#########\u001b[39;49;00m \u001b[37m# Agent #\u001b[39;49;00m \u001b[37m#########\u001b[39;49;00m agent_params = ClippedPPOAgentParameters() agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].learning_rate = \u001b[34m0.001\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].input_embedders_parameters[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].activation_function = \u001b[33m'\u001b[39;49;00m\u001b[33mtanh\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].input_embedders_parameters[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].scheme = [Dense(\u001b[34m32\u001b[39;49;00m)] agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].middleware_parameters.scheme = [Dense(\u001b[34m32\u001b[39;49;00m)] agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].middleware_parameters.activation_function = \u001b[33m'\u001b[39;49;00m\u001b[33mtanh\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].batch_size = \u001b[34m256\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].optimizer_epsilon = \u001b[34m1e-5\u001b[39;49;00m agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].adam_optimizer_beta2 = \u001b[34m0.999\u001b[39;49;00m agent_params.algorithm.clip_likelihood_ratio_using_epsilon = \u001b[34m0.3\u001b[39;49;00m agent_params.algorithm.clipping_decay_schedule = LinearSchedule(\u001b[34m0.5\u001b[39;49;00m, \u001b[34m0.1\u001b[39;49;00m, \u001b[34m10000\u001b[39;49;00m * \u001b[34m50\u001b[39;49;00m) agent_params.algorithm.beta_entropy = \u001b[34m0\u001b[39;49;00m agent_params.algorithm.gae_lambda = \u001b[34m0.95\u001b[39;49;00m agent_params.algorithm.discount = \u001b[34m0.999\u001b[39;49;00m agent_params.algorithm.estimate_state_value_using_gae = \u001b[36mTrue\u001b[39;49;00m agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m) agent_params.algorithm.num_episodes_in_experience_replay = \u001b[34m100\u001b[39;49;00m agent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(\u001b[34m10\u001b[39;49;00m) agent_params.algorithm.optimization_epochs = \u001b[34m10\u001b[39;49;00m agent_params.pre_network_filter.add_observation_filter(\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mnormalize_observation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, ObservationNormalizationFilter(name=\u001b[33m'\u001b[39;49;00m\u001b[33mnormalize_observation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)) \u001b[37m###############\u001b[39;49;00m \u001b[37m# Environment #\u001b[39;49;00m \u001b[37m###############\u001b[39;49;00m env_params = GymVectorEnvironment(level=\u001b[33m'\u001b[39;49;00m\u001b[33mpatient_envs:PatientContinuousMountainCar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[37m#################\u001b[39;49;00m \u001b[37m# Visualization #\u001b[39;49;00m \u001b[37m#################\u001b[39;49;00m vis_params = VisualizationParameters() vis_params.dump_gifs = \u001b[36mTrue\u001b[39;49;00m vis_params.video_dump_filters = [SelectedPhaseOnlyDumpFilter(RunPhase.TEST), MaxDumpFilter()] \u001b[37m########\u001b[39;49;00m \u001b[37m# Test #\u001b[39;49;00m \u001b[37m########\u001b[39;49;00m preset_validation_params = PresetValidationParameters() preset_validation_params.test = \u001b[36mTrue\u001b[39;49;00m preset_validation_params.min_reward_threshold = \u001b[34m150\u001b[39;49;00m preset_validation_params.max_episodes_to_achieve_reward = \u001b[34m250\u001b[39;49;00m graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params, schedule_params=schedule_params, vis_params=vis_params, preset_validation_params=preset_validation_params)","title":"Configure the presets for RL algorithm"},{"location":"rl_mountain_car_coach_gymEnv/#write-the-training-code","text":"The training code is written in the file \u201ctrain-coach.py\u201d which is uploaded in the /src directory. We create a custom SageMakerCoachPresetLauncher which sets the default preset, maps and ties hyperparameters. !pygmentize src/train-coach.py \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_rl.coach_launcher\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerCoachPresetLauncher \u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyLauncher\u001b[39;49;00m(SageMakerCoachPresetLauncher): \u001b[34mdef\u001b[39;49;00m \u001b[32mdefault_preset_name\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m): \u001b[33m\"\"\"This points to a .py file that configures everything about the RL job.\u001b[39;49;00m \u001b[33m It can be overridden at runtime by specifying the RLCOACH_PRESET hyperparameter.\u001b[39;49;00m \u001b[33m \"\"\"\u001b[39;49;00m \u001b[34mreturn\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mpreset-mountaincarcontinuous-clippedppo.py\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mdef\u001b[39;49;00m \u001b[32mmap_hyperparameter\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, name, value): \u001b[33m\"\"\"Here we configure some shortcut names for hyperparameters that we expect to use frequently.\u001b[39;49;00m \u001b[33m Essentially anything in the preset file can be overridden through a hyperparameter with a name \u001b[39;49;00m \u001b[33m like \"rl.agent_params.algorithm.etc\". \u001b[39;49;00m \u001b[33m \"\"\"\u001b[39;49;00m \u001b[37m# maps from alias (key) to fully qualified coach parameter (value)\u001b[39;49;00m mapping = { \u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation_freq_env_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.steps_between_evaluation_periods:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation_episodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.evaluation_steps:EnvironmentEpisodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mimprove_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.improve_steps:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mdiscount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.discount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mgae_lambda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.gae_lambda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_freq_env_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.num_consecutive_playing_steps:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.network_wrappers.main.learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.network_wrappers.main.batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining_epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.optimization_epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m } \u001b[34mif\u001b[39;49;00m name \u001b[35min\u001b[39;49;00m mapping: \u001b[36mself\u001b[39;49;00m.apply_hyperparameter(mapping[name], value) \u001b[34melse\u001b[39;49;00m: \u001b[36msuper\u001b[39;49;00m().map_hyperparameter(name, value) \u001b[34mdef\u001b[39;49;00m \u001b[32mget_config_args\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, parser): args = \u001b[36msuper\u001b[39;49;00m().get_config_args(parser) \u001b[37m# Above line creates `self.hyperparameters` which is a collection of hyperparameters\u001b[39;49;00m \u001b[37m# that are to be added to graph manager when it's created. At this stage they are \u001b[39;49;00m \u001b[37m# fully qualified names since already passed through `map_hyperparameter`.\u001b[39;49;00m \u001b[37m# Keeps target network the same for all epochs of a single 'policy training' stage.\u001b[39;49;00m src_hp = \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.num_consecutive_playing_steps:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m target_hp = \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.num_steps_between_copying_online_weights_to_target:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict.get(src_hp, \u001b[36mFalse\u001b[39;49;00m): src_val = \u001b[36mint\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict[src_hp]) \u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict[target_hp] = src_val \u001b[37m# Evaluate after each 'policy training' stage\u001b[39;49;00m src_hp = \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.num_consecutive_playing_steps:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m target_hp = \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.steps_between_evaluation_periods:EnvironmentSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict.get(src_hp, \u001b[36mFalse\u001b[39;49;00m): src_val = \u001b[36mint\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict[src_hp]) \u001b[36mself\u001b[39;49;00m.hyperparameters.hp_dict[target_hp] = src_val \u001b[34mreturn\u001b[39;49;00m args \u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: MyLauncher.train_main()","title":"Write the Training Code"},{"location":"rl_mountain_car_coach_gymEnv/#train-the-rl-model-using-the-python-sdk-script-mode","text":"If you are using local mode, the training will run on the notebook instance. When using SageMaker for training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs. Specify the source directory where the environment, presets and training code is uploaded. Specify the entry point as the training code Specify the choice of RL toolkit and framework. This automatically resolves to the ECR path for the RL Container. Define the training parameters such as the instance count, job name, S3 path for output and job name. Specify the hyperparameters for the RL agent algorithm. The RLCOACH_PRESET can be used to specify the RL agent algorithm you want to use. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks. We use a variant of Proximal Policy Optimization (PPO) called Clipped PPO, which removes the need for complex KL divergence calculations. if local_mode: instance_type = 'local' else: instance_type = \"ml.m4.4xlarge\" estimator = RLEstimator(entry_point=\"train-coach.py\", source_dir='src', dependencies=[\"common/sagemaker_rl\"], toolkit=RLToolkit.COACH, toolkit_version='0.11.0', framework=RLFramework.MXNET, role=role, train_instance_type=instance_type, train_instance_count=1, output_path=s3_output_path, base_job_name=job_name_prefix, hyperparameters = { \"RLCOACH_PRESET\": \"preset-mountain-car-continuous-clipped-ppo\", # \"preset-mountain-car-dqn\", \"discount\": 0.995, \"gae_lambda\": 0.997, \"evaluation_episodes\": 3, # approx 100 episodes \"improve_steps\": 100000, # approx 5 episodes to start with \"training_freq_env_steps\": 75000, \"training_learning_rate\": 0.004, \"training_batch_size\": 256, # times number below by training_freq_env_steps to get total samples per policy training \"training_epochs\": 15, 'save_model': 1 } ) estimator.fit(wait=local_mode)","title":"Train the RL model using the Python SDK Script mode"},{"location":"rl_mountain_car_coach_gymEnv/#store-intermediate-training-output-and-model-checkpoints","text":"The output from the training job above is stored on S3. The intermediate folder contains gifs and metadata of the training. job_name=estimator._current_job_name print(\"Job name: {}\".format(job_name)) s3_url = \"s3://{}/{}\".format(s3_bucket,job_name) if local_mode: output_tar_key = \"{}/output.tar.gz\".format(job_name) else: output_tar_key = \"{}/output/output.tar.gz\".format(job_name) intermediate_folder_key = \"{}/output/intermediate/\".format(job_name) output_url = \"s3://{}/{}\".format(s3_bucket, output_tar_key) intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key) print(\"S3 job path: {}\".format(s3_url)) print(\"Output.tar.gz location: {}\".format(output_url)) print(\"Intermediate folder path: {}\".format(intermediate_url)) tmp_dir = \"/tmp/{}\".format(job_name) os.system(\"mkdir {}\".format(tmp_dir)) print(\"Create local folder {}\".format(tmp_dir))","title":"Store intermediate training output and model checkpoints"},{"location":"rl_mountain_car_coach_gymEnv/#visualization","text":"","title":"Visualization"},{"location":"rl_mountain_car_coach_gymEnv/#plot-metrics-for-training-job","text":"We can pull the reward metric of the training and plot it to see the performance of the model over time. %matplotlib inline import pandas as pd csv_file_name = \"worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\" key = os.path.join(intermediate_folder_key, csv_file_name) wait_for_s3_object(s3_bucket, key, tmp_dir) csv_file = \"{}/{}\".format(tmp_dir, csv_file_name) df = pd.read_csv(csv_file) df = df.dropna(subset=['Training Reward']) x_axis = 'Episode #' y_axis = 'Training Reward' if len(df) > 0: plt = df.plot(x=x_axis,y=y_axis, figsize=(12,5), legend=True, style='b-') plt.set_ylabel(y_axis) plt.set_xlabel(x_axis)","title":"Plot metrics for training job"},{"location":"rl_mountain_car_coach_gymEnv/#visualize-the-rendered-gifs","text":"The latest gif file found in the gifs directory is displayed. You can replace the tmp.gif file below to visualize other files generated. key = os.path.join(intermediate_folder_key, 'gifs') wait_for_s3_object(s3_bucket, key, tmp_dir) print(\"Copied gifs files to {}\".format(tmp_dir)) glob_pattern = os.path.join(\"{}/*.gif\".format(tmp_dir)) gifs = [file for file in glob.iglob(glob_pattern, recursive=True)] extract_episode = lambda string: int(re.search('.*episode-(\\d*)_.*', string, re.IGNORECASE).group(1)) gifs.sort(key=extract_episode) print(\"GIFs found:\\n{}\".format(\"\\n\".join([os.path.basename(gif) for gif in gifs]))) GIFs found: 2020-04-22-06-41-49_episode-79_score--32.02112377668456.gif # visualize a specific episode gif_index = -1 # since we want last gif gif_filepath = gifs[gif_index] gif_filename = os.path.basename(gif_filepath) print(\"Selected GIF: {}\".format(gif_filename)) os.system(\"mkdir -p ./src/tmp/ && cp {} ./src/tmp/{}.gif\".format(gif_filepath, gif_filename)) HTML('<img src=\"./src/tmp/{}.gif\">'.format(gif_filename)) Selected GIF: 2020-04-22-06-41-49_episode-79_score--32.02112377668456.gif","title":"Visualize the rendered gifs"},{"location":"rl_mountain_car_coach_gymEnv/#evaluation-of-rl-models","text":"We use the last checkpointed model to run evaluation for the RL Agent.","title":"Evaluation of RL models"},{"location":"rl_mountain_car_coach_gymEnv/#load-checkpointed-model","text":"Checkpointed data from the previously trained models will be passed on for evaluation / inference in the checkpoint channel. In local mode, we can simply use the local directory, whereas in the SageMaker mode, it needs to be moved to S3 first. wait_for_s3_object(s3_bucket, output_tar_key, tmp_dir) if not os.path.isfile(\"{}/output.tar.gz\".format(tmp_dir)): raise FileNotFoundError(\"File output.tar.gz not found\") os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir)) if local_mode: checkpoint_dir = \"{}/data/checkpoint\".format(tmp_dir) else: checkpoint_dir = \"{}/checkpoint\".format(tmp_dir) print(\"Checkpoint directory {}\".format(checkpoint_dir)) if local_mode: checkpoint_path = 'file://{}'.format(checkpoint_dir) print(\"Local checkpoint file path: {}\".format(checkpoint_path)) else: checkpoint_path = \"s3://{}/{}/checkpoint/\".format(s3_bucket, job_name) if not os.listdir(checkpoint_dir): raise FileNotFoundError(\"Checkpoint files not found under the path\") os.system(\"aws s3 cp --recursive {} {}\".format(checkpoint_dir, checkpoint_path)) print(\"S3 checkpoint file path: {}\".format(checkpoint_path))","title":"Load checkpointed model"},{"location":"rl_mountain_car_coach_gymEnv/#run-the-evaluation-step","text":"Use the checkpointed model to run the evaluation step. estimator_eval = RLEstimator(role=role, source_dir='src/', dependencies=[\"common/sagemaker_rl\"], toolkit=RLToolkit.COACH, toolkit_version='0.11.0', framework=RLFramework.MXNET, entry_point=\"evaluate-coach.py\", train_instance_count=1, train_instance_type=instance_type, hyperparameters = { \"RLCOACH_PRESET\": \"preset-mountain-car-continuous-clipped-ppo\", \"evaluate_steps\": 10000*2 # evaluate on 2 episodes } ) estimator_eval.fit({'checkpoint': checkpoint_path})","title":"Run the evaluation step"},{"location":"rl_mountain_car_coach_gymEnv/#visualize-the-output","text":"Optionally, you can run the steps defined earlier to visualize the output","title":"Visualize the output"},{"location":"rl_mountain_car_coach_gymEnv/#model-deployment","text":"Since we specified MXNet when configuring the RLEstimator, the MXNet deployment container will be used for hosting. # Amazon SageMaker Pricing # ml.t2.medium $0.065 # ml.t2.large $0.1299 # ml.t2.xlarge $0.2598 # ml.m4.xlarge $0.28 predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.t2.large\", entry_point='deploy-mxnet-coach.py') Using already existing model: rl-mountain-car-2020-04-22-06-34-27-307 -----------! We can test the endpoint with 2 samples observations. Starting with the car on the right side, but starting to fall back down the hill. output = predictor.predict(np.array([0.5, -0.5])) action = output[1][0] action [-0.15561622381210327] We see the policy decides to move the car to the left (negative value). And similarly in the other direction. output = predictor.predict(np.array([-0.5, 0.5])) action = output[1][0] action [0.20560654997825623]","title":"Model deployment"},{"location":"rl_mountain_car_coach_gymEnv/#clean-up-endpoint","text":"#predictor.delete_endpoint()","title":"Clean up endpoint"},{"location":"rl_mountain_car_coach_gymEnv/#references","text":"https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_mountain_car_coach_gymEnv https://github.com/lbnl-science-it/aws-sagemaker-rl","title":"References"},{"location":"sagemaker_keras_text_classification/","text":"Overview In this tutorial, we will use Amazon SageMaker to build, train and deploy an NLP model using custom built TensorFlow Docker containers. The NLP model will classify news articles into the appropriate news category. To train the model, we will be using the UCI News Dataset which contains a list of about 420K articles and their appropriate categories (labels). There are four categories: Business (b) Science & Technology (t) Entertainment (e) Health & Medicine (m) Prerequisites: Create an Amazon SageMaker Notebook Open JupyterLab Local Test: Clone the sample code In JupyterLab , click the Terminal icon to open a new terminal To clone the tutorial repository , run the following commands git https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification.git cd aws-sagemaker-keras-text-classification Training and hosting the text classifier locally (on the JupyterLab notebook instance) cd container sh build_docker_local_test.sh Prediction Open another terminal in JupyterLab cd container/local_test ./predict.sh input.json application/json Data Exploration Navigate to aws-sagemaker-keras-text-classification and open sagemaker_keras_text_classification.ipynb Execute code in JupyterLab notebook import pandas as pd import tensorflow as tf import re import numpy as np import os from tensorflow.python.keras.preprocessing.text import Tokenizer from tensorflow.python.keras.preprocessing.sequence import pad_sequences from tensorflow.python.keras.utils import to_categorical WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead. column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"] news_dataset = pd.read_csv(os.path.join('./data', 'newsCorpora.csv'), names=column_names, header=None, delimiter='\\t') news_dataset.head() TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP Fed official says weak data caused by weather,... http://www.latimes.com/business/money/la-fi-mo... Los Angeles Times b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.latimes.com 1394470370698 news_dataset.groupby(['CATEGORY']).size() CATEGORY b 115967 e 152469 m 45639 t 108344 dtype: int64 Training and Hosting your Algorithm in Amazon SageMaker Building and registering the container The following shell code shows how to build the container image using docker build and push the container image to ECR using docker push . This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it. %%sh cd container sh build_docker.sh Login Succeeded Stopping docker: [ OK ] Starting docker: .[ OK ] Sending build context to Docker daemon 456.3MB Step 1/9 : FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:1.14.0-cpu-py36-ubuntu16.04 ---> e6a210ff54e4 Step 2/9 : RUN apt-get update && apt-get install -y nginx imagemagick graphviz ---> Using cache ---> 39d868b5172a Step 3/9 : RUN pip install --upgrade pip ---> Using cache ---> 83610cd980fc Step 4/9 : RUN pip install gevent gunicorn flask tensorflow_hub seqeval graphviz nltk spacy tqdm ---> Using cache ---> 090ab4fa935c Step 5/9 : RUN python -m spacy download en_core_web_sm ---> Using cache ---> cf5c0166e852 Step 6/9 : RUN python -m spacy download en ---> Using cache ---> d39488c451a6 Step 7/9 : ENV PATH=\"/opt/program:${PATH}\" ---> Using cache ---> 1b3759031fe0 Step 8/9 : COPY sagemaker_keras_text_classification /opt/program ---> Using cache ---> 092b378f8446 Step 9/9 : WORKDIR /opt/program ---> Using cache ---> b468053ed126 Successfully built b468053ed126 Successfully tagged sagemaker-keras-text-classification:latest The push refers to repository [485444084140.dkr.ecr.us-west-2.amazonaws.com/sagemaker-keras-text-classification] 6738a352b9b5: Preparing 1084a39dd41b: Preparing 394cd2fefa75: Preparing a8288ebab0e0: Preparing a71b5abf35ed: Preparing 0ba58b74173a: Preparing cc978a7bbd2a: Preparing 3a97a8d562fb: Preparing cb460459ddc8: Preparing b4064660a4cf: Preparing b6e9883adafa: Preparing 9ee6d909e5a7: Preparing e722e212cbab: Preparing 708ade65e147: Preparing 11fc4467b8a3: Preparing 0cf88c3675cd: Preparing d456742927ee: Preparing 8722c9641a57: Preparing 7083756ef61f: Preparing 9d2fda619715: Preparing e79142719515: Preparing aeda103e78c9: Preparing 2558e637fbff: Preparing f749b9b0fb21: Preparing e722e212cbab: Waiting 708ade65e147: Waiting 11fc4467b8a3: Waiting 0cf88c3675cd: Waiting d456742927ee: Waiting 8722c9641a57: Waiting 7083756ef61f: Waiting 9d2fda619715: Waiting e79142719515: Waiting aeda103e78c9: Waiting 2558e637fbff: Waiting f749b9b0fb21: Waiting 0ba58b74173a: Waiting cb460459ddc8: Waiting cc978a7bbd2a: Waiting 3a97a8d562fb: Waiting 9ee6d909e5a7: Waiting b6e9883adafa: Waiting 1084a39dd41b: Pushed 6738a352b9b5: Pushed cc978a7bbd2a: Layer already exists 3a97a8d562fb: Layer already exists cb460459ddc8: Layer already exists 394cd2fefa75: Pushed b4064660a4cf: Layer already exists b6e9883adafa: Layer already exists 9ee6d909e5a7: Layer already exists a71b5abf35ed: Pushed e722e212cbab: Layer already exists 708ade65e147: Layer already exists 0cf88c3675cd: Layer already exists 11fc4467b8a3: Layer already exists d456742927ee: Layer already exists 7083756ef61f: Layer already exists 8722c9641a57: Layer already exists 9d2fda619715: Layer already exists e79142719515: Layer already exists 2558e637fbff: Layer already exists aeda103e78c9: Layer already exists f749b9b0fb21: Layer already exists a8288ebab0e0: Pushed 0ba58b74173a: Pushed latest: digest: sha256:c684d06918d8e4c9ca2db9c35a663d044c89816a01df6fc34ddfd09b449c65ab size: 5344 WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Once you have your container packaged, you can use it to train and serve models. Let's do that with the algorithm we made above. Set up the environment Here we specify a bucket to use and the role that will be used for working with SageMaker. ### S3 prefix prefix = 'sagemaker-keras-text-classification' ### Define IAM role import boto3 import re import os import numpy as np import pandas as pd from sagemaker import get_execution_role role = get_execution_role() Create the session The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations. import sagemaker as sage from time import gmtime, strftime sess = sage.Session() Upload the data for training When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. We can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. WORK_DIRECTORY = 'data' data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix) Create an estimator and fit the model In order to use SageMaker to fit our algorithm, we'll create an Estimator that defines how to use the container to to train. This includes the configuration we need to invoke SageMaker training: The container name . This is constucted as in the shell commands above. The role . As defined above. The instance count which is the number of machines to use for training. The instance type which is the type of machine to use for training. The output path determines where the model artifact will be written. The session is the SageMaker session object that we defined above. Then we use fit() on the estimator to train against the data that we uploaded above. account = sess.boto_session.client('sts').get_caller_identity()['Account'] region = sess.boto_session.region_name image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-keras-text-classification'.format(account, region) tree = sage.estimator.Estimator(image, role, 1, 'ml.c5.2xlarge', output_path=\"s3://{}/output\".format(sess.default_bucket()), sagemaker_session=sess) To view the progress of training: navigate to Amazon SageMaker Studio > Training > Training Jobs tree.fit(data_location) 2020-04-22 01:18:26 Starting - Starting the training job... 2020-04-22 01:18:27 Starting - Launching requested ML instances...... 2020-04-22 01:19:56 Starting - Preparing the instances for training...... 2020-04-22 01:20:56 Downloading - Downloading input data 2020-04-22 01:20:56 Training - Downloading the training image...... 2020-04-22 01:21:50 Training - Training image download completed. Training in progress.\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m \u001b[34mStarting the training.\u001b[0m \u001b[34m TITLE ... TIMESTAMP\u001b[0m \u001b[34m1 Fed official says weak data caused by weather,... ... 1394470370698\u001b[0m \u001b[34m2 Fed's Charles Plosser sees high bar for change... ... 1394470371207\u001b[0m \u001b[34m3 US open: Stocks fall after Fed official hints ... ... 1394470371550\u001b[0m \u001b[34m4 Fed risks falling 'behind the curve', Charles ... ... 1394470371793\u001b[0m \u001b[34m5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... ... 1394470372027 \u001b[0m \u001b[34m[5 rows x 7 columns]\u001b[0m \u001b[34mFound 65990 unique tokens.\u001b[0m \u001b[34mShape of data tensor: (422417, 100)\u001b[0m \u001b[34mShape of label tensor: (422417, 4)\u001b[0m \u001b[34mx_train shape: (337933, 100)\u001b[0m \u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m \u001b[34mW0422 01:22:22.159757 140619530934016 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m \u001b[34mInstructions for updating:\u001b[0m \u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m \u001b[34mW0422 01:22:22.187488 140619530934016 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m \u001b[34mInstructions for updating:\u001b[0m \u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m \u001b[34mModel: \"sequential\"\u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34mLayer (type) Output Shape Param # \u001b[0m \u001b[34m=================================================================\u001b[0m \u001b[34membedding (Embedding) (None, 100, 100) 1000000 \u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34mflatten (Flatten) (None, 10000) 0 \u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34mdense (Dense) (None, 2) 20002 \u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34mdense_1 (Dense) (None, 4) 12 \u001b[0m \u001b[34m=================================================================\u001b[0m \u001b[34mTotal params: 1,020,014\u001b[0m \u001b[34mTrainable params: 1,020,014\u001b[0m \u001b[34mNon-trainable params: 0\u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34m2020-04-22 01:22:22.217855: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX512F\u001b[0m \u001b[34mTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m \u001b[34m2020-04-22 01:22:22.259738: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\u001b[0m \u001b[34m2020-04-22 01:22:22.260199: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23b2eb0 executing computations on platform Host. Devices:\u001b[0m \u001b[34m2020-04-22 01:22:22.260224: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): <undefined>, <undefined>\u001b[0m \u001b[34m2020-04-22 01:22:22.261048: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\u001b[0m \u001b[34m2020-04-22 01:22:22.291480: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set. If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU. To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m \u001b[34mTrain on 337933 samples, validate on 84484 samples\u001b[0m \u001b[34mEpoch 1/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.6303 - acc: 0.7653 - val_loss: 0.5580 - val_acc: 0.7959\u001b[0m \u001b[34mEpoch 2/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.5502 - acc: 0.7977 - val_loss: 0.5558 - val_acc: 0.7955\u001b[0m \u001b[34mEpoch 3/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.5427 - acc: 0.8000 - val_loss: 0.5450 - val_acc: 0.8004\u001b[0m \u001b[34mEpoch 4/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.5391 - acc: 0.8010 - val_loss: 0.5407 - val_acc: 0.8018\u001b[0m \u001b[34mEpoch 5/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.5363 - acc: 0.8022 - val_loss: 0.5403 - val_acc: 0.8035\u001b[0m \u001b[34mEpoch 6/6\u001b[0m 2020-04-22 01:25:23 Uploading - Uploading generated training model 2020-04-22 01:25:23 Completed - Training job completed \u001b[34m337933/337933 - 29s - loss: 0.5342 - acc: 0.8032 - val_loss: 0.5424 - val_acc: 0.8014\u001b[0m \u001b[34mTraining complete. Now saving model to: /opt/ml/model\u001b[0m \u001b[34mW0422 01:25:15.442514 140619530934016 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m \u001b[34mInstructions for updating:\u001b[0m \u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m \u001b[34mW0422 01:25:15.442924 140619530934016 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m \u001b[34mInstructions for updating:\u001b[0m \u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m \u001b[34mTest headline: What Improved Tech Means for Electric, Self-Driving and Flying Cars\u001b[0m \u001b[34mPredicted category: b\u001b[0m Training seconds: 283 Billable seconds: 283 Deploy the model Deploying the model to SageMaker hosting just requires a deploy call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint. This step may take about 10-20 min from sagemaker.predictor import json_serializer predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=json_serializer) -------------! Prediction request = { \"input\": \"\u2018Deadpool 2\u2019 Has More Swearing, Slicing and Dicing from Ryan Reynolds\"} print(predictor.predict(request).decode('utf-8')) {\"result\": \"Entertainment\"} Endpoint Test To view the endpoint_name you just deployed: navigate to Amazon SageMaker Studio > Inference > Endpoints InvokeEndpoint using boto3 import json import boto3 client = boto3.client('runtime.sagemaker') endpoint_name = 'sagemaker-keras-text-classification-2020-04-22-01-18-26-512' payload = { \"input\": \"\u2018Deadpool 2\u2019 Has More Swearing, Slicing and Dicing from Ryan Reynolds\"} response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Accept='text/plain', Body=json.dumps(payload)) response_body = response['Body'] prediction = response_body.read().decode('utf-8') print(prediction) {\"result\": \"Entertainment\"} InvokeEndpoint using AWS CLI %%sh ENDPOINT_NAME=\"sagemaker-keras-text-classification-2020-04-22-01-18-26-512\" CONTENT_TYPE='application/json' aws sagemaker-runtime invoke-endpoint \\ --endpoint-name ${ENDPOINT_NAME} \\ --content-type ${CONTENT_TYPE} \\ --body '{\"input\": \"Why Exercise Alone May Not Be the Key to Weight Loss\"}' prediction_response.json cat prediction_response.json { \"ContentType\": \"application/json\", \"InvokedProductionVariant\": \"AllTraffic\" } {\"result\": \"Health & Medicine\"} Optional cleanup When you're done with the endpoint, you'll want to clean it up. sess.delete_endpoint(predictor.endpoint) References https://github.com/aws-samples/amazon-sagemaker-keras-text-classification https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks.html","title":"AWS SageMaker: Text Classification Using Keras & TensorFlow"},{"location":"sagemaker_keras_text_classification/#overview","text":"In this tutorial, we will use Amazon SageMaker to build, train and deploy an NLP model using custom built TensorFlow Docker containers. The NLP model will classify news articles into the appropriate news category. To train the model, we will be using the UCI News Dataset which contains a list of about 420K articles and their appropriate categories (labels). There are four categories: Business (b) Science & Technology (t) Entertainment (e) Health & Medicine (m)","title":"Overview"},{"location":"sagemaker_keras_text_classification/#prerequisites","text":"","title":"Prerequisites:"},{"location":"sagemaker_keras_text_classification/#create-an-amazon-sagemaker-notebook","text":"","title":"Create an Amazon SageMaker Notebook"},{"location":"sagemaker_keras_text_classification/#open-jupyterlab","text":"","title":"Open JupyterLab"},{"location":"sagemaker_keras_text_classification/#local-test","text":"","title":"Local Test:"},{"location":"sagemaker_keras_text_classification/#clone-the-sample-code","text":"In JupyterLab , click the Terminal icon to open a new terminal To clone the tutorial repository , run the following commands git https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification.git cd aws-sagemaker-keras-text-classification","title":"Clone the sample code"},{"location":"sagemaker_keras_text_classification/#training-and-hosting-the-text-classifier-locally-on-the-jupyterlab-notebook-instance","text":"cd container sh build_docker_local_test.sh","title":"Training and hosting the text classifier locally (on the JupyterLab notebook instance)"},{"location":"sagemaker_keras_text_classification/#prediction","text":"Open another terminal in JupyterLab cd container/local_test ./predict.sh input.json application/json","title":"Prediction"},{"location":"sagemaker_keras_text_classification/#data-exploration","text":"Navigate to aws-sagemaker-keras-text-classification and open sagemaker_keras_text_classification.ipynb Execute code in JupyterLab notebook import pandas as pd import tensorflow as tf import re import numpy as np import os from tensorflow.python.keras.preprocessing.text import Tokenizer from tensorflow.python.keras.preprocessing.sequence import pad_sequences from tensorflow.python.keras.utils import to_categorical WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead. column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"] news_dataset = pd.read_csv(os.path.join('./data', 'newsCorpora.csv'), names=column_names, header=None, delimiter='\\t') news_dataset.head() TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP Fed official says weak data caused by weather,... http://www.latimes.com/business/money/la-fi-mo... Los Angeles Times b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.latimes.com 1394470370698 news_dataset.groupby(['CATEGORY']).size() CATEGORY b 115967 e 152469 m 45639 t 108344 dtype: int64","title":"Data Exploration"},{"location":"sagemaker_keras_text_classification/#training-and-hosting-your-algorithm-in-amazon-sagemaker","text":"","title":"Training and Hosting your Algorithm in Amazon SageMaker"},{"location":"sagemaker_keras_text_classification/#building-and-registering-the-container","text":"The following shell code shows how to build the container image using docker build and push the container image to ECR using docker push . This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it. %%sh cd container sh build_docker.sh Login Succeeded Stopping docker: [ OK ] Starting docker: .[ OK ] Sending build context to Docker daemon 456.3MB Step 1/9 : FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:1.14.0-cpu-py36-ubuntu16.04 ---> e6a210ff54e4 Step 2/9 : RUN apt-get update && apt-get install -y nginx imagemagick graphviz ---> Using cache ---> 39d868b5172a Step 3/9 : RUN pip install --upgrade pip ---> Using cache ---> 83610cd980fc Step 4/9 : RUN pip install gevent gunicorn flask tensorflow_hub seqeval graphviz nltk spacy tqdm ---> Using cache ---> 090ab4fa935c Step 5/9 : RUN python -m spacy download en_core_web_sm ---> Using cache ---> cf5c0166e852 Step 6/9 : RUN python -m spacy download en ---> Using cache ---> d39488c451a6 Step 7/9 : ENV PATH=\"/opt/program:${PATH}\" ---> Using cache ---> 1b3759031fe0 Step 8/9 : COPY sagemaker_keras_text_classification /opt/program ---> Using cache ---> 092b378f8446 Step 9/9 : WORKDIR /opt/program ---> Using cache ---> b468053ed126 Successfully built b468053ed126 Successfully tagged sagemaker-keras-text-classification:latest The push refers to repository [485444084140.dkr.ecr.us-west-2.amazonaws.com/sagemaker-keras-text-classification] 6738a352b9b5: Preparing 1084a39dd41b: Preparing 394cd2fefa75: Preparing a8288ebab0e0: Preparing a71b5abf35ed: Preparing 0ba58b74173a: Preparing cc978a7bbd2a: Preparing 3a97a8d562fb: Preparing cb460459ddc8: Preparing b4064660a4cf: Preparing b6e9883adafa: Preparing 9ee6d909e5a7: Preparing e722e212cbab: Preparing 708ade65e147: Preparing 11fc4467b8a3: Preparing 0cf88c3675cd: Preparing d456742927ee: Preparing 8722c9641a57: Preparing 7083756ef61f: Preparing 9d2fda619715: Preparing e79142719515: Preparing aeda103e78c9: Preparing 2558e637fbff: Preparing f749b9b0fb21: Preparing e722e212cbab: Waiting 708ade65e147: Waiting 11fc4467b8a3: Waiting 0cf88c3675cd: Waiting d456742927ee: Waiting 8722c9641a57: Waiting 7083756ef61f: Waiting 9d2fda619715: Waiting e79142719515: Waiting aeda103e78c9: Waiting 2558e637fbff: Waiting f749b9b0fb21: Waiting 0ba58b74173a: Waiting cb460459ddc8: Waiting cc978a7bbd2a: Waiting 3a97a8d562fb: Waiting 9ee6d909e5a7: Waiting b6e9883adafa: Waiting 1084a39dd41b: Pushed 6738a352b9b5: Pushed cc978a7bbd2a: Layer already exists 3a97a8d562fb: Layer already exists cb460459ddc8: Layer already exists 394cd2fefa75: Pushed b4064660a4cf: Layer already exists b6e9883adafa: Layer already exists 9ee6d909e5a7: Layer already exists a71b5abf35ed: Pushed e722e212cbab: Layer already exists 708ade65e147: Layer already exists 0cf88c3675cd: Layer already exists 11fc4467b8a3: Layer already exists d456742927ee: Layer already exists 7083756ef61f: Layer already exists 8722c9641a57: Layer already exists 9d2fda619715: Layer already exists e79142719515: Layer already exists 2558e637fbff: Layer already exists aeda103e78c9: Layer already exists f749b9b0fb21: Layer already exists a8288ebab0e0: Pushed 0ba58b74173a: Pushed latest: digest: sha256:c684d06918d8e4c9ca2db9c35a663d044c89816a01df6fc34ddfd09b449c65ab size: 5344 WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Once you have your container packaged, you can use it to train and serve models. Let's do that with the algorithm we made above.","title":"Building and registering the container"},{"location":"sagemaker_keras_text_classification/#set-up-the-environment","text":"Here we specify a bucket to use and the role that will be used for working with SageMaker. ### S3 prefix prefix = 'sagemaker-keras-text-classification' ### Define IAM role import boto3 import re import os import numpy as np import pandas as pd from sagemaker import get_execution_role role = get_execution_role()","title":"Set up the environment"},{"location":"sagemaker_keras_text_classification/#create-the-session","text":"The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations. import sagemaker as sage from time import gmtime, strftime sess = sage.Session()","title":"Create the session"},{"location":"sagemaker_keras_text_classification/#upload-the-data-for-training","text":"When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. We can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. WORK_DIRECTORY = 'data' data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)","title":"Upload the data for training"},{"location":"sagemaker_keras_text_classification/#create-an-estimator-and-fit-the-model","text":"In order to use SageMaker to fit our algorithm, we'll create an Estimator that defines how to use the container to to train. This includes the configuration we need to invoke SageMaker training: The container name . This is constucted as in the shell commands above. The role . As defined above. The instance count which is the number of machines to use for training. The instance type which is the type of machine to use for training. The output path determines where the model artifact will be written. The session is the SageMaker session object that we defined above. Then we use fit() on the estimator to train against the data that we uploaded above. account = sess.boto_session.client('sts').get_caller_identity()['Account'] region = sess.boto_session.region_name image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-keras-text-classification'.format(account, region) tree = sage.estimator.Estimator(image, role, 1, 'ml.c5.2xlarge', output_path=\"s3://{}/output\".format(sess.default_bucket()), sagemaker_session=sess) To view the progress of training: navigate to Amazon SageMaker Studio > Training > Training Jobs tree.fit(data_location) 2020-04-22 01:18:26 Starting - Starting the training job... 2020-04-22 01:18:27 Starting - Launching requested ML instances...... 2020-04-22 01:19:56 Starting - Preparing the instances for training...... 2020-04-22 01:20:56 Downloading - Downloading input data 2020-04-22 01:20:56 Training - Downloading the training image...... 2020-04-22 01:21:50 Training - Training image download completed. Training in progress.\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m \u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m \u001b[34mStarting the training.\u001b[0m \u001b[34m TITLE ... TIMESTAMP\u001b[0m \u001b[34m1 Fed official says weak data caused by weather,... ... 1394470370698\u001b[0m \u001b[34m2 Fed's Charles Plosser sees high bar for change... ... 1394470371207\u001b[0m \u001b[34m3 US open: Stocks fall after Fed official hints ... ... 1394470371550\u001b[0m \u001b[34m4 Fed risks falling 'behind the curve', Charles ... ... 1394470371793\u001b[0m \u001b[34m5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... ... 1394470372027 \u001b[0m \u001b[34m[5 rows x 7 columns]\u001b[0m \u001b[34mFound 65990 unique tokens.\u001b[0m \u001b[34mShape of data tensor: (422417, 100)\u001b[0m \u001b[34mShape of label tensor: (422417, 4)\u001b[0m \u001b[34mx_train shape: (337933, 100)\u001b[0m \u001b[34mWARNING: Logging before flag parsing goes to stderr.\u001b[0m \u001b[34mW0422 01:22:22.159757 140619530934016 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m \u001b[34mInstructions for updating:\u001b[0m \u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m \u001b[34mW0422 01:22:22.187488 140619530934016 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m \u001b[34mInstructions for updating:\u001b[0m \u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m \u001b[34mModel: \"sequential\"\u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34mLayer (type) Output Shape Param # \u001b[0m \u001b[34m=================================================================\u001b[0m \u001b[34membedding (Embedding) (None, 100, 100) 1000000 \u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34mflatten (Flatten) (None, 10000) 0 \u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34mdense (Dense) (None, 2) 20002 \u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34mdense_1 (Dense) (None, 4) 12 \u001b[0m \u001b[34m=================================================================\u001b[0m \u001b[34mTotal params: 1,020,014\u001b[0m \u001b[34mTrainable params: 1,020,014\u001b[0m \u001b[34mNon-trainable params: 0\u001b[0m \u001b[34m_________________________________________________________________\u001b[0m \u001b[34m2020-04-22 01:22:22.217855: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX512F\u001b[0m \u001b[34mTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m \u001b[34m2020-04-22 01:22:22.259738: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\u001b[0m \u001b[34m2020-04-22 01:22:22.260199: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23b2eb0 executing computations on platform Host. Devices:\u001b[0m \u001b[34m2020-04-22 01:22:22.260224: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): <undefined>, <undefined>\u001b[0m \u001b[34m2020-04-22 01:22:22.261048: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\u001b[0m \u001b[34m2020-04-22 01:22:22.291480: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set. If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU. To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m \u001b[34mTrain on 337933 samples, validate on 84484 samples\u001b[0m \u001b[34mEpoch 1/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.6303 - acc: 0.7653 - val_loss: 0.5580 - val_acc: 0.7959\u001b[0m \u001b[34mEpoch 2/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.5502 - acc: 0.7977 - val_loss: 0.5558 - val_acc: 0.7955\u001b[0m \u001b[34mEpoch 3/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.5427 - acc: 0.8000 - val_loss: 0.5450 - val_acc: 0.8004\u001b[0m \u001b[34mEpoch 4/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.5391 - acc: 0.8010 - val_loss: 0.5407 - val_acc: 0.8018\u001b[0m \u001b[34mEpoch 5/6\u001b[0m \u001b[34m337933/337933 - 29s - loss: 0.5363 - acc: 0.8022 - val_loss: 0.5403 - val_acc: 0.8035\u001b[0m \u001b[34mEpoch 6/6\u001b[0m 2020-04-22 01:25:23 Uploading - Uploading generated training model 2020-04-22 01:25:23 Completed - Training job completed \u001b[34m337933/337933 - 29s - loss: 0.5342 - acc: 0.8032 - val_loss: 0.5424 - val_acc: 0.8014\u001b[0m \u001b[34mTraining complete. Now saving model to: /opt/ml/model\u001b[0m \u001b[34mW0422 01:25:15.442514 140619530934016 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m \u001b[34mInstructions for updating:\u001b[0m \u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m \u001b[34mW0422 01:25:15.442924 140619530934016 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\u001b[0m \u001b[34mInstructions for updating:\u001b[0m \u001b[34mCall initializer instance with the dtype argument instead of passing it to the constructor\u001b[0m \u001b[34mTest headline: What Improved Tech Means for Electric, Self-Driving and Flying Cars\u001b[0m \u001b[34mPredicted category: b\u001b[0m Training seconds: 283 Billable seconds: 283","title":"Create an estimator and fit the model"},{"location":"sagemaker_keras_text_classification/#deploy-the-model","text":"Deploying the model to SageMaker hosting just requires a deploy call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint. This step may take about 10-20 min from sagemaker.predictor import json_serializer predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=json_serializer) -------------!","title":"Deploy the model"},{"location":"sagemaker_keras_text_classification/#prediction_1","text":"request = { \"input\": \"\u2018Deadpool 2\u2019 Has More Swearing, Slicing and Dicing from Ryan Reynolds\"} print(predictor.predict(request).decode('utf-8')) {\"result\": \"Entertainment\"}","title":"Prediction"},{"location":"sagemaker_keras_text_classification/#endpoint-test","text":"To view the endpoint_name you just deployed: navigate to Amazon SageMaker Studio > Inference > Endpoints","title":"Endpoint Test"},{"location":"sagemaker_keras_text_classification/#invokeendpoint-using-boto3","text":"import json import boto3 client = boto3.client('runtime.sagemaker') endpoint_name = 'sagemaker-keras-text-classification-2020-04-22-01-18-26-512' payload = { \"input\": \"\u2018Deadpool 2\u2019 Has More Swearing, Slicing and Dicing from Ryan Reynolds\"} response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Accept='text/plain', Body=json.dumps(payload)) response_body = response['Body'] prediction = response_body.read().decode('utf-8') print(prediction) {\"result\": \"Entertainment\"}","title":"InvokeEndpoint using boto3"},{"location":"sagemaker_keras_text_classification/#invokeendpoint-using-aws-cli","text":"%%sh ENDPOINT_NAME=\"sagemaker-keras-text-classification-2020-04-22-01-18-26-512\" CONTENT_TYPE='application/json' aws sagemaker-runtime invoke-endpoint \\ --endpoint-name ${ENDPOINT_NAME} \\ --content-type ${CONTENT_TYPE} \\ --body '{\"input\": \"Why Exercise Alone May Not Be the Key to Weight Loss\"}' prediction_response.json cat prediction_response.json { \"ContentType\": \"application/json\", \"InvokedProductionVariant\": \"AllTraffic\" } {\"result\": \"Health & Medicine\"}","title":"InvokeEndpoint using AWS CLI"},{"location":"sagemaker_keras_text_classification/#optional-cleanup","text":"When you're done with the endpoint, you'll want to clean it up. sess.delete_endpoint(predictor.endpoint)","title":"Optional cleanup"},{"location":"sagemaker_keras_text_classification/#references","text":"https://github.com/aws-samples/amazon-sagemaker-keras-text-classification https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks.html","title":"References"},{"location":"singularity_docker/","text":"Text Classification Using AWS Deep Learning Docker Containers Overview In this tutorial, we will build a Singularity container using one of available AWS Deep-Learning docker images , and run the Singularity container on Lawrencium CPU and GPU nodes, to train an NLP model (based on Keras & TensorFlow). The NLP model will classify news articles into the appropriate news category given the training data from UCI News Dataset which contains a list of about 420K articles and their appropriate categories (labels). There are four categories: Business (b) Science & Technology (t) Entertainment (e) Health & Medicine (m) Prerequisites: AWS CLI version 1 Docker Singularity Objectives 1) Build the Singularity container using available AWS Deep-Learning docker containers 2) Local Test 3) Train text classifier on Lawrencium: Upload the Singularity containers and training data Run the Singularity container on CPU nodes Run the Singularity container on GPU nodes CPU vs GPU Build the Singularity container using available AWS Deep-Learning docker containers AWS Deep-Learning images The following shell code shows how to build the container image using docker and convert the container image to a Singularity image. Download the GitHub repository for this tutorial %%sh git clone https://github.com/lbnl-science-it/singularity_aws_dl_container.git cd singularity_aws_dl_container Download and unzip the dataset %%sh cd container #################################################### ########## Download and unzip the dataset ########## #################################################### cd ../data/ wget https://danilop.s3-eu-west-1.amazonaws.com/reInvent-Workshop-Data-Backup.zip && unzip reInvent-Workshop-Data-Backup.zip mv reInvent-Workshop-Data-Backup/* ./ rm -rf reInvent-Workshop-Data-Backup reInvent-Workshop-Data-Backup.zip cd ../container/ Build the SageMaker Container & Convert it to Singularity image %%sh cd container ################################################################################### ######### Build the SageMaker Container & Convert it to Singularity image ######### ################################################################################### algorithm_name=sagemaker-keras-text-classification chmod +x sagemaker_keras_text_classification/train chmod +x sagemaker_keras_text_classification/serve ## Get the region defined in the current configuration region=$(aws configure get region) fullname=\"local_${algorithm_name}:latest\" ## Get the login command from ECR and execute it directly $(aws ecr get-login --no-include-email --region ${region} --registry-ids 763104351884) ## Build the docker image locally with the image name ## In the \"Dockerfile\", modify the source image to select one of the available deep learning docker containers images: ## https://aws.amazon.com/releasenotes/available-deep-learning-containers-images docker build -t ${algorithm_name} . docker tag ${algorithm_name} ${fullname} ## Build Singularity image from local docker image sifname=\"local_sagemaker-keras-text-classification.sif\" sudo singularity build ${sifname} docker-daemon:${fullname} Login Succeeded Sending build context to Docker daemon 456.3MB Step 1/9 : FROM 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:1.14.0-cpu-py36-ubuntu16.04 ---> e6a210ff54e4 Step 2/9 : RUN apt-get update && apt-get install -y nginx imagemagick graphviz ---> Using cache ---> 32ff2dce1af3 Step 3/9 : RUN pip install --upgrade pip ---> Using cache ---> 4e1b65ea3a65 Step 4/9 : RUN pip install gevent gunicorn flask tensorflow_hub seqeval graphviz nltk spacy tqdm ---> Using cache ---> d97c22f6de86 Step 5/9 : RUN python -m spacy download en_core_web_sm ---> Using cache ---> 14c8854a1901 Step 6/9 : RUN python -m spacy download en ---> Using cache ---> 185661d9e15d Step 7/9 : ENV PATH=\"/opt/program:${PATH}\" ---> Using cache ---> b5d5c6867074 Step 8/9 : COPY sagemaker_keras_text_classification /opt/program ---> Using cache ---> ac73b50bd646 Step 9/9 : WORKDIR /opt/program ---> Using cache ---> c5fe52a83024 Successfully built c5fe52a83024 Successfully tagged sagemaker-keras-text-classification:latest \u001b[34mINFO: \u001b[0m Starting build... Getting image source signatures Copying blob sha256:87e513ddb4a6ce37dabf3de74b0284d49e08f4d7a3f0de393e6a533577e00f11 ... Copying config sha256:77b2a54a3da8891391f609455182127c0944edb40397fbaf24f9ec80a9be5460 Writing manifest to image destination Storing signatures 2020/06/08 22:14:47 info unpack layer: sha256:647dce8a9de5ada5719e82c2ff5408867fcaa83145665bea4103d3705c2326b1 ... 2020/06/08 22:14:49 info unpack layer: sha256:1df727cf7f1435f496890edded1650193af403065eff27929a8b374d5b36d743 2020/06/08 22:14:49 info unpack layer: sha256:df2ccfca12a78a5c880fd30514c57c84f250a81c223915e124cff93833f6b5d2 2020/06/08 22:14:49 info unpack layer: sha256:88f2c64e66817e60a415e82323d1a2d3f19ca75eb4ea9ae7692a2fccc09c2de5 \u001b[34mINFO: \u001b[0m Creating SIF file... \u001b[34mINFO: \u001b[0m Build complete: local_sagemaker-keras-text-classification.sif Train Text Classifier %%sh cd container ################################ ########## Local Test ########## ################################ cd ../data cp -a . ../container/local_test/test_dir/input/data/training/ cd ../container cd local_test ### Train sifname=\"local_sagemaker-keras-text-classification.sif\" ./train_local.sh ../${sifname} Starting the training. TITLE ... TIMESTAMP 1 Fed official says weak data caused by weather,... ... 1394470370698 2 Fed's Charles Plosser sees high bar for change... ... 1394470371207 3 US open: Stocks fall after Fed official hints ... ... 1394470371550 4 Fed risks falling 'behind the curve', Charles ... ... 1394470371793 5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... ... 1394470372027 [5 rows x 7 columns] Found 65990 unique tokens. Shape of data tensor: (422417, 100) Shape of label tensor: (422417, 4) x_train shape: (337933, 100) Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 100) 1000000 _________________________________________________________________ flatten (Flatten) (None, 10000) 0 _________________________________________________________________ dense (Dense) (None, 2) 20002 _________________________________________________________________ dense_1 (Dense) (None, 4) 12 ================================================================= Total params: 1,020,014 Trainable params: 1,020,014 Non-trainable params: 0 _________________________________________________________________ Train on 337933 samples, validate on 84484 samples Epoch 1/6 337933/337933 - 25s - loss: 0.6788 - acc: 0.7409 - val_loss: 0.6146 - val_acc: 0.7757 Epoch 2/6 337933/337933 - 25s - loss: 0.5958 - acc: 0.7824 - val_loss: 0.5889 - val_acc: 0.7840 Epoch 3/6 337933/337933 - 25s - loss: 0.5778 - acc: 0.7882 - val_loss: 0.5755 - val_acc: 0.7893 Epoch 4/6 337933/337933 - 25s - loss: 0.5707 - acc: 0.7904 - val_loss: 0.5697 - val_acc: 0.7918 Epoch 5/6 337933/337933 - 25s - loss: 0.5673 - acc: 0.7918 - val_loss: 0.5684 - val_acc: 0.7915 Epoch 6/6 337933/337933 - 25s - loss: 0.5648 - acc: 0.7920 - val_loss: 0.5657 - val_acc: 0.7923 Training complete. Now saving model to: /opt/ml/model Test headline: What Improved Tech Means for Electric, Self-Driving and Flying Cars Predicted category: t Data Exploration import pandas as pd import tensorflow as tf import re import numpy as np import os from tensorflow.python.keras.preprocessing.text import Tokenizer from tensorflow.python.keras.preprocessing.sequence import pad_sequences from tensorflow.python.keras.utils import to_categorical column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"] news_dataset = pd.read_csv(os.path.join('./data', 'newsCorpora.csv'), names=column_names, header=None, delimiter='\\t') news_dataset.head() TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP Fed official says weak data caused by weather,... http://www.latimes.com/business/money/la-fi-mo... Los Angeles Times b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.latimes.com 1394470370698 news_dataset.groupby(['CATEGORY']).size() CATEGORY b 115967 e 152469 m 45639 t 108344 dtype: int64 Local Test Build Singularity container using AWS Deep-Learning Docker CPU image cd container sh build_singularity_local_test.sh Build Singularity container using AWS Deep-Learning Docker GPU image sh build_singularity_local_test_gpu.sh Train text classifier on Lawrencium Upload the Singularity containers and training data sftp lrc-xfer.lbl.gov put local_sagemaker-keras-text-classification.sif put local_sagemaker-keras-text-classification_gpu.sif put -r local_test Run the Singularity container on Lawrencium CPU node ssh lrc-login.lbl.gov cd local_test srun -N 1 -p lr4 -A $ACCOUNT -t 1:0:0 -q lr_normal --pty bash sh train_local.sh ../local_sagemaker-keras-text-classification.sif Run the Singularity container on Lawrencium GPU node ssh lrc-login.lbl.gov cd local_test srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash sh train_local_gpu.sh ../local_sagemaker-keras-text-classification_gpu.sif Starting the training. TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP 1 Fed official says weak data caused by weather,... http://www.latimes.com/business/money/la-fi-mo... Los Angeles Times b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.latimes.com 1394470370698 2 Fed's Charles Plosser sees high bar for change... http://www.livemint.com/Politics/H2EvwJSK2VE6O... Livemint b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.livemint.com 1394470371207 3 US open: Stocks fall after Fed official hints ... http://www.ifamagazine.com/news/us-open-stocks... IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371550 4 Fed risks falling 'behind the curve', Charles ... http://www.ifamagazine.com/news/fed-risks-fall... IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371793 5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... http://www.moneynews.com/Economy/federal-reser... Moneynews b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.moneynews.com 1394470372027 Found 65990 unique tokens. Shape of data tensor: (422417, 100) Shape of label tensor: (422417, 4) x_train shape: (337933, 100) Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 100) 1000000 _________________________________________________________________ flatten (Flatten) (None, 10000) 0 _________________________________________________________________ dense (Dense) (None, 2) 20002 _________________________________________________________________ dense_1 (Dense) (None, 4) 12 ================================================================= Total params: 1,020,014 Trainable params: 1,020,014 Non-trainable params: 0 _________________________________________________________________ 2020-06-23 23:15:14.245462: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1 2020-06-23 23:15:16.127338: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3672c00 executing computations on platform CUDA. Devices: 2020-06-23 23:15:16.127380: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1 2020-06-23 23:15:16.127388: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1 2020-06-23 23:15:16.127394: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (2): GeForce GTX 1080 Ti, Compute Capability 6.1 2020-06-23 23:15:16.127399: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (3): GeForce GTX 1080 Ti, Compute Capability 6.1 2020-06-23 23:15:16.154635: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999685000 Hz 2020-06-23 23:15:16.154819: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3685470 executing computations on platform Host. Devices: 2020-06-23 23:15:16.154871: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): <undefined>, <undefined> 2020-06-23 23:15:16.157342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:02:00.0 2020-06-23 23:15:16.158085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:03:00.0 2020-06-23 23:15:16.158826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:81:00.0 2020-06-23 23:15:16.159564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:82:00.0 2020-06-23 23:15:16.164279: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2020-06-23 23:15:16.245853: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2020-06-23 23:15:16.286560: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0 2020-06-23 23:15:16.311842: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0 2020-06-23 23:15:16.418419: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0 2020-06-23 23:15:16.483660: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0 2020-06-23 23:15:16.684841: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2020-06-23 23:15:16.694707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3 2020-06-23 23:15:16.697050: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2020-06-23 23:15:16.702301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-06-23 23:15:16.702355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187] 0 1 2 3 2020-06-23 23:15:16.702409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0: N Y N N 2020-06-23 23:15:16.702424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1: Y N N N 2020-06-23 23:15:16.702445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2: N N N Y 2020-06-23 23:15:16.702465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3: N N Y N 2020-06-23 23:15:16.711677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1) 2020-06-23 23:15:16.713584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10481 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1) 2020-06-23 23:15:16.715242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10481 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1) 2020-06-23 23:15:16.716880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10481 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1) Train on 337933 samples, validate on 84484 samples Epoch 1/6 2020-06-23 23:15:21.391855: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 337933/337933 - 18s - loss: 0.6149 - acc: 0.7661 - val_loss: 0.5618 - val_acc: 0.7933 Epoch 2/6 337933/337933 - 16s - loss: 0.5476 - acc: 0.7972 - val_loss: 0.5513 - val_acc: 0.7981 Epoch 3/6 337933/337933 - 16s - loss: 0.5391 - acc: 0.8008 - val_loss: 0.5402 - val_acc: 0.8021 Epoch 4/6 337933/337933 - 17s - loss: 0.5355 - acc: 0.8016 - val_loss: 0.5385 - val_acc: 0.8023 Epoch 5/6 337933/337933 - 17s - loss: 0.5333 - acc: 0.8029 - val_loss: 0.5374 - val_acc: 0.8013 Epoch 6/6 337933/337933 - 16s - loss: 0.5315 - acc: 0.8032 - val_loss: 0.5361 - val_acc: 0.8023 Training complete. Now saving model to: /opt/ml/model Test headline: What Improved Tech Means for Electric, Self-Driving and Flying Cars Predicted category: t CPU vs GPU In this tutorial we trained the same NLP model on Lawrencium nodes with and without GPU; the training took about 17s per epoch (337933 samples) on GPU node, and 23s per epoch on CPU node. References https://aws.amazon.com/releasenotes/available-deep-learning-containers-images https://github.com/aws-samples/amazon-sagemaker-keras-text-classification https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification https://sylabs.io/guides/3.5/user-guide/ https://www.digitalocean.com/community/questions/how-to-fix-docker-got-permission-denied-while-trying-to-connect-to-the-docker-daemon-socket https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html","title":"Lawrencium: Singularity container using AWS DL docker images"},{"location":"singularity_docker/#text-classification-using-aws-deep-learning-docker-containers","text":"","title":"Text Classification Using AWS Deep Learning Docker Containers"},{"location":"singularity_docker/#overview","text":"In this tutorial, we will build a Singularity container using one of available AWS Deep-Learning docker images , and run the Singularity container on Lawrencium CPU and GPU nodes, to train an NLP model (based on Keras & TensorFlow). The NLP model will classify news articles into the appropriate news category given the training data from UCI News Dataset which contains a list of about 420K articles and their appropriate categories (labels). There are four categories: Business (b) Science & Technology (t) Entertainment (e) Health & Medicine (m)","title":"Overview"},{"location":"singularity_docker/#prerequisites","text":"","title":"Prerequisites:"},{"location":"singularity_docker/#aws-cli-version-1","text":"","title":"AWS CLI version 1"},{"location":"singularity_docker/#docker","text":"","title":"Docker"},{"location":"singularity_docker/#singularity","text":"","title":"Singularity"},{"location":"singularity_docker/#objectives","text":"1) Build the Singularity container using available AWS Deep-Learning docker containers 2) Local Test 3) Train text classifier on Lawrencium: Upload the Singularity containers and training data Run the Singularity container on CPU nodes Run the Singularity container on GPU nodes CPU vs GPU","title":"Objectives"},{"location":"singularity_docker/#build-the-singularity-container-using-available-aws-deep-learning-docker-containers","text":"AWS Deep-Learning images The following shell code shows how to build the container image using docker and convert the container image to a Singularity image.","title":"Build the Singularity container using available AWS Deep-Learning docker containers"},{"location":"singularity_docker/#download-the-github-repository-for-this-tutorial","text":"%%sh git clone https://github.com/lbnl-science-it/singularity_aws_dl_container.git cd singularity_aws_dl_container","title":"Download the GitHub repository for this tutorial"},{"location":"singularity_docker/#download-and-unzip-the-dataset","text":"%%sh cd container #################################################### ########## Download and unzip the dataset ########## #################################################### cd ../data/ wget https://danilop.s3-eu-west-1.amazonaws.com/reInvent-Workshop-Data-Backup.zip && unzip reInvent-Workshop-Data-Backup.zip mv reInvent-Workshop-Data-Backup/* ./ rm -rf reInvent-Workshop-Data-Backup reInvent-Workshop-Data-Backup.zip cd ../container/","title":"Download and unzip the dataset"},{"location":"singularity_docker/#build-the-sagemaker-container-convert-it-to-singularity-image","text":"%%sh cd container ################################################################################### ######### Build the SageMaker Container & Convert it to Singularity image ######### ################################################################################### algorithm_name=sagemaker-keras-text-classification chmod +x sagemaker_keras_text_classification/train chmod +x sagemaker_keras_text_classification/serve ## Get the region defined in the current configuration region=$(aws configure get region) fullname=\"local_${algorithm_name}:latest\" ## Get the login command from ECR and execute it directly $(aws ecr get-login --no-include-email --region ${region} --registry-ids 763104351884) ## Build the docker image locally with the image name ## In the \"Dockerfile\", modify the source image to select one of the available deep learning docker containers images: ## https://aws.amazon.com/releasenotes/available-deep-learning-containers-images docker build -t ${algorithm_name} . docker tag ${algorithm_name} ${fullname} ## Build Singularity image from local docker image sifname=\"local_sagemaker-keras-text-classification.sif\" sudo singularity build ${sifname} docker-daemon:${fullname} Login Succeeded Sending build context to Docker daemon 456.3MB Step 1/9 : FROM 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:1.14.0-cpu-py36-ubuntu16.04 ---> e6a210ff54e4 Step 2/9 : RUN apt-get update && apt-get install -y nginx imagemagick graphviz ---> Using cache ---> 32ff2dce1af3 Step 3/9 : RUN pip install --upgrade pip ---> Using cache ---> 4e1b65ea3a65 Step 4/9 : RUN pip install gevent gunicorn flask tensorflow_hub seqeval graphviz nltk spacy tqdm ---> Using cache ---> d97c22f6de86 Step 5/9 : RUN python -m spacy download en_core_web_sm ---> Using cache ---> 14c8854a1901 Step 6/9 : RUN python -m spacy download en ---> Using cache ---> 185661d9e15d Step 7/9 : ENV PATH=\"/opt/program:${PATH}\" ---> Using cache ---> b5d5c6867074 Step 8/9 : COPY sagemaker_keras_text_classification /opt/program ---> Using cache ---> ac73b50bd646 Step 9/9 : WORKDIR /opt/program ---> Using cache ---> c5fe52a83024 Successfully built c5fe52a83024 Successfully tagged sagemaker-keras-text-classification:latest \u001b[34mINFO: \u001b[0m Starting build... Getting image source signatures Copying blob sha256:87e513ddb4a6ce37dabf3de74b0284d49e08f4d7a3f0de393e6a533577e00f11 ... Copying config sha256:77b2a54a3da8891391f609455182127c0944edb40397fbaf24f9ec80a9be5460 Writing manifest to image destination Storing signatures 2020/06/08 22:14:47 info unpack layer: sha256:647dce8a9de5ada5719e82c2ff5408867fcaa83145665bea4103d3705c2326b1 ... 2020/06/08 22:14:49 info unpack layer: sha256:1df727cf7f1435f496890edded1650193af403065eff27929a8b374d5b36d743 2020/06/08 22:14:49 info unpack layer: sha256:df2ccfca12a78a5c880fd30514c57c84f250a81c223915e124cff93833f6b5d2 2020/06/08 22:14:49 info unpack layer: sha256:88f2c64e66817e60a415e82323d1a2d3f19ca75eb4ea9ae7692a2fccc09c2de5 \u001b[34mINFO: \u001b[0m Creating SIF file... \u001b[34mINFO: \u001b[0m Build complete: local_sagemaker-keras-text-classification.sif","title":"Build the SageMaker Container &amp; Convert it to Singularity image"},{"location":"singularity_docker/#train-text-classifier","text":"%%sh cd container ################################ ########## Local Test ########## ################################ cd ../data cp -a . ../container/local_test/test_dir/input/data/training/ cd ../container cd local_test ### Train sifname=\"local_sagemaker-keras-text-classification.sif\" ./train_local.sh ../${sifname} Starting the training. TITLE ... TIMESTAMP 1 Fed official says weak data caused by weather,... ... 1394470370698 2 Fed's Charles Plosser sees high bar for change... ... 1394470371207 3 US open: Stocks fall after Fed official hints ... ... 1394470371550 4 Fed risks falling 'behind the curve', Charles ... ... 1394470371793 5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... ... 1394470372027 [5 rows x 7 columns] Found 65990 unique tokens. Shape of data tensor: (422417, 100) Shape of label tensor: (422417, 4) x_train shape: (337933, 100) Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 100) 1000000 _________________________________________________________________ flatten (Flatten) (None, 10000) 0 _________________________________________________________________ dense (Dense) (None, 2) 20002 _________________________________________________________________ dense_1 (Dense) (None, 4) 12 ================================================================= Total params: 1,020,014 Trainable params: 1,020,014 Non-trainable params: 0 _________________________________________________________________ Train on 337933 samples, validate on 84484 samples Epoch 1/6 337933/337933 - 25s - loss: 0.6788 - acc: 0.7409 - val_loss: 0.6146 - val_acc: 0.7757 Epoch 2/6 337933/337933 - 25s - loss: 0.5958 - acc: 0.7824 - val_loss: 0.5889 - val_acc: 0.7840 Epoch 3/6 337933/337933 - 25s - loss: 0.5778 - acc: 0.7882 - val_loss: 0.5755 - val_acc: 0.7893 Epoch 4/6 337933/337933 - 25s - loss: 0.5707 - acc: 0.7904 - val_loss: 0.5697 - val_acc: 0.7918 Epoch 5/6 337933/337933 - 25s - loss: 0.5673 - acc: 0.7918 - val_loss: 0.5684 - val_acc: 0.7915 Epoch 6/6 337933/337933 - 25s - loss: 0.5648 - acc: 0.7920 - val_loss: 0.5657 - val_acc: 0.7923 Training complete. Now saving model to: /opt/ml/model Test headline: What Improved Tech Means for Electric, Self-Driving and Flying Cars Predicted category: t","title":"Train Text Classifier"},{"location":"singularity_docker/#data-exploration","text":"import pandas as pd import tensorflow as tf import re import numpy as np import os from tensorflow.python.keras.preprocessing.text import Tokenizer from tensorflow.python.keras.preprocessing.sequence import pad_sequences from tensorflow.python.keras.utils import to_categorical column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"] news_dataset = pd.read_csv(os.path.join('./data', 'newsCorpora.csv'), names=column_names, header=None, delimiter='\\t') news_dataset.head() TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP Fed official says weak data caused by weather,... http://www.latimes.com/business/money/la-fi-mo... Los Angeles Times b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.latimes.com 1394470370698 news_dataset.groupby(['CATEGORY']).size() CATEGORY b 115967 e 152469 m 45639 t 108344 dtype: int64","title":"Data Exploration"},{"location":"singularity_docker/#local-test","text":"Build Singularity container using AWS Deep-Learning Docker CPU image cd container sh build_singularity_local_test.sh Build Singularity container using AWS Deep-Learning Docker GPU image sh build_singularity_local_test_gpu.sh","title":"Local Test"},{"location":"singularity_docker/#train-text-classifier-on-lawrencium","text":"","title":"Train text classifier on Lawrencium"},{"location":"singularity_docker/#upload-the-singularity-containers-and-training-data","text":"sftp lrc-xfer.lbl.gov put local_sagemaker-keras-text-classification.sif put local_sagemaker-keras-text-classification_gpu.sif put -r local_test","title":"Upload the Singularity containers and training data"},{"location":"singularity_docker/#run-the-singularity-container-on-lawrencium-cpu-node","text":"ssh lrc-login.lbl.gov cd local_test srun -N 1 -p lr4 -A $ACCOUNT -t 1:0:0 -q lr_normal --pty bash sh train_local.sh ../local_sagemaker-keras-text-classification.sif","title":"Run the Singularity container on Lawrencium CPU node"},{"location":"singularity_docker/#run-the-singularity-container-on-lawrencium-gpu-node","text":"ssh lrc-login.lbl.gov cd local_test srun -N 1 -p es1 -A $ACCOUNT -t 1:0:0 --gres=gpu:2 -n 4 -q es_normal --pty bash sh train_local_gpu.sh ../local_sagemaker-keras-text-classification_gpu.sif Starting the training. TITLE URL PUBLISHER CATEGORY STORY HOSTNAME TIMESTAMP 1 Fed official says weak data caused by weather,... http://www.latimes.com/business/money/la-fi-mo... Los Angeles Times b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.latimes.com 1394470370698 2 Fed's Charles Plosser sees high bar for change... http://www.livemint.com/Politics/H2EvwJSK2VE6O... Livemint b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.livemint.com 1394470371207 3 US open: Stocks fall after Fed official hints ... http://www.ifamagazine.com/news/us-open-stocks... IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371550 4 Fed risks falling 'behind the curve', Charles ... http://www.ifamagazine.com/news/fed-risks-fall... IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371793 5 Fed's Plosser: Nasty Weather Has Curbed Job Gr... http://www.moneynews.com/Economy/federal-reser... Moneynews b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.moneynews.com 1394470372027 Found 65990 unique tokens. Shape of data tensor: (422417, 100) Shape of label tensor: (422417, 4) x_train shape: (337933, 100) Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 100) 1000000 _________________________________________________________________ flatten (Flatten) (None, 10000) 0 _________________________________________________________________ dense (Dense) (None, 2) 20002 _________________________________________________________________ dense_1 (Dense) (None, 4) 12 ================================================================= Total params: 1,020,014 Trainable params: 1,020,014 Non-trainable params: 0 _________________________________________________________________ 2020-06-23 23:15:14.245462: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1 2020-06-23 23:15:16.127338: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3672c00 executing computations on platform CUDA. Devices: 2020-06-23 23:15:16.127380: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1 2020-06-23 23:15:16.127388: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1 2020-06-23 23:15:16.127394: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (2): GeForce GTX 1080 Ti, Compute Capability 6.1 2020-06-23 23:15:16.127399: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (3): GeForce GTX 1080 Ti, Compute Capability 6.1 2020-06-23 23:15:16.154635: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999685000 Hz 2020-06-23 23:15:16.154819: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3685470 executing computations on platform Host. Devices: 2020-06-23 23:15:16.154871: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): <undefined>, <undefined> 2020-06-23 23:15:16.157342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:02:00.0 2020-06-23 23:15:16.158085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:03:00.0 2020-06-23 23:15:16.158826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:81:00.0 2020-06-23 23:15:16.159564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582 pciBusID: 0000:82:00.0 2020-06-23 23:15:16.164279: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2020-06-23 23:15:16.245853: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 2020-06-23 23:15:16.286560: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0 2020-06-23 23:15:16.311842: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0 2020-06-23 23:15:16.418419: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0 2020-06-23 23:15:16.483660: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0 2020-06-23 23:15:16.684841: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7 2020-06-23 23:15:16.694707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3 2020-06-23 23:15:16.697050: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0 2020-06-23 23:15:16.702301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-06-23 23:15:16.702355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187] 0 1 2 3 2020-06-23 23:15:16.702409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0: N Y N N 2020-06-23 23:15:16.702424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1: Y N N N 2020-06-23 23:15:16.702445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2: N N N Y 2020-06-23 23:15:16.702465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3: N N Y N 2020-06-23 23:15:16.711677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1) 2020-06-23 23:15:16.713584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10481 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1) 2020-06-23 23:15:16.715242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10481 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1) 2020-06-23 23:15:16.716880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10481 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1) Train on 337933 samples, validate on 84484 samples Epoch 1/6 2020-06-23 23:15:21.391855: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0 337933/337933 - 18s - loss: 0.6149 - acc: 0.7661 - val_loss: 0.5618 - val_acc: 0.7933 Epoch 2/6 337933/337933 - 16s - loss: 0.5476 - acc: 0.7972 - val_loss: 0.5513 - val_acc: 0.7981 Epoch 3/6 337933/337933 - 16s - loss: 0.5391 - acc: 0.8008 - val_loss: 0.5402 - val_acc: 0.8021 Epoch 4/6 337933/337933 - 17s - loss: 0.5355 - acc: 0.8016 - val_loss: 0.5385 - val_acc: 0.8023 Epoch 5/6 337933/337933 - 17s - loss: 0.5333 - acc: 0.8029 - val_loss: 0.5374 - val_acc: 0.8013 Epoch 6/6 337933/337933 - 16s - loss: 0.5315 - acc: 0.8032 - val_loss: 0.5361 - val_acc: 0.8023 Training complete. Now saving model to: /opt/ml/model Test headline: What Improved Tech Means for Electric, Self-Driving and Flying Cars Predicted category: t","title":"Run the Singularity container on Lawrencium GPU node"},{"location":"singularity_docker/#cpu-vs-gpu","text":"In this tutorial we trained the same NLP model on Lawrencium nodes with and without GPU; the training took about 17s per epoch (337933 samples) on GPU node, and 23s per epoch on CPU node.","title":"CPU vs GPU"},{"location":"singularity_docker/#references","text":"https://aws.amazon.com/releasenotes/available-deep-learning-containers-images https://github.com/aws-samples/amazon-sagemaker-keras-text-classification https://github.com/lbnl-science-it/aws-sagemaker-keras-text-classification https://sylabs.io/guides/3.5/user-guide/ https://www.digitalocean.com/community/questions/how-to-fix-docker-got-permission-denied-while-trying-to-connect-to-the-docker-daemon-socket https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html","title":"References"},{"location":"sparc_on_gcp/","text":"Running SpaRC on Google Cloud Dataproc Building SpaRC: Open a new Cloud Shell and run: git clone https://github.com/shawfdong/sparc.git cd sparc wget https://piccolo.link/sbt-1.3.10.tgz tar xvzf sbt-1.3.10.tgz ./sbt/bin/sbt assembly A jar file should be created at: target/scala-2.11/LocalCluster-assembly-0.2.jar Upload Data to Google Cloud Storage Navigate to Storage and select Storage > Browser . Click Create Bucket . Specify your project name as the bucket name . Click Create . Copy the compiled SpaRC LocalCluster-assembly-0.2.jar and a sample input file sample_small.seq to the project bucket you just created, by running the below in Cloud Shell: gsutil cp target/scala-2.11/LocalCluster-assembly-0.2.jar gs://$DEVSHELL_PROJECT_ID cd data/small cp sample.seq sample_small.seq gsutil cp sample_small.seq gs://$DEVSHELL_PROJECT_ID Launch Dataproc Run SpaRC job on Dataproc In the Dataproc console, click Jobs . Click Submit job . For Job type , select Spark ; for Main class or jar and Jar files , specify the location of the SpaRC jar file you uploaded to your bucket. Your bucket-name is your project name : gs://<my-project-name>/LocalCluster-assembly-0.2.jar . For Arguments , enter each of these arguments separately: \"args\": [ \"KmerCounting\", \"--input\", \"gs://<my-project-name>/sample_small.seq\", \"--output\", \"test.log\", \"--kmer_length\", \"31\" ] For Properties , enter these Key-Value pairs separately: \"properties\": { \"spark.executor.extraClassPath\": \"gs://<my-project-name>/LocalCluster-assembly-0.2.jar\", \"spark.driver.maxResultSize\": \"8g\", \"spark.network.timeout\": \"360000\", \"spark.default.parallelism\": \"4\", \"spark.eventLog.enabled\": \"false\" } Click Submit","title":"SpaRC on GCP"},{"location":"sparc_on_gcp/#running-sparc-on-google-cloud-dataproc","text":"","title":"Running SpaRC on Google Cloud Dataproc"},{"location":"sparc_on_gcp/#building-sparc","text":"Open a new Cloud Shell and run: git clone https://github.com/shawfdong/sparc.git cd sparc wget https://piccolo.link/sbt-1.3.10.tgz tar xvzf sbt-1.3.10.tgz ./sbt/bin/sbt assembly A jar file should be created at: target/scala-2.11/LocalCluster-assembly-0.2.jar","title":"Building SpaRC:"},{"location":"sparc_on_gcp/#upload-data-to-google-cloud-storage","text":"Navigate to Storage and select Storage > Browser . Click Create Bucket . Specify your project name as the bucket name . Click Create . Copy the compiled SpaRC LocalCluster-assembly-0.2.jar and a sample input file sample_small.seq to the project bucket you just created, by running the below in Cloud Shell: gsutil cp target/scala-2.11/LocalCluster-assembly-0.2.jar gs://$DEVSHELL_PROJECT_ID cd data/small cp sample.seq sample_small.seq gsutil cp sample_small.seq gs://$DEVSHELL_PROJECT_ID","title":"Upload Data to Google Cloud Storage"},{"location":"sparc_on_gcp/#launch-dataproc","text":"","title":"Launch Dataproc"},{"location":"sparc_on_gcp/#run-sparc-job-on-dataproc","text":"In the Dataproc console, click Jobs . Click Submit job . For Job type , select Spark ; for Main class or jar and Jar files , specify the location of the SpaRC jar file you uploaded to your bucket. Your bucket-name is your project name : gs://<my-project-name>/LocalCluster-assembly-0.2.jar . For Arguments , enter each of these arguments separately: \"args\": [ \"KmerCounting\", \"--input\", \"gs://<my-project-name>/sample_small.seq\", \"--output\", \"test.log\", \"--kmer_length\", \"31\" ] For Properties , enter these Key-Value pairs separately: \"properties\": { \"spark.executor.extraClassPath\": \"gs://<my-project-name>/LocalCluster-assembly-0.2.jar\", \"spark.driver.maxResultSize\": \"8g\", \"spark.network.timeout\": \"360000\", \"spark.default.parallelism\": \"4\", \"spark.eventLog.enabled\": \"false\" } Click Submit","title":"Run SpaRC job on Dataproc"},{"location":"sparc_on_lawrencium/","text":"Running SpaRC on Lawrencium SpaRC is an Apache Spark-based scalable genomic sequence clustering application . SpaRC has been running successfully on AWS EMR , as well as on the Bridges supercomputer at PSC. In this tutorial, I describe how to run SpaRC on Lawrencium . Spark On Demand on Lawrencium Users can run Spark jobs on Lawrencium in Spark On Demand (SOD) fashion, in which a standalone Spark cluster will be created on demand in a Slurm job. Note that the Spark cluster will be running in standalone mode , so there will be no YARN cluster manager, nor HDFS. In lieu of HDFS, we'll use Lustre scratch for storage. As of this writing, there is only Spark 2.1.0 available on Lawrencium. We may install a more up-to-date version in the near future. Building SpaRC You'll build SpaRC against Spark 2.1.0. The source code of SpaRC is hosted on Bitbucket at https://bitbucket.org/LizhenShi/sparc . I don't have write access to the repo, so I imported it to GitHub, at https://github.com/shawfdong/sparc . I've added a new file build.sbt.spark2.1.0 to the GitHub repo, which, as the name suggests, will be used to build SpaRC against Spark 2.1.0. Note that you can't build SpaRC on the login nodes of Lawrencium, because rsync (which is required by sbt) is disabled there. You'll have to use the data transfer node lrc-xfer.lbl.gov : $ ssh lrc-xfer.lbl.gov Download and unpack the Scala build tool sbt : $ wget https://piccolo.link/sbt-1.3.10.tgz $ tar xvz sbt-1.3.10.tgz Load the module for JDK 1.8.0: $ export MODULEPATH=$MODULEPATH:/global/software/sl-7.x86_64/modfiles/langs $ module load java $ java -version java version \"1.8.0_121\" Java(TM) SE Runtime Environment (build 1.8.0_121-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) Clone and build SpaRC: $ git clone https://github.com/shawfdong/sparc.git $ cd sparc $ cp build.sbt.spark2.1.0 build.sbt $ ~/sbt/bin/sbt assembly This will create a fat jar file ~/sparc/target/scala-2.11/LocalCluster-assembly-0.2.jar . Copy it to your Lustre scratch space: $ cp target/scala-2.11/LocalCluster-assembly-0.2.jar /global/scratch/$USER/ While you are at it, also download a sample sequence data file: $ cd /global/scratch/$USER $ curl http://s3.amazonaws.com/share.jgi-ga.org/sparc_example/illumina_02G_merged_with_id.seq.gz -o sample.seq.gz $ gunzip sample.seq.gz $ wc -l sample.seq 6343345 sample.seq $ head -2 sample.seq 6 HISEQ13:204:C8T6VANXX:1:1101:3726:1992 NATATTCCCGTTCTGATATTGCGTTAAGTCGTTCCCCTAAGCCGGCCCTCCTTATCGAGCGCGCCGGCTTTTTTTGCCATGTTCAGCGAATCACAGGACAAGATACTTCACCTAACGTAGTAGATGGTTCTATGCTTAAGGGCAAGGTGTNTTAATCTCGATATCCGCCTGTTTTAATAAATCAGCGACGAAGCGATGGGAGGATAAGCGCTCGTCAAAAACCACGCGCTTTTTTTCTAAGGTGGGTAAGTTCAAGGTAACACCCCCACTATGCCTATGAGTGAATTGGTAACACCTTGCC 60 HISEQ13:204:C8T6VANXX:1:1101:4370:1919 NCGTGCGCCCATCTCCGTGGCTAAACAGCTTGAGGTGGAAATTCGCCAGTGGATACAGCAGCATGCAGCGACAGGCGGGCGTCGCCTCCCTTCGATACGCCATTTAGCAGCAACACATAACGTCAGCCGCAATGCAGTCATTGAAGCTTANGTAAGGTCTTCTCCTTCGCGCCAATCGTTAGGTAACCAGCCGCAGCCCAGTTTCAATGACTGTTCATCGGTGTTAAACACGCCCCATAAGCCATTCGTCACTTCTTCCAATGGCGTTGATGACGCGGGTTGAACCAGTTTCAGCGCGTTA Now you can exit from lrc-xfer.lbl.gov . Alternatively , you could build SpaRC on your local computer, then upload the assembled fat jar file to your Lustre scratch space on Lawrencium. Running SpaRC interactively SSH to a Lawrencium login node: $ ssh lrc-login.lbl.gov For demonstration purpose, request 2 nodes from the lr6 partition for the interactive job: $ cd /global/scratch/$USER $ srun -p lr6 --qos=lr_normal -N 2 -t 1:00:0 --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> --pty bash When the job starts, you'll be given exclusive allocation to 2 compute nodes in the lr6 partition, each one of which has 2x 16-core Intel Xeon Gold 6130 CPUs and 96 GB memory (so in total, there will be 64 cores and 192GB memory available in your Spark cluster). And you'll be dropped to a bash shell on one of the compute nodes. Start Spark On Demand (SOD): $ source /global/home/groups/allhands/bin/spark_helper.sh $ spark-start Run the first Spark job on SOD (you might want to tune the values for --executor-cores , --num-executors and --executor-memory ): $ SCRATCH=/global/scratch/$USER $ JAR=$SCRATCH/LocalCluster-assembly-0.2.jar $ OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" $ OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" $ spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C Run the second Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 Run the third Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 Run the fifth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 Once you are done, don't forget to stop Spark On Demand and exit from the interactive job: $ spark-stop $ exit Running SpaRC in batch mode To run SpaRC in batch mode, write a Slurm job script and call it sparc.slurm : #!/bin/bash #SBATCH --job-name=sparc #SBATCH --partition=lr6 #SBATCH --qos=lr_normal #SBATCH --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> #SBATCH --nodes=2 #SBATCH --time=01:00:00 source /global/home/groups/allhands/bin/spark_helper.sh # Start Spark On Demand spark-start SCRATCH=/global/scratch/$USER JAR=$SCRATCH/LocalCluster-assembly-0.2.jar OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" # 1st Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C # 2nd Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 # 3rd Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: # 4th Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 # 5th Spark job spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 # Stop Spark On Demand spark-stop Then submit the job with: sbatch sparc.slurm Known issues and future improvements Presumably, there is a switch -i to spark-start that would enable communications over the IPoIB network. But it doesn't work! Spark 2.1.0 is a bit old. References https://academic.oup.com/bioinformatics/article/35/5/760/5078476 https://peerj.com/articles/8966/","title":"SpaRC on Lawrencium"},{"location":"sparc_on_lawrencium/#running-sparc-on-lawrencium","text":"SpaRC is an Apache Spark-based scalable genomic sequence clustering application . SpaRC has been running successfully on AWS EMR , as well as on the Bridges supercomputer at PSC. In this tutorial, I describe how to run SpaRC on Lawrencium .","title":"Running SpaRC on Lawrencium"},{"location":"sparc_on_lawrencium/#spark-on-demand-on-lawrencium","text":"Users can run Spark jobs on Lawrencium in Spark On Demand (SOD) fashion, in which a standalone Spark cluster will be created on demand in a Slurm job. Note that the Spark cluster will be running in standalone mode , so there will be no YARN cluster manager, nor HDFS. In lieu of HDFS, we'll use Lustre scratch for storage. As of this writing, there is only Spark 2.1.0 available on Lawrencium. We may install a more up-to-date version in the near future.","title":"Spark On Demand on Lawrencium"},{"location":"sparc_on_lawrencium/#building-sparc","text":"You'll build SpaRC against Spark 2.1.0. The source code of SpaRC is hosted on Bitbucket at https://bitbucket.org/LizhenShi/sparc . I don't have write access to the repo, so I imported it to GitHub, at https://github.com/shawfdong/sparc . I've added a new file build.sbt.spark2.1.0 to the GitHub repo, which, as the name suggests, will be used to build SpaRC against Spark 2.1.0. Note that you can't build SpaRC on the login nodes of Lawrencium, because rsync (which is required by sbt) is disabled there. You'll have to use the data transfer node lrc-xfer.lbl.gov : $ ssh lrc-xfer.lbl.gov Download and unpack the Scala build tool sbt : $ wget https://piccolo.link/sbt-1.3.10.tgz $ tar xvz sbt-1.3.10.tgz Load the module for JDK 1.8.0: $ export MODULEPATH=$MODULEPATH:/global/software/sl-7.x86_64/modfiles/langs $ module load java $ java -version java version \"1.8.0_121\" Java(TM) SE Runtime Environment (build 1.8.0_121-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) Clone and build SpaRC: $ git clone https://github.com/shawfdong/sparc.git $ cd sparc $ cp build.sbt.spark2.1.0 build.sbt $ ~/sbt/bin/sbt assembly This will create a fat jar file ~/sparc/target/scala-2.11/LocalCluster-assembly-0.2.jar . Copy it to your Lustre scratch space: $ cp target/scala-2.11/LocalCluster-assembly-0.2.jar /global/scratch/$USER/ While you are at it, also download a sample sequence data file: $ cd /global/scratch/$USER $ curl http://s3.amazonaws.com/share.jgi-ga.org/sparc_example/illumina_02G_merged_with_id.seq.gz -o sample.seq.gz $ gunzip sample.seq.gz $ wc -l sample.seq 6343345 sample.seq $ head -2 sample.seq 6 HISEQ13:204:C8T6VANXX:1:1101:3726:1992 NATATTCCCGTTCTGATATTGCGTTAAGTCGTTCCCCTAAGCCGGCCCTCCTTATCGAGCGCGCCGGCTTTTTTTGCCATGTTCAGCGAATCACAGGACAAGATACTTCACCTAACGTAGTAGATGGTTCTATGCTTAAGGGCAAGGTGTNTTAATCTCGATATCCGCCTGTTTTAATAAATCAGCGACGAAGCGATGGGAGGATAAGCGCTCGTCAAAAACCACGCGCTTTTTTTCTAAGGTGGGTAAGTTCAAGGTAACACCCCCACTATGCCTATGAGTGAATTGGTAACACCTTGCC 60 HISEQ13:204:C8T6VANXX:1:1101:4370:1919 NCGTGCGCCCATCTCCGTGGCTAAACAGCTTGAGGTGGAAATTCGCCAGTGGATACAGCAGCATGCAGCGACAGGCGGGCGTCGCCTCCCTTCGATACGCCATTTAGCAGCAACACATAACGTCAGCCGCAATGCAGTCATTGAAGCTTANGTAAGGTCTTCTCCTTCGCGCCAATCGTTAGGTAACCAGCCGCAGCCCAGTTTCAATGACTGTTCATCGGTGTTAAACACGCCCCATAAGCCATTCGTCACTTCTTCCAATGGCGTTGATGACGCGGGTTGAACCAGTTTCAGCGCGTTA Now you can exit from lrc-xfer.lbl.gov . Alternatively , you could build SpaRC on your local computer, then upload the assembled fat jar file to your Lustre scratch space on Lawrencium.","title":"Building SpaRC"},{"location":"sparc_on_lawrencium/#running-sparc-interactively","text":"SSH to a Lawrencium login node: $ ssh lrc-login.lbl.gov For demonstration purpose, request 2 nodes from the lr6 partition for the interactive job: $ cd /global/scratch/$USER $ srun -p lr6 --qos=lr_normal -N 2 -t 1:00:0 --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> --pty bash When the job starts, you'll be given exclusive allocation to 2 compute nodes in the lr6 partition, each one of which has 2x 16-core Intel Xeon Gold 6130 CPUs and 96 GB memory (so in total, there will be 64 cores and 192GB memory available in your Spark cluster). And you'll be dropped to a bash shell on one of the compute nodes. Start Spark On Demand (SOD): $ source /global/home/groups/allhands/bin/spark_helper.sh $ spark-start Run the first Spark job on SOD (you might want to tune the values for --executor-cores , --num-executors and --executor-memory ): $ SCRATCH=/global/scratch/$USER $ JAR=$SCRATCH/LocalCluster-assembly-0.2.jar $ OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" $ OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" $ spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C Run the second Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 Run the third Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 Run the fifth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 Once you are done, don't forget to stop Spark On Demand and exit from the interactive job: $ spark-stop $ exit","title":"Running SpaRC interactively"},{"location":"sparc_on_lawrencium/#running-sparc-in-batch-mode","text":"To run SpaRC in batch mode, write a Slurm job script and call it sparc.slurm : #!/bin/bash #SBATCH --job-name=sparc #SBATCH --partition=lr6 #SBATCH --qos=lr_normal #SBATCH --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> #SBATCH --nodes=2 #SBATCH --time=01:00:00 source /global/home/groups/allhands/bin/spark_helper.sh # Start Spark On Demand spark-start SCRATCH=/global/scratch/$USER JAR=$SCRATCH/LocalCluster-assembly-0.2.jar OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" # 1st Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C # 2nd Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 # 3rd Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: # 4th Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 # 5th Spark job spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 # Stop Spark On Demand spark-stop Then submit the job with: sbatch sparc.slurm","title":"Running SpaRC in batch mode"},{"location":"sparc_on_lawrencium/#known-issues-and-future-improvements","text":"Presumably, there is a switch -i to spark-start that would enable communications over the IPoIB network. But it doesn't work! Spark 2.1.0 is a bit old.","title":"Known issues and future improvements"},{"location":"sparc_on_lawrencium/#references","text":"https://academic.oup.com/bioinformatics/article/35/5/760/5078476 https://peerj.com/articles/8966/","title":"References"},{"location":"storage/","text":"","title":"Storage Resources"},{"location":"support/","text":"Science General Support Science IT supports provision of all services and capabilities offered by the IT Division at LBL -- we are a one-stop shop for any need you may have including: Provision of virtual resources through our Science Virtual Machine program Help with data management plans and research needs through the LBL Research Library Assistance with IT requirements development, program management, architecture development and procurement support Identifying software needs, resources and deployment options","title":"Science General Support"},{"location":"support/#science-general-support","text":"Science IT supports provision of all services and capabilities offered by the IT Division at LBL -- we are a one-stop shop for any need you may have including: Provision of virtual resources through our Science Virtual Machine program Help with data management plans and research needs through the LBL Research Library Assistance with IT requirements development, program management, architecture development and procurement support Identifying software needs, resources and deployment options","title":"Science General Support"}]}