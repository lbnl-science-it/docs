{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Whether you need a virtual server, help deciding on a data storage solution, help with procuring lab equipment, or access to the Lab's Lawrencium HPC, the place to start is with an email to scienceit@lbl.gov! Initial consultations are always free We have many low cost services We support on and off premises capabilities - if the right answer isn't available at LBL, we can find what you need at UCB, with external or cloud services providers (AWS, GCP) or elsewhere! Free drop-in office hours are available every Wednesday from 10:30am to Noon in 50B-3209","title":"Home"},{"location":"#getting-started","text":"Whether you need a virtual server, help deciding on a data storage solution, help with procuring lab equipment, or access to the Lab's Lawrencium HPC, the place to start is with an email to scienceit@lbl.gov! Initial consultations are always free We have many low cost services We support on and off premises capabilities - if the right answer isn't available at LBL, we can find what you need at UCB, with external or cloud services providers (AWS, GCP) or elsewhere! Free drop-in office hours are available every Wednesday from 10:30am to Noon in 50B-3209","title":"Getting Started"},{"location":"cloud/","text":"Cloud Resources LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP). This program provides the following benefits: No charge to have an account in the program Charges only for actual usage of cloud services like storage or compute Monthly billing for cloud usage are direct to your PID via recharge mechanism Simple enrollment process. De-enrollment only changes the billing setup in your account, and your account will continue to be active. You maintain complete control of your own AWS or GCP dashboard, and your data, tools, and services in your account are only accessible by you. Both AWS and GCP make discounts available to LBNL users for using their services. Generally speaking, costs for using GCP will be lower than AWS due to lower pricing and additional discounts. The discounts are: Type AWS GCP Overall 7% 13% Data Egress 15% 25% For AWS, restrictions apply Currently there are over 125 people at LBNL with cloud accounts on AWS and GCP. Most commonly people use virtual machines and storage, however there are many advanced users using containerization, machine learning, AI, and data visualization services on both platforms. Maybe list a few of the more interesting projects using AWS and GCP here? To set up a cloud account on either AWS or GCP, please send email to scienceit@lbl.gov to start the process.","title":"Cloud Resources"},{"location":"cloud/#cloud-resources","text":"LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP). This program provides the following benefits: No charge to have an account in the program Charges only for actual usage of cloud services like storage or compute Monthly billing for cloud usage are direct to your PID via recharge mechanism Simple enrollment process. De-enrollment only changes the billing setup in your account, and your account will continue to be active. You maintain complete control of your own AWS or GCP dashboard, and your data, tools, and services in your account are only accessible by you. Both AWS and GCP make discounts available to LBNL users for using their services. Generally speaking, costs for using GCP will be lower than AWS due to lower pricing and additional discounts. The discounts are: Type AWS GCP Overall 7% 13% Data Egress 15% 25% For AWS, restrictions apply Currently there are over 125 people at LBNL with cloud accounts on AWS and GCP. Most commonly people use virtual machines and storage, however there are many advanced users using containerization, machine learning, AI, and data visualization services on both platforms. Maybe list a few of the more interesting projects using AWS and GCP here? To set up a cloud account on either AWS or GCP, please send email to scienceit@lbl.gov to start the process.","title":"Cloud Resources"},{"location":"compute/","text":"","title":"Compute Resources"},{"location":"consult/","text":"Consulting & Training Science IT provides on-demand and long-term \"embedded\" research consulting support, depending on your needs. We also support hiring of interns for science programs through the Science IT Intern program, as well as collaborative evaluation & support for hiring of staff to meet science project IT needs. One email to scienceit@lbl.gov is all you need to do to get the help you want. We provide a wide range of training opportunities -- all at no cost to science users. Via our partnerships with UCB's D-Lab, NSF's XSEDE, and AWS/GCP training programs we offer brown-bags, technical sessions, and custom opportunities to grow your IT skills. For more information, and to register for upcoming events, please see ittraining.lbl.gov.","title":"Consulting & Training"},{"location":"consult/#consulting-training","text":"Science IT provides on-demand and long-term \"embedded\" research consulting support, depending on your needs. We also support hiring of interns for science programs through the Science IT Intern program, as well as collaborative evaluation & support for hiring of staff to meet science project IT needs. One email to scienceit@lbl.gov is all you need to do to get the help you want. We provide a wide range of training opportunities -- all at no cost to science users. Via our partnerships with UCB's D-Lab, NSF's XSEDE, and AWS/GCP training programs we offer brown-bags, technical sessions, and custom opportunities to grow your IT skills. For more information, and to register for upcoming events, please see ittraining.lbl.gov.","title":"Consulting &amp; Training"},{"location":"data/","text":"Data Management & Storage Science IT can help you design the right data management solution supporting your research, and we have a wide variety of options supporting scientific data collection, storage, and analytics. These include: Our Science Project Storage Service ($25/Mo/TB) for performance data storage on demand, including free data movement and migration Our Compellent and Lawrencium storage condo option for those storing fixed amounts of data for longer than 6 months Access to cloud storage and assistance with local storage provision Help with data migration, including Globus and unlimited, no cost GDrive data storage","title":"Data Management & Storage"},{"location":"data/#data-management-storage","text":"Science IT can help you design the right data management solution supporting your research, and we have a wide variety of options supporting scientific data collection, storage, and analytics. These include: Our Science Project Storage Service ($25/Mo/TB) for performance data storage on demand, including free data movement and migration Our Compellent and Lawrencium storage condo option for those storing fixed amounts of data for longer than 6 months Access to cloud storage and assistance with local storage provision Help with data migration, including Globus and unlimited, no cost GDrive data storage","title":"Data Management &amp; Storage"},{"location":"hpc/","text":"High-Performance Computing Science IT supports and operates LBL's institutional high performance compute resource called Lawrencium. Lawrencium is a 55,000 core cluster which consist of shared nodes and PI-contributed condo nodes currently supporting compute needs across virtually every scientific discipline studied at LBL. For account information (including access to our PI Grant program) please see scs.lbl.gov. Support for both shared and private compute node hosting GPU nodes available now (Dual V100 with NVLink or Quad GTX1080Ti nodes) supporting machine learning and other needs High Speed Network connections between nodes - FDR & EDR Infiniband interconnect Users may also directly purchase service units ($0.01) on-demand","title":"High-Performance Computing"},{"location":"hpc/#high-performance-computing","text":"Science IT supports and operates LBL's institutional high performance compute resource called Lawrencium. Lawrencium is a 55,000 core cluster which consist of shared nodes and PI-contributed condo nodes currently supporting compute needs across virtually every scientific discipline studied at LBL. For account information (including access to our PI Grant program) please see scs.lbl.gov. Support for both shared and private compute node hosting GPU nodes available now (Dual V100 with NVLink or Quad GTX1080Ti nodes) supporting machine learning and other needs High Speed Network connections between nodes - FDR & EDR Infiniband interconnect Users may also directly purchase service units ($0.01) on-demand","title":"High-Performance Computing"},{"location":"sparc_on_gcp/","text":"Running SpaRC on Google Cloud Dataproc Building SpaRC: Open a new Cloud Shell and run: git clone https://github.com/shawfdong/sparc.git cd sparc wget https://piccolo.link/sbt-1.3.10.tgz tar xvzf sbt-1.3.10.tgz ./sbt/bin/sbt assembly A jar file should be created at: target/scala-2.11/LocalCluster-assembly-0.2.jar Upload Data to Google Cloud Storage Navigate to Storage and select Storage > Browser . Click Create Bucket . ` Specify your project name as the bucket name . Click Create . Copy the compiled SpaRC LocalCluster-assembly-0.2.jar and a sample input file sample_small.seq to the project bucket you just created, by running the below in Cloud Shell: gsutil cp target/scala-2.11/LocalCluster-assembly-0.2.jar gs://$DEVSHELL_PROJECT_ID cd data/small cp sample.seq sample_small.seq gsutil cp sample_small.seq gs://$DEVSHELL_PROJECT_ID Launch Dataproc Run SpaRC job on Dataproc In the Dataproc console, click Jobs . Click Submit job . For Job type , select Spark ; for Main class or jar and Jar files , specify the location of the SpaRC jar file you uploaded to your bucket. Your bucket-name is your project name : gs://<my-project-name>/LocalCluster-assembly-0.2.jar . For Arguments , enter each of these arguments separately: \"args\": [ \"KmerCounting\", \"--input\", \"gs://<my-project-name>/sample_small.seq\", \"--output\", \"test.log\", \"--kmer_length\", \"31\" ] For Properties , enter these Key-Value pairs separately: ``` \"properties\": { \"spark.executor.extraClassPath\": \"gs://<my-project-name>/LocalCluster-assembly-0.2.jar\", \"spark.driver.maxResultSize\": \"8g\", \"spark.network.timeout\": \"360000\", \"spark.default.parallelism\": \"4\", \"spark.eventLog.enabled\": \"false\" } ``` Click Submit","title":"SpaRC on GCP"},{"location":"sparc_on_gcp/#running-sparc-on-google-cloud-dataproc","text":"","title":"Running SpaRC on Google Cloud Dataproc"},{"location":"sparc_on_gcp/#building-sparc","text":"Open a new Cloud Shell and run: git clone https://github.com/shawfdong/sparc.git cd sparc wget https://piccolo.link/sbt-1.3.10.tgz tar xvzf sbt-1.3.10.tgz ./sbt/bin/sbt assembly A jar file should be created at: target/scala-2.11/LocalCluster-assembly-0.2.jar","title":"Building SpaRC:"},{"location":"sparc_on_gcp/#upload-data-to-google-cloud-storage","text":"Navigate to Storage and select Storage > Browser . Click Create Bucket . ` Specify your project name as the bucket name . Click Create . Copy the compiled SpaRC LocalCluster-assembly-0.2.jar and a sample input file sample_small.seq to the project bucket you just created, by running the below in Cloud Shell: gsutil cp target/scala-2.11/LocalCluster-assembly-0.2.jar gs://$DEVSHELL_PROJECT_ID cd data/small cp sample.seq sample_small.seq gsutil cp sample_small.seq gs://$DEVSHELL_PROJECT_ID","title":"Upload Data to Google Cloud Storage"},{"location":"sparc_on_gcp/#launch-dataproc","text":"","title":"Launch Dataproc"},{"location":"sparc_on_gcp/#run-sparc-job-on-dataproc","text":"In the Dataproc console, click Jobs . Click Submit job . For Job type , select Spark ; for Main class or jar and Jar files , specify the location of the SpaRC jar file you uploaded to your bucket. Your bucket-name is your project name : gs://<my-project-name>/LocalCluster-assembly-0.2.jar . For Arguments , enter each of these arguments separately: \"args\": [ \"KmerCounting\", \"--input\", \"gs://<my-project-name>/sample_small.seq\", \"--output\", \"test.log\", \"--kmer_length\", \"31\" ] For Properties , enter these Key-Value pairs separately: ``` \"properties\": { \"spark.executor.extraClassPath\": \"gs://<my-project-name>/LocalCluster-assembly-0.2.jar\", \"spark.driver.maxResultSize\": \"8g\", \"spark.network.timeout\": \"360000\", \"spark.default.parallelism\": \"4\", \"spark.eventLog.enabled\": \"false\" } ``` Click Submit","title":"Run SpaRC job on Dataproc"},{"location":"sparc_on_lawrencium/","text":"Running SpaRC on Lawrencium SpaRC is an Apache Spark-based scalable genomic sequence clustering application . SpaRC has been running successfully on AWS EMR , as well as on the Bridges supercomputer at PSC. In this tutorial, I describe how to run SpaRC on Lawrencium . Spark On Demand on Lawrencium Users can run Spark jobs on Lawrencium in Spark On Demand (SOD) fashion, in which a standalone Spark cluster will be created on demand in a Slurm job. Note that the Spark cluster will be running in standalone mode , so there will be no YARN cluster manager, nor HDFS. In lieu of HDFS, we'll use Lustre scratch for storage. As of this writing, there is only Spark 2.1.0 available on Lawrencium. We may install a more up-to-date version in the near future. Building SpaRC You'll build SpaRC against Spark 2.1.0. The source code of SpaRC is hosted on Bitbucket at https://bitbucket.org/LizhenShi/sparc . I don't have write access to the repo, so I imported it to GitHub, at https://github.com/shawfdong/sparc . I've added a new file build.sbt.spark2.1.0 to the GitHub repo, which, as the name suggests, will be used to build SpaRC against Spark 2.1.0. Note that you can't build SpaRC on the login nodes of Lawrencium, because rsync (which is required by sbt) is disabled there. You'll have to use the data transfer node lrc-xfer.lbl.gov : $ ssh lrc-xfer.lbl.gov Download and unpack the Scala build tool sbt : $ wget https://piccolo.link/sbt-1.3.10.tgz $ tar xvz sbt-1.3.10.tgz Load the module for JDK 1.8.0: $ export MODULEPATH=$MODULEPATH:/global/software/sl-7.x86_64/modfiles/langs $ module load java $ java -version java version \"1.8.0_121\" Java(TM) SE Runtime Environment (build 1.8.0_121-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) Clone and build SpaRC: $ git clone https://github.com/shawfdong/sparc.git $ cd sparc $ cp build.sbt.spark2.1.0 build.sbt $ ~/sbt/bin/sbt assembly This will create a fat jar file ~/sparc/target/scala-2.11/LocalCluster-assembly-0.2.jar . Copy it to your Lustre scratch space: $ cp target/scala-2.11/LocalCluster-assembly-0.2.jar /global/scratch/$USER/ While you are at it, also download a sample sequence data file: $ cd /global/scratch/$USER $ curl http://s3.amazonaws.com/share.jgi-ga.org/sparc_example/illumina_02G_merged_with_id.seq.gz -o sample.seq.gz $ gunzip sample.seq.gz $ wc -l sample.seq 6343345 sample.seq $ head -2 sample.seq 6 HISEQ13:204:C8T6VANXX:1:1101:3726:1992 NATATTCCCGTTCTGATATTGCGTTAAGTCGTTCCCCTAAGCCGGCCCTCCTTATCGAGCGCGCCGGCTTTTTTTGCCATGTTCAGCGAATCACAGGACAAGATACTTCACCTAACGTAGTAGATGGTTCTATGCTTAAGGGCAAGGTGTNTTAATCTCGATATCCGCCTGTTTTAATAAATCAGCGACGAAGCGATGGGAGGATAAGCGCTCGTCAAAAACCACGCGCTTTTTTTCTAAGGTGGGTAAGTTCAAGGTAACACCCCCACTATGCCTATGAGTGAATTGGTAACACCTTGCC 60 HISEQ13:204:C8T6VANXX:1:1101:4370:1919 NCGTGCGCCCATCTCCGTGGCTAAACAGCTTGAGGTGGAAATTCGCCAGTGGATACAGCAGCATGCAGCGACAGGCGGGCGTCGCCTCCCTTCGATACGCCATTTAGCAGCAACACATAACGTCAGCCGCAATGCAGTCATTGAAGCTTANGTAAGGTCTTCTCCTTCGCGCCAATCGTTAGGTAACCAGCCGCAGCCCAGTTTCAATGACTGTTCATCGGTGTTAAACACGCCCCATAAGCCATTCGTCACTTCTTCCAATGGCGTTGATGACGCGGGTTGAACCAGTTTCAGCGCGTTA Now you can exit from lrc-xfer.lbl.gov . Alternatively , you could build SpaRC on your local computer, then upload the assembled fat jar file to your Lustre scratch space on Lawrencium. Running SpaRC interactively SSH to a Lawrencium login node: $ ssh lrc-login.lbl.gov For demonstration purpose, request 2 nodes from the lr6 partition for the interactive job: $ cd /global/scratch/$USER $ srun -p lr6 --qos=lr_normal -N 2 -t 1:00:0 --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> --pty bash When the job starts, you'll be given exclusive allocation to 2 compute nodes in the lr6 partition, each one of which has 2x 16-core Intel Xeon Gold 6130 CPUs and 96 GB memory (so in total, there will be 64 cores and 192GB memory available in your Spark cluster). And you'll be dropped to a bash shell on one of the compute nodes. Start Spark On Demand (SOD): $ source /global/home/groups/allhands/bin/spark_helper.sh $ spark-start Run the first Spark job on SOD (you might want to tune the values for --executor-cores , --num-executors and --executor-memory ): $ SCRATCH=/global/scratch/$USER $ JAR=$SCRATCH/LocalCluster-assembly-0.2.jar $ OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" $ OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" $ spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C Run the second Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 Run the third Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 Run the fifth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 Once you are done, don't forget to stop Spark On Demand and exit from the interactive job: $ spark-stop $ exit Running SpaRC in batch mode To run SpaRC in batch mode, write a Slurm job script and call it sparc.slurm : #!/bin/bash #SBATCH --job-name=sparc #SBATCH --partition=lr6 #SBATCH --qos=lr_normal #SBATCH --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> #SBATCH --nodes=2 #SBATCH --time=01:00:00 source /global/home/groups/allhands/bin/spark_helper.sh # Start Spark On Demand spark-start SCRATCH=/global/scratch/$USER JAR=$SCRATCH/LocalCluster-assembly-0.2.jar OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" # 1st Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C # 2nd Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 # 3rd Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: # 4th Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 # 5th Spark job spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 # Stop Spark On Demand spark-stop Then submit the job with: sbatch sparc.slurm Known issues and future improvements Presumably, there is a switch -i to spark-start that would enable communications over the IPoIB network. But it doesn't work! Spark 2.1.0 is a bit old. References https://academic.oup.com/bioinformatics/article/35/5/760/5078476 https://peerj.com/articles/8966/","title":"SpaRC on Lawrencium"},{"location":"sparc_on_lawrencium/#running-sparc-on-lawrencium","text":"SpaRC is an Apache Spark-based scalable genomic sequence clustering application . SpaRC has been running successfully on AWS EMR , as well as on the Bridges supercomputer at PSC. In this tutorial, I describe how to run SpaRC on Lawrencium .","title":"Running SpaRC on Lawrencium"},{"location":"sparc_on_lawrencium/#spark-on-demand-on-lawrencium","text":"Users can run Spark jobs on Lawrencium in Spark On Demand (SOD) fashion, in which a standalone Spark cluster will be created on demand in a Slurm job. Note that the Spark cluster will be running in standalone mode , so there will be no YARN cluster manager, nor HDFS. In lieu of HDFS, we'll use Lustre scratch for storage. As of this writing, there is only Spark 2.1.0 available on Lawrencium. We may install a more up-to-date version in the near future.","title":"Spark On Demand on Lawrencium"},{"location":"sparc_on_lawrencium/#building-sparc","text":"You'll build SpaRC against Spark 2.1.0. The source code of SpaRC is hosted on Bitbucket at https://bitbucket.org/LizhenShi/sparc . I don't have write access to the repo, so I imported it to GitHub, at https://github.com/shawfdong/sparc . I've added a new file build.sbt.spark2.1.0 to the GitHub repo, which, as the name suggests, will be used to build SpaRC against Spark 2.1.0. Note that you can't build SpaRC on the login nodes of Lawrencium, because rsync (which is required by sbt) is disabled there. You'll have to use the data transfer node lrc-xfer.lbl.gov : $ ssh lrc-xfer.lbl.gov Download and unpack the Scala build tool sbt : $ wget https://piccolo.link/sbt-1.3.10.tgz $ tar xvz sbt-1.3.10.tgz Load the module for JDK 1.8.0: $ export MODULEPATH=$MODULEPATH:/global/software/sl-7.x86_64/modfiles/langs $ module load java $ java -version java version \"1.8.0_121\" Java(TM) SE Runtime Environment (build 1.8.0_121-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) Clone and build SpaRC: $ git clone https://github.com/shawfdong/sparc.git $ cd sparc $ cp build.sbt.spark2.1.0 build.sbt $ ~/sbt/bin/sbt assembly This will create a fat jar file ~/sparc/target/scala-2.11/LocalCluster-assembly-0.2.jar . Copy it to your Lustre scratch space: $ cp target/scala-2.11/LocalCluster-assembly-0.2.jar /global/scratch/$USER/ While you are at it, also download a sample sequence data file: $ cd /global/scratch/$USER $ curl http://s3.amazonaws.com/share.jgi-ga.org/sparc_example/illumina_02G_merged_with_id.seq.gz -o sample.seq.gz $ gunzip sample.seq.gz $ wc -l sample.seq 6343345 sample.seq $ head -2 sample.seq 6 HISEQ13:204:C8T6VANXX:1:1101:3726:1992 NATATTCCCGTTCTGATATTGCGTTAAGTCGTTCCCCTAAGCCGGCCCTCCTTATCGAGCGCGCCGGCTTTTTTTGCCATGTTCAGCGAATCACAGGACAAGATACTTCACCTAACGTAGTAGATGGTTCTATGCTTAAGGGCAAGGTGTNTTAATCTCGATATCCGCCTGTTTTAATAAATCAGCGACGAAGCGATGGGAGGATAAGCGCTCGTCAAAAACCACGCGCTTTTTTTCTAAGGTGGGTAAGTTCAAGGTAACACCCCCACTATGCCTATGAGTGAATTGGTAACACCTTGCC 60 HISEQ13:204:C8T6VANXX:1:1101:4370:1919 NCGTGCGCCCATCTCCGTGGCTAAACAGCTTGAGGTGGAAATTCGCCAGTGGATACAGCAGCATGCAGCGACAGGCGGGCGTCGCCTCCCTTCGATACGCCATTTAGCAGCAACACATAACGTCAGCCGCAATGCAGTCATTGAAGCTTANGTAAGGTCTTCTCCTTCGCGCCAATCGTTAGGTAACCAGCCGCAGCCCAGTTTCAATGACTGTTCATCGGTGTTAAACACGCCCCATAAGCCATTCGTCACTTCTTCCAATGGCGTTGATGACGCGGGTTGAACCAGTTTCAGCGCGTTA Now you can exit from lrc-xfer.lbl.gov . Alternatively , you could build SpaRC on your local computer, then upload the assembled fat jar file to your Lustre scratch space on Lawrencium.","title":"Building SpaRC"},{"location":"sparc_on_lawrencium/#running-sparc-interactively","text":"SSH to a Lawrencium login node: $ ssh lrc-login.lbl.gov For demonstration purpose, request 2 nodes from the lr6 partition for the interactive job: $ cd /global/scratch/$USER $ srun -p lr6 --qos=lr_normal -N 2 -t 1:00:0 --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> --pty bash When the job starts, you'll be given exclusive allocation to 2 compute nodes in the lr6 partition, each one of which has 2x 16-core Intel Xeon Gold 6130 CPUs and 96 GB memory (so in total, there will be 64 cores and 192GB memory available in your Spark cluster). And you'll be dropped to a bash shell on one of the compute nodes. Start Spark On Demand (SOD): $ source /global/home/groups/allhands/bin/spark_helper.sh $ spark-start Run the first Spark job on SOD (you might want to tune the values for --executor-cores , --num-executors and --executor-memory ): $ SCRATCH=/global/scratch/$USER $ JAR=$SCRATCH/LocalCluster-assembly-0.2.jar $ OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" $ OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" $ spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C Run the second Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 Run the third Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 Run the fifth Spark job: $ spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 Once you are done, don't forget to stop Spark On Demand and exit from the interactive job: $ spark-stop $ exit","title":"Running SpaRC interactively"},{"location":"sparc_on_lawrencium/#running-sparc-in-batch-mode","text":"To run SpaRC in batch mode, write a Slurm job script and call it sparc.slurm : #!/bin/bash #SBATCH --job-name=sparc #SBATCH --partition=lr6 #SBATCH --qos=lr_normal #SBATCH --account=<NAME_OF_YOUR_PROJECT_ACCOUNT> #SBATCH --nodes=2 #SBATCH --time=01:00:00 source /global/home/groups/allhands/bin/spark_helper.sh # Start Spark On Demand spark-start SCRATCH=/global/scratch/$USER JAR=$SCRATCH/LocalCluster-assembly-0.2.jar OPT1=\"--master $SPARK_URL --executor-cores 4 --num-executors 16 --executor-memory 12g\" OPT2=\"--conf spark.executor.extraClassPath=$JAR \\ --conf spark.driver.maxResultSize=8g \\ --conf spark.network.timeout=360000 \\ --conf spark.speculation=true \\ --conf spark.default.parallelism=100 \\ --conf spark.eventLog.enabled=false\" # 1st Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerCounting --wait 1 \\ -i $SCRATCH/sample.seq \\ -o $SCRATCH/test_kc_seq_31 --format seq -k 31 -C # 2nd Spark job spark-submit $OPT1 $OPT2 \\ $JAR KmerMapReads2 --wait 1 \\ --reads $SCRATCH/sample.seq \\ --format seq -o $SCRATCH/test_kmerreads.txt_31 -k 31 \\ --kmer $SCRATCH/test_kc_seq_31 \\ --contamination 0 --min_kmer_count 2 \\ --max_kmer_count 100000 -C --n_iteration 1 # 3rd Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphGen2 --wait 1 \\ -i $SCRATCH/test_kmerreads.txt_31 \\ -o $SCRATCH/test_edges.txt_31 \\ --min_shared_kmers 2 --max_degree 50 -n 1000 Run the fourth Spark job: # 4th Spark job spark-submit $OPT1 $OPT2 \\ $JAR GraphLPA2 --wait 1 \\ -i $SCRATCH/test_edges.txt_31 \\ -o $SCRATCH/test_lpa.txt_31 \\ --min_shared_kmers 2 --max_shared_kmers 20000 \\ --min_reads_per_cluster 2 --max_iteration 10 -n 1000 # 5th Spark job spark-submit $OPT1 $OPT2 \\ $JAR CCAddSeq --wait 1 \\ -i $SCRATCH/test_lpa.txt_31 \\ --reads $SCRATCH/sample.seq \\ -o $SCRATCH/sample_lpaseq.txt_31 # Stop Spark On Demand spark-stop Then submit the job with: sbatch sparc.slurm","title":"Running SpaRC in batch mode"},{"location":"sparc_on_lawrencium/#known-issues-and-future-improvements","text":"Presumably, there is a switch -i to spark-start that would enable communications over the IPoIB network. But it doesn't work! Spark 2.1.0 is a bit old.","title":"Known issues and future improvements"},{"location":"sparc_on_lawrencium/#references","text":"https://academic.oup.com/bioinformatics/article/35/5/760/5078476 https://peerj.com/articles/8966/","title":"References"},{"location":"storage/","text":"","title":"Storage Resources"},{"location":"support/","text":"Science General Support Science IT supports provision of all services and capabilities offered by the IT Division at LBL -- we are a one-stop shop for any need you may have including: Provision of virtual resources through our Science Virtual Machine program Help with data management plans and research needs through the LBL Research Library Assistance with IT requirements development, program management, architecture development and procurement support Identifying software needs, resources and deployment options","title":"Science General Support"},{"location":"support/#science-general-support","text":"Science IT supports provision of all services and capabilities offered by the IT Division at LBL -- we are a one-stop shop for any need you may have including: Provision of virtual resources through our Science Virtual Machine program Help with data management plans and research needs through the LBL Research Library Assistance with IT requirements development, program management, architecture development and procurement support Identifying software needs, resources and deployment options","title":"Science General Support"}]}